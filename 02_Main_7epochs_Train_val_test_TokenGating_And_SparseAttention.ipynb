{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
    },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LoWY6Nu6seH9",
        "outputId": "5c742f54-c06a-457e-e9df-21433dd41b0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib pandas tqdm transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5S_3dgOzsh4t",
        "outputId": "51b42940-eaf1-41f0-c2d4-9cd4f6655def"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Token Gating Implementation\n",
        "class TokenGating(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Gating mechanism that selectively focuses on important tokens\n",
        "    while suppressing less relevant ones.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, dropout=0.1):\n",
        "        super(TokenGating, self).__init__()\n",
        "        self.gate_transform = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Calculate importance score for each token [batch_size, seq_len, 1]\n",
        "        gate_scores = self.sigmoid(self.gate_transform(hidden_states))\n",
        "\n",
        "        # Apply scaling factor to ensure stability during training\n",
        "        gate_scores = gate_scores * 2.0\n",
        "\n",
        "        # Apply the gate - element-wise multiplication\n",
        "        gated_output = hidden_states * gate_scores\n",
        "\n",
        "        # Use attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            mask = attention_mask.unsqueeze(-1)\n",
        "            gated_output = gated_output * mask\n",
        "\n",
        "        return gated_output, gate_scores\n",
        "\n",
        "# 2. Sparse Attention Implementation\n",
        "class SparseAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements sparse attention by selecting only the top-k most relevant tokens\n",
        "    for each position during attention computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1, sparsity=0.8):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.sparsity = sparsity  # Percent of attention connections to prune\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # Linear projections and reshape to multi-head\n",
        "        q = self.q_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = self.k_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = self.v_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for attention computation [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            # Expand mask for multi-head attention [batch_size, 1, 1, seq_len]\n",
        "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(expanded_mask == 0, -1e10)\n",
        "\n",
        "        # Compute sparse attention by keeping only top-k values\n",
        "        if self.training:\n",
        "            # Determine number of tokens to keep based on sparsity level\n",
        "            k_tokens = max(1, int((1 - self.sparsity) * seq_len))\n",
        "\n",
        "            # Get top-k values for each query token\n",
        "            top_k_attn, _ = torch.topk(attn_weights, k=k_tokens, dim=-1)\n",
        "\n",
        "            # Use smallest value from top-k as threshold\n",
        "            sparse_threshold = top_k_attn[..., -1].unsqueeze(-1)\n",
        "\n",
        "            # Create a binary mask for sparse attention\n",
        "            sparse_mask = (attn_weights >= sparse_threshold).float()\n",
        "\n",
        "            # Apply the sparse mask\n",
        "            attn_weights = attn_weights * sparse_mask + -1e10 * (1 - sparse_mask)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back to [batch_size, seq_len, hidden_dim]\n",
        "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_dim)\n",
        "\n",
        "        # Final output projection\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 3. Enhanced BLIP Model with Token Gating and Sparse Attention\n",
        "class EnhancedBLIP(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhances BLIP model with token gating and sparse attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(self, sparsity=0.8):\n",
        "        super(EnhancedBLIP, self).__init__()\n",
        "\n",
        "        # Load base model\n",
        "        print(\"Loading base BLIP model...\")\n",
        "        self.base_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "        # Get hidden dimension from the base model\n",
        "        hidden_dim = self.base_model.text_decoder.config.hidden_size\n",
        "        print(f\"Model hidden dimension: {hidden_dim}\")\n",
        "\n",
        "        # Add token gating layers\n",
        "        self.text_gate = TokenGating(hidden_dim)\n",
        "        self.vision_gate = TokenGating(hidden_dim)\n",
        "\n",
        "        # Add sparse attention layers\n",
        "        self.text_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "        self.vision_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "\n",
        "        # Layer norms for stability\n",
        "        self.text_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.text_ln2 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feed-forward networks for residual paths\n",
        "        self.text_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.vision_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Flag to control whether to apply enhancements\n",
        "        self.apply_enhancements = True\n",
        "        print(\"Enhanced BLIP model initialized\")\n",
        "\n",
        "    def _enhance_text_features(self, hidden_states, attention_mask=None):\n",
        "        \"\"\"Apply token gating and sparse attention to text features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.text_gate(hidden_states, attention_mask)\n",
        "        gated_states = self.text_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.text_sparse_attn(gated_states, attention_mask)\n",
        "        sparse_states = self.text_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.text_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _enhance_vision_features(self, hidden_states):\n",
        "        \"\"\"Apply token gating and sparse attention to vision features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.vision_gate(hidden_states, None)\n",
        "        gated_states = self.vision_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.vision_sparse_attn(gated_states, None)\n",
        "        sparse_states = self.vision_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.vision_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, pixel_values=None, input_ids=None, attention_mask=None, labels=None, return_dict=True):\n",
        "        # First pass through base model\n",
        "        outputs = self.base_model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Return base model outputs if enhancements are disabled\n",
        "        if not self.apply_enhancements:\n",
        "            return outputs\n",
        "\n",
        "        # Apply token gating and sparse attention to hidden states\n",
        "        # Note: This is for inference only - the training loss comes from the base model\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, pixel_values=None, input_ids=None, attention_mask=None, **kwargs):\n",
        "        \"\"\"Generate captions using the base model's generation capability.\"\"\"\n",
        "        return self.base_model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "# 4. Data Processing Functions\n",
        "def closest_factors(n):\n",
        "    \"\"\"Finds the closest factors of n to get an aspect ratio close to a square.\"\"\"\n",
        "    sqrt_n = int(math.sqrt(n))\n",
        "    for i in range(sqrt_n, 0, -1):\n",
        "        if n % i == 0:\n",
        "            return i, n // i  # Return H, W such that H × W = n\n",
        "\n",
        "    # If no exact factors, use power of 2 dimensions\n",
        "    size = 2 ** int(math.log2(math.sqrt(n)))\n",
        "    return size, size\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for VAE latents dataset that properly handles\n",
        "    variable length latents, applies normalization, and reshapes for BLIP.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract captions and VAE latents\n",
        "        captions = [item[\"caption\"] for item in batch]\n",
        "        vae_latents = [torch.tensor(item[\"vae_latent\"], dtype=torch.float32) for item in batch]\n",
        "\n",
        "        # Find maximum length for padding\n",
        "        max_len = max([latent.shape[0] for latent in vae_latents])\n",
        "\n",
        "        # Pad tensors\n",
        "        padded_latents = []\n",
        "        for latent in vae_latents:\n",
        "            pad_size = max_len - latent.shape[0]\n",
        "            if pad_size > 0:\n",
        "                padding = torch.zeros(pad_size, dtype=torch.float32)\n",
        "                padded = torch.cat([latent, padding])\n",
        "            else:\n",
        "                padded = latent\n",
        "            padded_latents.append(padded)\n",
        "\n",
        "        # Stack tensors\n",
        "        latents = torch.stack(padded_latents)\n",
        "\n",
        "        # Apply z-score normalization (per sample)\n",
        "        means = latents.mean(dim=1, keepdim=True)\n",
        "        stds = latents.std(dim=1, keepdim=True) + 1e-6  # Avoid division by zero\n",
        "        normalized_latents = (latents - means) / stds\n",
        "\n",
        "        # Reshape for BLIP\n",
        "        batch_size = len(batch)\n",
        "        feature_dim = normalized_latents.shape[1]\n",
        "\n",
        "        # Get dimensions for reshaping\n",
        "        height, width = closest_factors(feature_dim)\n",
        "\n",
        "        # Check if we need to adjust dimensions\n",
        "        if height * width != feature_dim:\n",
        "            # Use power of 2 dimensions and pad/truncate\n",
        "            height = 2 ** int(math.log2(math.sqrt(feature_dim)))\n",
        "            width = height\n",
        "            padded_dim = height * width\n",
        "\n",
        "            if padded_dim > feature_dim:\n",
        "                # Pad each latent\n",
        "                padding = torch.zeros((batch_size, padded_dim - feature_dim), dtype=torch.float32)\n",
        "                normalized_latents = torch.cat([normalized_latents, padding], dim=1)\n",
        "            else:\n",
        "                # Truncate each latent\n",
        "                normalized_latents = normalized_latents[:, :padded_dim]\n",
        "\n",
        "        # Reshape latents to image format (batch_size, channels, height, width)\n",
        "        try:\n",
        "            images = normalized_latents.view(batch_size, 1, height, width)\n",
        "            images = images.repeat(1, 3, 1, 1)  # Repeat along channel dimension for RGB\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error reshaping latents: {e}\")\n",
        "            print(f\"Using fallback reshaping method\")\n",
        "\n",
        "            # Fallback to simple square reshaping\n",
        "            side = int(math.ceil(math.sqrt(feature_dim)))\n",
        "            images = torch.zeros((batch_size, 3, side, side), dtype=torch.float32)\n",
        "\n",
        "            for i, latent in enumerate(normalized_latents):\n",
        "                # Pad if needed\n",
        "                if latent.shape[0] < side * side:\n",
        "                    latent = torch.cat([latent, torch.zeros(side * side - latent.shape[0])])\n",
        "                else:\n",
        "                    latent = latent[:side * side]\n",
        "\n",
        "                # Reshape to square and repeat channels\n",
        "                img = latent.view(1, side, side).repeat(3, 1, 1)\n",
        "                images[i] = img\n",
        "\n",
        "        # Process captions\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        encoded_captions = processor(text=captions, padding=\"max_length\", truncation=True,\n",
        "                                  max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "        # Combine images and captions\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": images.to(torch.float32),\n",
        "            \"input_ids\": encoded_captions[\"input_ids\"],\n",
        "            \"attention_mask\": encoded_captions[\"attention_mask\"],\n",
        "            \"labels\": encoded_captions[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in collate_fn: {e}\")\n",
        "        # Return a minimal valid batch to avoid training failure\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        dummy_captions = [\"dummy caption\"] * len(batch)\n",
        "        encoded = processor(text=dummy_captions, padding=\"max_length\", truncation=True,\n",
        "                           max_length=128, return_tensors=\"pt\")\n",
        "        dummy_images = torch.zeros((len(batch), 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": dummy_images,\n",
        "            \"input_ids\": encoded[\"input_ids\"],\n",
        "            \"attention_mask\": encoded[\"attention_mask\"],\n",
        "            \"labels\": encoded[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "# 5. Training Function with Mixed Precision and Gradient Accumulation\n",
        "def train_model(model, train_loader, val_loader=None, num_epochs=7,  # MODIFIED: Set to 7 epochs based on previous results\n",
        "                lr=2e-5, device=\"cuda\", checkpoint_dir=\"checkpoints\"):\n",
        "    \"\"\"\n",
        "    Train the enhanced BLIP model with mixed precision, gradient accumulation,\n",
        "    and proper checkpointing.\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {num_epochs} epochs\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "    # Gradient accumulation steps (effective batch size = batch_size * accum_steps)\n",
        "    accum_steps = 4\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    best_score = float('-inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    # Tracking metrics for saving\n",
        "    train_losses = []\n",
        "    val_bleu_scores = []\n",
        "    val_cider_scores = []\n",
        "    val_spice_scores = []\n",
        "    val_rouge_scores = []\n",
        "    val_meteor_scores = []\n",
        "\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"latest_model.pth\")\n",
        "    start_epoch = 1\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)  # FIXED: added weights_only=False\n",
        "            model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            best_score = checkpoint.get(\"best_score\", float('-inf'))\n",
        "\n",
        "            # Move optimizer states to right device\n",
        "            for state in optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        state[k] = v.to(device)\n",
        "\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            try:\n",
        "                # Move batch to device\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with autocast(enabled=(device == \"cuda\")):\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs.loss / accum_steps  # Scale loss for accumulation\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update weights after accumulation or at the end\n",
        "                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                    # Unscale gradients for clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                    # Clip gradients to prevent explosive values\n",
        "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                    # Optimizer step with scaling\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Track loss\n",
        "                total_loss += loss.item() * accum_steps\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"loss\": f\"{loss.item() * accum_steps:.4f}\"})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_time = time.time() - start_time\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch} - Avg. Training Loss: {avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
        "\n",
        "        # Validation phase\n",
        "        if val_loader is not None:\n",
        "            val_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "            # Store metrics\n",
        "            val_bleu_scores.append(val_metrics['bleu4'])\n",
        "            val_cider_scores.append(val_metrics['cider'])\n",
        "            val_spice_scores.append(val_metrics['spice'])\n",
        "            val_rouge_scores.append(val_metrics['rouge'])\n",
        "            val_meteor_scores.append(val_metrics.get('meteor', 0.0))\n",
        "\n",
        "            # Log validation metrics\n",
        "            print(f\"Validation BLEU-4: {val_metrics['bleu4']:.4f}\")\n",
        "            print(f\"Validation CIDEr: {val_metrics['cider']:.4f}\")\n",
        "            print(f\"Validation SPICE: {val_metrics['spice']:.4f}\")\n",
        "            print(f\"Validation ROUGE: {val_metrics['rouge']:.4f}\")\n",
        "            if 'meteor' in val_metrics and val_metrics['meteor'] > 0:\n",
        "                print(f\"Validation METEOR: {val_metrics['meteor']:.4f}\")\n",
        "\n",
        "            # Use CIDEr + BLEU-4 as overall score for early stopping\n",
        "            current_score = val_metrics['cider'] + val_metrics['bleu4']\n",
        "        else:\n",
        "            # If no validation set, use negative training loss as score\n",
        "            current_score = -avg_loss\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"best_score\": best_score,\n",
        "            \"train_losses\": train_losses,\n",
        "            \"val_bleu_scores\": val_bleu_scores,\n",
        "            \"val_cider_scores\": val_cider_scores,\n",
        "            \"val_spice_scores\": val_spice_scores,\n",
        "            \"val_rouge_scores\": val_rouge_scores,\n",
        "            \"val_meteor_scores\": val_meteor_scores\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Check for improvement\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            no_improve_epochs = 0\n",
        "\n",
        "            # Save best model\n",
        "            best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"best_score\": best_score,\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_bleu_scores\": val_bleu_scores,\n",
        "                \"val_cider_scores\": val_cider_scores,\n",
        "                \"val_spice_scores\": val_spice_scores,\n",
        "                \"val_rouge_scores\": val_rouge_scores,\n",
        "                \"val_meteor_scores\": val_meteor_scores\n",
        "            }, best_model_path)\n",
        "\n",
        "            print(f\"New best model saved at epoch {epoch}\")\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
        "            break\n",
        "\n",
        "    # Save training history\n",
        "    history = {\n",
        "        \"epochs\": list(range(1, len(train_losses) + 1)),\n",
        "        \"train_loss\": train_losses,\n",
        "        \"val_bleu4\": val_bleu_scores,\n",
        "        \"val_cider\": val_cider_scores,\n",
        "        \"val_spice\": val_spice_scores,\n",
        "        \"val_rouge\": val_rouge_scores,\n",
        "        \"val_meteor\": val_meteor_scores\n",
        "    }\n",
        "\n",
        "    history_path = os.path.join(checkpoint_dir, \"training_history.pt\")\n",
        "    torch.save(history, history_path)\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n",
        "\n",
        "# 6. Evaluation Function\n",
        "def evaluate_model(model, val_loader, device=\"cuda\", max_samples=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation set with BLEU, CIDEr, SPICE, and ROUGE metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Count samples for potential limit\n",
        "    sample_count = 0\n",
        "\n",
        "    # Process validation batches\n",
        "    for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Generate captions with beam search\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                output_ids = model.generate(\n",
        "                    pixel_values=batch[\"pixel_values\"],\n",
        "                    max_length=50,\n",
        "                    num_beams=5,\n",
        "                    length_penalty=1.0,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "\n",
        "                # Decode generated captions\n",
        "                pred_captions = model.processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "                # Decode reference captions\n",
        "                ref_captions = model.processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "                # Store predictions and references\n",
        "                predictions.extend(pred_captions)\n",
        "                references.extend([[ref] for ref in ref_captions])  # BLEU expects list of references per example\n",
        "\n",
        "                # Update sample count\n",
        "                sample_count += len(pred_captions)\n",
        "\n",
        "                # Print sample predictions (first batch only)\n",
        "                if batch_idx == 0:\n",
        "                    print(\"\\nSample predictions:\")\n",
        "                    for i in range(min(3, len(pred_captions))):\n",
        "                        print(f\"  Reference: {ref_captions[i]}\")\n",
        "                        print(f\"  Prediction: {pred_captions[i]}\")\n",
        "                        print()\n",
        "\n",
        "                # Check if we've processed enough samples\n",
        "                if max_samples is not None and sample_count >= max_samples:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating captions for batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    # Prepare data for other metrics\n",
        "    metric_refs = {i: [ref[0]] for i, ref in enumerate(references)}\n",
        "    metric_preds = {i: [pred] for i, pred in enumerate(predictions)}\n",
        "\n",
        "    # Compute other metrics with better error handling\n",
        "    cider_score = 0.0\n",
        "    spice_score = 0.0\n",
        "    rouge_score = 0.0\n",
        "    meteor_score = 0.0\n",
        "\n",
        "    # Try CIDEr first\n",
        "    try:\n",
        "        cider_score = Cider().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"CIDEr score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing CIDEr: {e}\")\n",
        "\n",
        "    # Try ROUGE next\n",
        "    try:\n",
        "        rouge_score = Rouge().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"ROUGE score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing ROUGE: {e}\")\n",
        "\n",
        "    # Try SPICE with timeout handling\n",
        "    try:\n",
        "        import threading\n",
        "        import time\n",
        "\n",
        "        def compute_spice_with_timeout():\n",
        "            nonlocal spice_score\n",
        "            try:\n",
        "                spice_score = Spice().compute_score(metric_refs, metric_preds)[0]\n",
        "            except Exception as e:\n",
        "                print(f\"Error in SPICE thread: {e}\")\n",
        "\n",
        "        # Start SPICE in a separate thread with timeout\n",
        "        print(\"Starting SPICE computation (may take time)...\")\n",
        "        spice_thread = threading.Thread(target=compute_spice_with_timeout)\n",
        "        spice_thread.daemon = True\n",
        "        spice_thread.start()\n",
        "\n",
        "        # Wait for 3 minutes max\n",
        "        spice_thread.join(timeout=180)\n",
        "\n",
        "        if spice_thread.is_alive():\n",
        "            print(\"Warning: SPICE computation timed out after 3 minutes, skipping.\")\n",
        "        else:\n",
        "            print(\"SPICE score computed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up SPICE computation: {e}\")\n",
        "        print(\"Skipping SPICE evaluation\")\n",
        "\n",
        "    # Try METEOR\n",
        "    try:\n",
        "        meteor_score = Meteor().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"METEOR score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing METEOR: {e}\")\n",
        "\n",
        "    return {\n",
        "        \"bleu4\": bleu4,\n",
        "        \"cider\": cider_score,\n",
        "        \"spice\": spice_score,\n",
        "        \"rouge\": rouge_score,\n",
        "        \"meteor\": meteor_score\n",
        "    }\n",
        "\n",
        "# 7. Main function to run the training pipeline with train/val/test split\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    # Add safe globals for PyTorch 2.6 compatibility\n",
        "    import numpy as np\n",
        "    from numpy._core.multiarray import scalar\n",
        "    try:\n",
        "        torch.serialization.add_safe_globals([scalar])\n",
        "    except:\n",
        "        print(\"Warning: Could not add safe globals for PyTorch 2.6. If loading checkpoints fails, try using weights_only=False.\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"SwayStar123/preprocessed_recap-coco30k-moondream\")['train']\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "\n",
        "        # Create a proper train/val/test split (80%/10%/10%)\n",
        "        total_size = len(dataset)\n",
        "\n",
        "        # Shuffle the dataset indices with a fixed seed for reproducibility\n",
        "        shuffled_indices = torch.randperm(total_size).tolist()\n",
        "\n",
        "        # Calculate split sizes\n",
        "        train_size = int(0.8 * total_size)\n",
        "        val_size = int(0.1 * total_size)\n",
        "        test_size = total_size - train_size - val_size\n",
        "\n",
        "        # Create the splits\n",
        "        train_indices = shuffled_indices[:train_size]\n",
        "        val_indices = shuffled_indices[train_size:train_size+val_size]\n",
        "        test_indices = shuffled_indices[train_size+val_size:]\n",
        "\n",
        "        train_ds = dataset.select(train_indices)\n",
        "        val_ds = dataset.select(val_indices)\n",
        "        test_ds = dataset.select(test_indices)\n",
        "\n",
        "        print(f\"Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}, Test samples: {len(test_ds)}\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize enhanced model\n",
        "        print(\"Initializing enhanced BLIP model...\")\n",
        "        model = EnhancedBLIP(sparsity=0.7)  # Keeping your preferred sparsity value\n",
        "        model.to(device)\n",
        "\n",
        "        # Train model\n",
        "        print(\"Starting training...\")\n",
        "        train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=7,  # Set to 7 epochs based on previous results\n",
        "            lr=2e-5,\n",
        "            device=device,\n",
        "            checkpoint_dir=\"token_gating_checkpoints\"\n",
        "        )\n",
        "\n",
        "        # Load best model for evaluation\n",
        "        best_model_path = os.path.join(\"token_gating_checkpoints\", \"best_model.pth\")\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(f\"Loading best model from {best_model_path} for final evaluation\")\n",
        "            try:\n",
        "                # Try with weights_only=False first\n",
        "                checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "                model.load_state_dict(checkpoint[\"model_state\"])\n",
        "                best_epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
        "                print(f\"Best model was from epoch {best_epoch}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading with weights_only=False: {e}\")\n",
        "                print(\"Trying alternative loading method...\")\n",
        "                # If that fails, try a more forgiving approach\n",
        "                import numpy as np\n",
        "                from numpy._core.multiarray import scalar\n",
        "                torch.serialization.add_safe_globals([scalar])\n",
        "                checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "                model.load_state_dict(checkpoint[\"model_state\"])\n",
        "                best_epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
        "                print(f\"Best model was from epoch {best_epoch}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        print(\"\\nPerforming validation set evaluation...\")\n",
        "        val_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(\"\\nPerforming test set evaluation (first time evaluating on these samples)...\")\n",
        "        test_metrics = evaluate_model(model, test_loader, device=device)\n",
        "\n",
        "        # Print validation results\n",
        "        print(\"\\nValidation Set Results:\")\n",
        "        print(f\"BLEU-4: {val_metrics['bleu4']:.4f} ({val_metrics['bleu4'] * 100:.2f})\")\n",
        "        print(f\"CIDEr: {val_metrics['cider']:.4f} ({val_metrics['cider'] * 100:.2f})\")\n",
        "        print(f\"SPICE: {val_metrics['spice']:.4f} ({val_metrics['spice'] * 100:.2f})\")\n",
        "        print(f\"ROUGE: {val_metrics['rouge']:.4f} ({val_metrics['rouge'] * 100:.2f})\")\n",
        "        if 'meteor' in val_metrics and val_metrics['meteor'] > 0:\n",
        "            print(f\"METEOR: {val_metrics['meteor']:.4f} ({val_metrics['meteor'] * 100:.2f})\")\n",
        "\n",
        "        # Print test results\n",
        "        print(\"\\nTest Set Results:\")\n",
        "        print(f\"BLEU-4: {test_metrics['bleu4']:.4f} ({test_metrics['bleu4'] * 100:.2f})\")\n",
        "        print(f\"CIDEr: {test_metrics['cider']:.4f} ({test_metrics['cider'] * 100:.2f})\")\n",
        "        print(f\"SPICE: {test_metrics['spice']:.4f} ({test_metrics['spice'] * 100:.2f})\")\n",
        "        print(f\"ROUGE: {test_metrics['rouge']:.4f} ({test_metrics['rouge'] * 100:.2f})\")\n",
        "        if 'meteor' in test_metrics and test_metrics['meteor'] > 0:\n",
        "            print(f\"METEOR: {test_metrics['meteor']:.4f} ({test_metrics['meteor'] * 100:.2f})\")\n",
        "\n",
        "        # Save test results\n",
        "        test_results_path = os.path.join(\"token_gating_checkpoints\", \"test_results.pt\")\n",
        "        torch.save({\n",
        "            \"bleu4\": test_metrics['bleu4'],\n",
        "            \"cider\": test_metrics['cider'],\n",
        "            \"spice\": test_metrics['spice'],\n",
        "            \"rouge\": test_metrics['rouge'],\n",
        "            \"meteor\": test_metrics.get('meteor', 0.0)\n",
        "        }, test_results_path)\n",
        "\n",
        "        # Compare validation and test results\n",
        "        print(\"\\nValidation vs Test Performance:\")\n",
        "        metrics_comparison = {\n",
        "            'Metric': ['BLEU-4', 'CIDEr', 'SPICE', 'ROUGE', 'METEOR'],\n",
        "            'Validation': [\n",
        "                f\"{val_metrics['bleu4']:.4f}\",\n",
        "                f\"{val_metrics['cider']:.4f}\",\n",
        "                f\"{val_metrics['spice']:.4f}\",\n",
        "                f\"{val_metrics['rouge']:.4f}\",\n",
        "                f\"{val_metrics.get('meteor', 0.0):.4f}\"\n",
        "            ],\n",
        "            'Test': [\n",
        "                f\"{test_metrics['bleu4']:.4f}\",\n",
        "                f\"{test_metrics['cider']:.4f}\",\n",
        "                f\"{test_metrics['spice']:.4f}\",\n",
        "                f\"{test_metrics['rouge']:.4f}\",\n",
        "                f\"{test_metrics.get('meteor', 0.0):.4f}\"\n",
        "            ],\n",
        "            'Diff (Test-Val)': [\n",
        "                f\"{test_metrics['bleu4'] - val_metrics['bleu4']:.4f}\",\n",
        "                f\"{test_metrics['cider'] - val_metrics['cider']:.4f}\",\n",
        "                f\"{test_metrics['spice'] - val_metrics['spice']:.4f}\",\n",
        "                f\"{test_metrics['rouge'] - val_metrics['rouge']:.4f}\",\n",
        "                f\"{test_metrics.get('meteor', 0.0) - val_metrics.get('meteor', 0.0):.4f}\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        comparison_df = pd.DataFrame(metrics_comparison)\n",
        "        print(comparison_df.to_string(index=False))\n",
        "\n",
        "        # Save comparison results\n",
        "        comparison_path = os.path.join(\"token_gating_checkpoints\", \"val_test_comparison.csv\")\n",
        "        comparison_df.to_csv(comparison_path, index=False)\n",
        "        print(f\"Comparison results saved to {comparison_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main function: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "1094eff4b9534d579fe0dc46e005d1d4",
            "b5bd5ec1ce23459c86ac7d7df8fd193c",
            "74d2c4ee20444fef86d36c028cf4b8a7",
            "e896b20e8da5456e85d735a71d89188a",
            "77c60c0c7118429b83e3435501b28fc9",
            "970eb025e35f4c46bb5a6c42582b46a5",
            "b441c42b31114730b562ac0773f29590",
            "bb6825bd5a554b09bb1e814964b5221e",
            "9f1a033db57a48fa93d125cb539466a4",
            "e2ecaa631c954122983e9216531f2624",
            "e1b21f13e1eb40f1a87ef62f463a98cb",
            "f1275e2ec456476aa0cbb13029aa9e4c",
            "7b0b8e60c1554e1aba266016f7ec4a95",
            "364e7f9b372b44908f93c9139b56c328",
            "396925ac292d45dba280d086fabaf3db",
            "45f2c399a2ad4cb0855558dce66f9291",
            "323f4ae5565c483585d59d7ba6c93b99",
            "827aade4ff7d48c0a71c5e406c160031",
            "b709786fad024eceaec7e367bd906fff",
            "c238e077d88845e3944902d03a90fc48",
            "5d0c97203a844a4592755a7d7a85eafa",
            "fa1bd4ba7bae44a0b44226440232c958",
            "ce99d589257644e9a70d4fd51db16942",
            "617f9615366c4289af8574295ec27c9d",
            "48cc8a42f2ab428b9e561fe4151e9490",
            "a72fd1081b6e451e84d0f344b090f434",
            "20d91318d26f45f6a2bfa7faff101e38",
            "1df0e11f4ea54cb0aa0e52b776909417",
            "ce5b7ba997f446e7bf8c793040ab1a66",
            "1686cb77c48d4695bf03775640798758",
            "cf5b0ddc2d644bd1b97649a467640a1c",
            "a0122f539abf468f8a755df88188f001",
            "38fe10c29c414792b2bac9e5e26dde00",
            "dacb409420d2435ba0d2bf30f7904ee7",
            "cb2e2712d56c4f6198c0cd9ca1b73f0a",
            "7a3dfbc2b2194342a5ff7d5c4c03b603",
            "a7f8611287614fc8b373ae23e543d7f9",
            "cd75951a18654e0a90614316749cc972",
            "92d7a48f4e3d4427a112cd7c0148fe31",
            "834db1d44d86403f881a15c93fab86ba",
            "c3a726f0a5744d0191d83c50b389d56f",
            "73c3cf4f65b14354a9bb8e45cffef4d6",
            "07246db2933b4d0e8cbd2dc5f1a3f37f",
            "e5ed4d9b9cae45a28d0aab143724cff5",
            "97d83c9f31994a4d8f9dee8593071a4b",
            "d87f508036324cbda0583d8601b99241",
            "f4c89d81a3574bfbbd1b1603533a3bd9",
            "7aecc76e422b41c89b05beaf1e47f124",
            "299745688ba049f2bff2506dfd47f3f5",
            "8d97d3a43c804517a156c111ff6f2ed8",
            "43310cd1a0cd4f26941d8af2c9ba5531",
            "bd0936c2349643118724da37387b1c80",
            "cfc21cad8e774969aa0afc85752aa372",
            "a77675bb16cd4a70a4dbacffdeb6f1c1",
            "ec3367053daf47fabf95af4f04e73b06",
            "e818f16ef7b44c938d78ee13b934f727",
            "ae0325e3dce94f589ba6125594a57e49",
            "2d46f18418144d65a1c4b2e157a5249e",
            "98aac73a82624b44870631f54b88f06d",
            "b8e0032d62074e7bbad97dbb1874c4f8",
            "ef7be4f341b446c1ab3c7a333981f768",
            "a0ab31690f23447fb1d5203349c2c5c5",
            "f520a8a7a5704c599a25c6aad8a63dee",
            "87f5b54e5d944a0c9e40f751094be5d0",
            "18edab456420401dacc50b91376b2a18",
            "57050245f0314983a0212f78749741d0",
            "ee79be65645741a5b91efab1c5288dc8",
            "674d5eda6c534c77aba018aca0c143d0",
            "11ff93b5931f40c085dad648dc90ff3d",
            "1a47efac6fdc45b79f6b06078515d7dc",
            "1b8c364964f140a5b49fcd24a1cadc29",
            "1087a526dad64393acb586ecc3ac72a3",
            "b5d7abffd5734cad8233440d8c282a9f",
            "3b52b7df535549cb8e175c0f845b2b68",
            "fbb2844599784a3783ddc64ca76c8692",
            "98921bc212a54ffca12ab4717673136a",
            "b0fb514d38a04cf7b01aed549c62ad3a",
            "523d425f5a5b4998a387b60d6d22e14a",
            "529c4814babe430dbd83427a66fb2f88",
            "01a24f97f43145f8902e0ddf0cfcfbc5",
            "484398ae49054318ad44efc69b0cf839",
            "6f149e62c3d648128b1ab9081aaa8a19",
            "c0c230293c4c482997165c2e272201ba",
            "50d9c49ec55844289523425876bd4525",
            "d22449bb756a4451a6d798c638a7f10f",
            "5e314045ddd14678952e5b1f42edc254",
            "1d1f66c484534a909209d126d35b5916",
            "3b071df2a4ac4396abdfa4f65d02ec68",
            "1d26cc66a6004062a9ae80625ac9f309",
            "efca37c05ee8434293f0367094600176",
            "047f04d9226843589e82f3eca40b547e",
            "f34e219d73fb403e8b2bbeeef5ca1a32",
            "068d1c35da1f4b94a0aeb9470ea0fe13",
            "e35b5a44049245ea8edff3cc05312ae0",
            "3eea8cafd40f453598f6ee649746fdfb",
            "b5e01d0cdfc74615af89312ac8be2457",
            "d64cc7a2ca5e4d718a44301d14a54eb6",
            "97262553ef944c7aa356b7084262c392",
            "e4cbd989e295466586010e6fca617477",
            "8162e18885ca447098e1a0be81fb8890",
            "5207cfa86e0348be83a66b522624b261",
            "9190c7405de141d5bcdc76f653ce59d7",
            "0ffaacd7753840a2979f43a81debf5b2",
            "1d4c99efcda34748a83b6cee50cf7b4e",
            "8f73977ab45f404bbd111e5b253cd989",
            "341369bdd10543df8a72c4d163d023c1",
            "458e6ed8f251416aa8805d8588d78b0c",
            "d82a98d8edf04dd08dea35973d186595",
            "ac8675f1a17b4375b4f4c14d75c67997",
            "2ea148211b76498ba33ab0fd725e72a1",
            "ef9ee196f5c54cd5a69f6affb484ca25",
            "f500c61aa9a5489da4d57d0556e454c3",
            "2b82eb04c82e49afa8ec45386e69c4f5",
            "94db555c30f1432486fc62634e0375fb",
            "ac4de8781a814ea0b60c126910499b77",
            "6fcaaa26c1eb4c469e3792b5189afbde",
            "2f62d3ffea024ef8b736a578c51334b9",
            "274d33a2ce5e45e399db4bb211851c9a",
            "bf5fde63a3e74496b093bfb743074a6e",
            "3fcc503f07a34037833d315a7ceedc8f",
            "933034c5fcac40a6a4eafcefffcea272",
            "c2a7fc319d5343f7887061816d0862ec",
            "889b571c741e463aabee2f37a20a18cb",
            "647d594182ed40fdb638ca5527a60b16",
            "89158687d6e1469ba6740b0ff334346d",
            "03134f20d17c46da899ecbd9e2b82171",
            "7159a233c65c4d07b4994642b25b8aa5",
            "54b1bda1d4e6499b8fc8144e1ab226fc",
            "208f6d6f3cff42a6bacec8cc0b1ba702",
            "2592cac144a24523b6b6fe3abde72f53",
            "0dc8af5642734d4380d849dfea7a8ed6",
            "8c25cf01ed65400ca7295cfdae4985c4",
            "638fc86f8b2c41bcb8c0270a4a8509b7",
            "4a2c6a0f0d884f46bbcc44f6abeb1a39",
            "23c5bd042ed7404ca328ba820ec3e00b",
            "2ef0bde35af5456ca97e2d7d0417f7b4",
            "020f01d70b7349c9b0e06992cc4b67e4",
            "3ad26fa8a2d54d7c8767bb4ef5500892",
            "28b1efad2b3c47a6908926535eafd56f",
            "8d27cfe35b1443fcab2fb3d0cd215a09",
            "7f7cd136ec474fce846c963ba453bc74",
            "1a5dd36ac4304425a98b30112ee46ad1",
            "bb9773d3b52442ba9fbe0146e3cdef28",
            "1bf00eef33324bb4a1c19f25671a403d",
            "f561476702414714a902c97932f66457",
            "1ae1fe948d5c49ccb68ba8cedb2705a1",
            "87d3494fe54146c5861c3bee844ac42d",
            "8361e0b932224f57b89bfe79fbf73e91",
            "0d3470e04527427e909d94e0e75f0f8c",
            "05f62c76cfb742188c9775dffd899d5e",
            "5568312247bf449fb8bfd6e1c4fa4bba",
            "2371bc71a5aa43f38908fe934cd55e5b",
            "74425bf14d0c448f831b39fa146c73c1",
            "3fded6fc088144f79f80e772be67842b",
            "8f617184e8cc4bb093e3c21369cc283f",
            "98725199615b4c6f86dbc38d625864c5",
            "76d1ffd6486b448faf4342fdf25df88b",
            "526169e1df054aa9b72ea9e4639cd292",
            "69ed402942414e9f889e1906ad1b9ded",
            "761063d33eac40a993694d05eb9cddbc",
            "9b3b406529e645dbabc14acaf1cc837d",
            "018b619affbb408b80a2bbae1dfc5471",
            "bb700f16e1b240669121b0ba17767c80",
            "ec3151a217c44f55824112a10e6964b8",
            "941f92edb9b64d8baffa14759cf3cb5a",
            "a34571070f1c4b31846b77e45f5716bc",
            "0d118c752f734e7181899e5ec38f1ae5",
            "60dceed42a7b483d89773c0d0dea5ebe",
            "23d8dffce5f343bea0f149484630e1b8",
            "0aa55eaf5d464b08b3c1201770ad2801",
            "0de4b82b8c1a43c0bc4e7cf0d82e47a5",
            "140577a091f14ba18c882f2c9fa2959d",
            "5223a41ec8ed46e6a96bb9efcb11bc3d",
            "c2f2d48e8bc74ddbbceed29e10637a95",
            "c2f8bc3919754202afb613a88b2c43e1",
            "050018f277b2408f9c60c4647745bb12",
            "d167ea0538134ad6bf5acf35be24ba79",
            "aa7476cb4e95491493cbf004db76ea7f",
            "270ef1a4f7504ac7916b9cec73473e70",
            "b05375eb85a6449183275cab99c8158a",
            "9357af46743d42698be72a50220c9938",
            "bebb3bb77c09439ebe897a69369f2a36",
            "2298d13d0ba34a119f0e773dfbbb8174",
            "ce753a06aacd4776bff611ffd41f8ed6",
            "0ed365b88b8f46558810f5e78aca2677",
            "51a4c21f26d8453596db7e33b2b28a37",
            "743067b7c4354ebea8845545019016ee",
            "b65cd516f8e24ab698380b7836cb35d3",
            "3c2bfec4bba34aa196aa28e2ccd60001",
            "240430662feb40e99311a804ab50f9a1",
            "7707d73617924b3e9bc501c82132f0a9",
            "0bcded45a795451b9ac75a2a1624bc65",
            "9532a260503f496caa506fbaac74ee1b",
            "9fa47bb6fa384e2ca49a76db5fb62df0",
            "eb8a7b75df29498aa4ec7557162d8dbf",
            "e4869911dec84bd0a723d260c164fb12",
            "48f9dc4f7cb648e197a3c6906efe462c",
            "132593a6baa840308cd57cc931194548",
            "9e8edb670d6b4cb3b80f040e0923954f",
            "6b88d636ee9e4a96a06049fd44c5c5bd",
            "c3929947c75f4b8fa85a341c911352bf",
            "c110698b1c494c01b2db6bea319b09b7",
            "7e182d877b664e89b1b3ece5670f8cc9",
            "2e5eae33f9754f60b1e2a6e5425698b1",
            "33457b2f15fb486086afc7fff4b03f61",
            "4aef80b37c3b4ce8b4cd022ae27b66d3",
            "c0c2446847c74a4db787bb44f01f4658",
            "6753d1b032964182907d4e34e4f80a92",
            "95d3db81d6064db7b2a4595cd8dbb8bd",
            "c7329290ba7b4dd987652f6d9efd9a8e",
            "be71af31814147e785f85b75ef511c39",
            "ab8f2dec38ee4a09b285f9794f1923c6",
            "74e3b7ad810542728dd68b68cd477a0f",
            "74abdea87f2646abb69a8a8e579f4570",
            "bb738f49821643cba2dd19938807e97e",
            "796d40dde2d144749742343019b656fa",
            "05bf557add1046c880e7edec13b146c4",
            "c82d45308a184075b233cf4fbd30421d",
            "8d3e4781ed4c49368385e4131081a67b",
            "adf405c126c2434d892adb056e8a2d9b",
            "9f98a512989649d5a97bbb49390fe819",
            "ae08841b69ac4472ab1d0ad49ab91138",
            "807197ae948c45f68c01b86e500e0941",
            "e73f0d2d01c54e8e849f2c4c32209b8c",
            "fe6b2d8a2eeb4e74a49314399843b5d2",
            "8aa51d6fb8d940e9a53d7ddabf383945",
            "c2ec234887794949844747c0420c95ff",
            "64345acb62a94538bd3f43986a6098e5",
            "014519f225d04cada10ea02ad77bd9ee",
            "a5eaa14c5aba41e081d1aa83e6c47bac",
            "51405609176e41d0a658288b20995d2f",
            "cea15b6fab9e423f85d82d1f8b0d2e76",
            "c3fcd233026f4b91afa76a9baafb1057",
            "d3a173217d12400295eabac8b975bf0f",
            "53cc8c68d9404eeba248787e353f07bb",
            "68301c4b49f64f2282f20caaf2202b39",
            "b6a75b2ece9942cba4d8ab96b05109d5",
            "3f25c7e6bc194b8b94f1584ca48e7135",
            "15ec401d945a4445a92ce31712dc3a3a",
            "a6dea3cbc2bf4218b56ce1cf0f5fb7dd",
            "cb072d9e47c94c238643740d017b9340",
            "77824f1b9a2140cf8c41767dc444ff2c",
            "5b15cb67c9a5487e8012997c1003986f",
            "6c54922e9413415bb7bc370014e3ad16",
            "0102c768fba844918408b3b1b2d15c8f",
            "0b2df3709e25432f9d59d2c86a9e671e",
            "dd159d4c2e0d4f55b4438162baa4bf5b",
            "da9a0160a26f4c1cbdc2f633c0b357f6",
            "8349a362d30346c3b91ad33e9fa4fa0a",
            "5ef4a2d4061e44d7bc0fdf6e88c71a9e",
            "39dcb021a3d14420934e1a86b9532bdf",
            "a3862ae16ecd43b48a84e53327060e44",
            "6fd4b75ca2ca4e4ca19c2d8cffb33afa",
            "bb91fff619a44ace96e6c229d721f328",
            "1ab8b2b7f0534032be013cb20b5b2d2e",
            "999918579b604caeae5529caa161ac21",
            "0e89671b57644450968ff4ac2664895e",
            "71638b7f71a14cdc88a86dae0e48787c",
            "a2e38db7ae6e411f870aad242efd25a1",
            "afff04e4f09c46daa55b124225f22532",
            "02b7b79b3e8b4ddfab78dc5c32920b12",
            "29a5164d315c42e5992b4284a2a80cc8",
            "86f4c92fc1a240c08278e66a4081bffb",
            "4b9ce876427642c0ba881a412d8469c1",
            "5a005ca3c3f040d9946bd33a8b77dfd8",
            "81098c8a8cf54ecaa07749de8274a707",
            "e18529e7e88b4c4db030366ef48adecb",
            "e4357e034d524d10baa3eba31478db1b",
            "43831a5be9344c2a9bb3a541fd604a87",
            "d96bce18e4024fda93bbc8a29e8c9975",
            "54f1e201224740058f04fbf61be4dab7",
            "8df2813b88fa4cb1976f71722b52f49f",
            "83e12b02f46a41db85392fa27722d8cc",
            "d20dab8e09104d259102a88983e32c6e",
            "f6a9fa21459c48ab8825a68d731d3de6",
            "941d4b87e2f246a09ad4d8e4b4074769",
            "1fbac85b39bb4f46a9990ee9ef3d2802",
            "da98b6f364bd4db8a38e03b2c2590a17",
            "7a52b0d31cb44e7aa45fd75d3a3d2372",
            "7191b84d123841f0b03864fda29a51a0",
            "30fdd76de17b485aa931daefcb819e27",
            "00f68c881fcd4019a25daa88f22e9cd6",
            "48a3870b1c4d44a98973f8ae55e3eb1d",
            "01541857f55c4e719403b6329ecc689f",
            "aa4467efe0484bb49583b6bfdcba6495",
            "2c943b2dfc5d45e7b852b69cd249cb54",
            "8512dd9a766447f1bce70078ef1d39ef",
            "6f00d6f6e1564494aea1dfbfd539ced9",
            "8e96a46c1cbf414cbc55e1aa0166c05b",
            "c11a19d8f1b44079b6fb0b351009f649",
            "1264cbbc825748f1b6f26e304bf71b9a",
            "deab3e44855b4d0c80eb754a4a2b092f",
            "50717b078b9e412aac90d4736d794b8f",
            "eee55df3bb874734b23d8fe6103e2122",
            "bf764c01758d404fa1ce0be96502cdf1",
            "0c175cb572634ae3893b8ae3c85a600a",
            "259ad14589204ebba36610df192a678e",
            "58d806b04935410f9328f575aa5f2cbe",
            "d3f4e3cb0f304b64a7dbe23f54cd49b0",
            "52c50729bdfa44259e795c1b1582f659",
            "4687e7158f494f089ab6fb2794da4bcb",
            "76f8a722dafc4cc3b461cb269ae324d7",
            "e3b3b91d89344d1997e44ab576f388a1",
            "5573df89a44149159d42fa58af16bd0a",
            "8a34f6afb2e349f4974ab96df4c2e3e0",
            "ee72a49103824e17a734ad8cae724275",
            "7bb34f3921ec43f6a0a74e6ea021e1d9",
            "a71f6b9ec9e3457ab01e194b2e20f0c6",
            "bfb627245084412c94067efbfe89a1f4",
            "385291a9d7094ca1b88e4612dcda319f",
            "314e7cecdb904d4eb81d0a22b2a44efe",
            "40755a92aa2c412d9e9e9190b6384e6b",
            "2717f27b343e41c29cb6dca2b713f235",
            "59ffbbae33c647f8bdcc87630f5f1167",
            "8ce4d7c86d204422a11b4e98985f6635",
            "721c2eda5d744b2f851892c5e62ae665",
            "8f645b1072494836a6ec5851b4adff58",
            "ecaa271b5ec441838bc714be0b2f9127",
            "ed5e110062f1412699b7cec6eb070217",
            "86162979242447c9a01d884ad98c460a",
            "68707a463ba24a058e8e91b4d4c9d145",
            "29d0b52cd41347b98cf8b4edc2d4303f",
            "0448a352dc0f4d2399308f1bd4a57dd0",
            "41afe187e46245f4a1fa1360d6c2e303",
            "9bd22e83f2674b4fb3eb8769d12249b1",
            "2c032a9be6d3463ab9f8806783154232",
            "a2e3cf787c0c4a5a9260e327dbc61477",
            "7de1ddbbe07c42cda7789d22cfa48b28",
            "f4160263ff66492daedd712e0a58dfe8",
            "5c2e8e08448f4c2389e90fcf144331ae",
            "344c408390e84ef98d32afa2c28dbf91",
            "aed9a8ba5f864a8db11984cc23fe571d",
            "64795415cca145e88966be890e9f011f",
            "b1901b4056c242b9bb3b3ef289d82d56",
            "f3f14e0204af4b3fa8af5d5372a40986",
            "db48947d0c7144d8a42bdc7593ff698b",
            "5a341aaffc3a4c039acd4b1253b38d08",
            "02eec3c6acfe411dafe0038d321dbd4f",
            "6b82aaa9345a4bcfa7dd3bf58b5d3ad7",
            "1b2b6c001ac84c1089499c64a15db03e",
            "caa2e49479e74bb2b0db96ba5765b551",
            "eaa6aee65bba42be821d8278da6cde72",
            "7dd25ec4ef9247cc9caf4ae94a8de375",
            "71d436336a91453681bf6bb28a123514",
            "6499b7b2b1bc4066be34b463b6f357a7",
            "c52558246d9b4394bfbee36b7104ab3b",
            "1597fb99cd0f40a3a1cc395ac62664d6",
            "a549c006bc894bd487466e613ba45fb6",
            "126883e1e53a4523bf725855ab699c79",
            "f0007f48421b485e8747effa0dd36433",
            "4d8be19db7ee4933ba916b893974601c",
            "df5b4cd6b29140e4831a133bbe135c18",
            "2821822b0f47428bb9300f5a477fbda3",
            "3dfa326a63f74a87a7ae00508623a9d3",
            "e60a507ee2eb4568bb211c6fb9d951c4",
            "29beba2f7a5d419199ca18a66dd3263c",
            "59bc8befabc642f3857adde93186d789",
            "2eb6660fa1e34ce9a0a74a3775640287",
            "2ed96f89bedb430c99b72a56e5e05a41",
            "2e2b080673a94b57acdebafea7fe222e",
            "747d301a804b4cedbf153c9e050ef519",
            "78843151c64c4b61b3e0c9560a2bcd12",
            "09597f29b5c343fb9bb2e4fe646b6ebb",
            "74a1666b14bf46cd800662447b2c76e7",
            "650496f517dd4763899bb9553bc1efe9",
            "33977a4772b34e22b419c81f9d20ba7b",
            "edd29ef861974a6b9eac277800660ef8",
            "a1bf799be68d4b178af34227d3a773a4",
            "04f2ce03d3e1489491209c3158ec5d71",
            "ec80e0be3ce54cddb7708bef495b4697",
            "6b21e1e6b1224d7aade98e9290f72d16",
            "f4d0bfaace6240f5bc5ab7a41e9c63f0",
            "cda33c7563cb40f88b3e4b094a4fe44b",
            "dc090adaf0384688b6a0d711a56182cf",
            "dcfa211dda944bde9db0b8340dbe9a06",
            "8fb15c1fe94e41e4aebf93da0a58475d",
            "5f0d52e436ff4608979518a78fa66043",
            "72be3bbad3cf47d2bbf463307a26e22f",
            "1ee2718ba98e4a4997de8f28b39962d2",
            "2f9a3f5cbf584823a0cb71e5d5e189ce",
            "81b6f2f1a5c64447b48c607fcf2e976c",
            "9108637c1a1f4418ada2c526e37a8cb4",
            "dd5b43288a524b0ba569022a9b1e10b6",
            "6ba52a20ac1647c3b0a72c89b71ed3d8",
            "f3918989980b4ff080667807263a3ece",
            "473b7f1784d546d3b4140d4f6ad27d68",
            "9269f0d6e47f43e5a34ce0e0d34c2c90",
            "4d5f269c25014a40b6a777efbdfe3aa0",
            "15e815f86fec44f5a415d87b782cc9a8",
            "d79a8458b92f4347869991a696b183ab",
            "d940198084224390a0ac803ea1dff323",
            "b0399c7ce418439a864a20e98c2030ad",
            "f160d844efab47f9830b871b6fd055f4",
            "10b55ff045b54db0aaa2fd4143fce05d",
            "d0c2c38843ca45e698bcaf0ae23ebbc8",
            "ecac955b0234438cb2b7ebe9a5ad8f9e",
            "b5abdd6f8a674bf1a7f0a1bb261b6176",
            "8d96e8848841423494f12ef16fd78346",
            "7d5f3b41929e433cbff773740a096093",
            "95da265d83d740e7823bfb44b85d70d1",
            "1451bf76fe8d438fb8a3c2025a14f1a2",
            "0ba13fabec524b00b2936451c5d554ba",
            "b376649b2d824249ba746788697ed4a3",
            "688f9c847e984989b8f58a9d10ba2265",
            "d601fb26789745b2b967118676fd71e7",
            "d9deb9f2782b453f841fd6f4247d381b",
            "71916380a6b54ca19bd73af7fae018de",
            "863dc18b88d1488fb5d62b1091a1d429",
            "7fd980e1a84041daac4711bcbaafeaaa",
            "e4a058a487e14abebd9203a4c4e51598",
            "8ced9f3cbbf24387bfa5b197ef964c96",
            "8a223f55a02941889112ed8993fff367",
            "a69fd086173c4f0c9c65c237ed34bba8",
            "b202e9f6c0674389986c713930fbdd27",
            "bcdf9dc85b10470294dc64b9ab3100ff",
            "9bbe0940730d449e88a703c45dc2a5b7",
            "1c3ddf7311e3486e82aab76eb6619698",
            "34fabffc222b4fb088ffeeca7d70b3d4",
            "2e29b80a61cb4f1eb17dd2fbc0cd7d5e",
            "9243b52c6a8f4631841e1c6c44819ac5",
            "ccc490be680242b7acc72f5a8902d10a",
            "bda100b05c6d45109c5676a83aea5768",
            "ca5da5fd962d48a7b21600f7c5719046",
            "0ae80993e7cc480584fcbad94761f4ab",
            "eb6c897ff6324621b75f1022027523ac",
            "7fa5cc4ec8bf4031a98a7dbd6e4d1795",
            "35732ad77ca64c5c800bde561bd27ef3",
            "bf6f2fcb91bb49ad89e47bd4f69c2863",
            "f33100919c2f41ac8c6be34853c350c3",
            "1048cbb933b34a6ebd314c39a4fc08f7",
            "dc08455d3abc43cd906be0a1d3354a5d",
            "1bf502291e92483e811cc12a9f230e79",
            "3e65490a7c294ed7bb7cc7b4593ed5dd",
            "74107732d6ca47539294b99b9a2c3932",
            "4caead68f978434f933aa253f721397b",
            "b311d1496fe34e78b5dd75a3825fd17b",
            "548b5392058644f0bbefc78f6e4fb236",
            "46e4e6a1562b4a429e09e3ecbc7223ca",
            "7d2070d0f561492c8207a9f4c15d5e0b",
            "0c3a3ee960b1463aaa5d6c65a455d28e",
            "e27864c6fa2e4554845074998adeae83",
            "65aaf0e090cd45f6ae6b4f1cbe40acf1",
            "cfb67a899a9d4021be702cd43c19047b",
            "d2f95eeecf4040e68b3f0a437c3da326",
            "b37e0fec56ce409dbfaf157171dd1c6d",
            "a38c147acdf44f448fa88a6b447351ac",
            "6b30ee7448374884bd9d40e818138f2d",
            "10b300bc770043c1882a5e2b20e794a5",
            "725deb4b09fa4cd4847a130eea3a71c6",
            "70cc56ee29c649daa6aaa17d9cb78f97",
            "aa3e2ee8ee484ebfacad79929833b47a",
            "5832022415854322aa0982df09da682f",
            "dac804edadb84f7d8427b234a9bf19c5",
            "7bbf3beace5f49da9dcd7cf24df6c62f",
            "e73ba46d9f6540208f3b8f2a3ece9b7e",
            "4f28cda37c764faf9a288473689a2f0b",
            "dcb7697738614c18ae05c61efcf3da76",
            "b446fca4fe0d416383989cdc4a70ada8",
            "0c74f5be48d54ae08ff8fa17d7c8d399",
            "f01c4cdb26d848ed91fd1abf5ea5c645",
            "60e5164965ee4fe78bae26ad4f4a8519",
            "13915e0a8bf04e8aa761e72be7410864",
            "7abce59d394c4ac99ce9c011ced776a9",
            "5303dcc6efb74d55a2ab798511eaed7b",
            "fa22c26dc78446fbac9af6a03310fb76",
            "5fd5171148ee47cba48ab8b6e554192c",
            "8a398bb8c5ca4754b4f26cd0a2c1fc38",
            "599ee3049dc54cc9b8b083e6709300dc",
            "07363a808d5d4c31a05b98b8875e6ad6",
            "370e547e5e39488290572c193122aea9",
            "04bae061ae6a47c3aa0a57e4e8d7208d",
            "6f48aa27141c4af7846b462d6cc93678",
            "9e57439ae10b4eac84e0fb865b075edc",
            "53470e565e0341e69e3b1665a4c9a47d",
            "1db9add8e68b479db7a96b0d2e58151b",
            "b00975c592684fa1b9a3cf037ad86e03",
            "457db9e43bfb481ba7e3bd6a244cfe38",
            "11300d409f664c9e91ec279e1538aa8e",
            "040beeb1dbe74fd782e6983bc590c90b",
            "61a6b4f5170944ed96540a735d327ab9",
            "e6a309271bfa482ba46e7bd9a4adffd5",
            "3f2bee3854464c37a7fc8c0965ea8bcc",
            "b625352e84c441d7b4f153da6429a9d8",
            "938b6763df384593a7db1d64660d15a7",
            "3e9a2e8818534c9baecdaf57a344a02b",
            "3ec32f4f4a3741918726b1d30b5b2dc5",
            "30767c7d02c3441d9ead158475bff3d6",
            "9dfbee97bc64469fada86206c8c3b12c",
            "e6475c9f754b496bb84272b4056f4801",
            "e33c37d3f36e45159470e023fa06b5f9",
            "a50277e8faba4a10866e1aa864b57388",
            "e797b91902ad4b8682641698e66a49a9",
            "068132cd12bf4cb9b659923cc97d2389",
            "eec71740d31844329289157ea729b232",
            "09fcf640847f4a47b4eee47777e70f27",
            "6c920de5dc6e4d7ab99b0798daa7fd52",
            "63a5aaf0a5764e27b7df6fee6e4448f5",
            "c9046067e4e34b3c9ff0d8ff14a1995a",
            "6479e74c1d3d48f793877210a921186b",
            "c868bcae86d24e13ab1be26c00bd7736",
            "4f4d20fbb7944f07847c6642436ffb0d",
            "d4474471d65a43d1b5331f1136201498",
            "ebea1341d69d4334854aa3b954995c88",
            "80ae1e6b178e42a8ad035b9cbce1ee9a",
            "d746fa3b3ed746f2bd358fac683de3a1",
            "62ed9ed4038c47de864037bb2d9c9ed2",
            "241e5792e7234579a52d33f4a8edad9d",
            "0919bc93cc16414697da8cd8c39752b8",
            "f0bfce492cdd4be29dd72bb3acaaa2b4",
            "dcf53e5dc3cb42c0886c73ad2c8cf440",
            "3879fcb693e74884826154e3e2e02542",
            "a8f1ffeef41347dca159dd8bb1671891",
            "95fdbd9d2e714fe6a3e9c2c2b06932ec",
            "e36749e1b54444de9ad709c13f88d4a8",
            "2fb7a51ebf564d32b22242e89c41841e",
            "7372e7c240ac4992b7559c0db7b15e9d",
            "cefd085b7c3843d3b8b8d53dc4739b27",
            "d201296fe744466b8d090a0ec805007e",
            "39b3a4fda44e4fbca5968164e0727635",
            "4de8f26f31384027aec1e13cd8f5f342",
            "49214fc7cd864e2f9842b8c2942a2182",
            "01c0fe88f77f456585c79e59a3166d90",
            "8aa40b6ecbf942ad8f95a5659cb1f025",
            "0dd7ddc2e2e348f989e998076903a79b",
            "9497eb411227439893cdd23aaaf8fbe5",
            "58e7c18ae97245d5a62a57333cad9cc5",
            "c6a4d28ea5d845dbb19ecabb506d4e79",
            "1e36c69690ef47bab4180a02040d9a4b",
            "8a54a9851fa14a19b50bfbd38800273f",
            "ba1e69412e0643e78735254b41ddefb8",
            "1aedbd27237243079e5742c234cdb643",
            "f1c3dd191b604600bc20483c84466640",
            "42529d990098405db3e3170d76439bb3",
            "bb39d53df4654b27a184903169fe5f4c",
            "62794eac79c74e3c963b1e4c874d5176",
            "74790a1e56c24d139a3487119a8a7687",
            "b306890a008f41e78f7e3fcd47f45005",
            "57b2b906a90c4fe785d1c44bc7b40f8e",
            "9b8d04ffbfb74b4eb649fcbc41b7a422",
            "bc7f5d5dd0bc49ac85c5122cd5185779",
            "c0d53723e73749eb943d4fabfb19a857",
            "348bce0251994c948a3e669ef2fa1adf",
            "bfe54f3feb71461db34db5e4a8d22d0e",
            "e97cea2e96ba4ac18816552df3451c66",
            "680e01c9aa4042bba4a22798d72a50a9",
            "c6938676a8d542b6b3a81727a85be80f",
            "0702759b3de04227895cb43b39ef27f2",
            "401a805e0eaa4468a9a2c4881f8671c8",
            "221f10d1453e4cbdbb1f9ab8ddc763e7",
            "51f2eb3f2c18432d85eb498059b3cfa1",
            "156690458ce24092bcfbe572ce55b2c5",
            "60a518942d644ac79fd396b1dd457bbc",
            "c5d5f135342d4187865b36ba5f3c1582",
            "1ec615fcc7834f898b8b4b29afcd6dc2",
            "6b07640b1a74469e8647b99eb06bd8cf",
            "30285322041143bd9c6ee20d9f88ed2b",
            "c5b285ab2bab48ae9ca02d241ba54d44",
            "43e8d66711234bbc99c42be40e00e15c",
            "9a2fa2a1666e4193856316e9e735a07e",
            "77d16bfbbf784b5a8b4c65e60d10d526",
            "51ed4cecf5d54c38a253b8b8ca4b707b",
            "09e4e70deccc4652835dece30f7c5a7a",
            "1de721aeec7f4dbfa8b38c554f12b114",
            "90107d212b7c47ffb0e126753bf6b72d",
            "6a2175929a5644188965b53edf32165c",
            "e3d4b2f7c54944c48bc19cbdabf2a7cd",
            "21867d32b836428283008ebb2f4a9a7b",
            "ee949fde20774e0396ea635fb47bea3a",
            "fd62be2dcf6749bf9a1d79b10bf224cc",
            "735d97231a0d4af5ba9a89eb77d3c986",
            "0853e311299443c290c8f16f4d5043d8",
            "7c74a55348fb4a47bb84f5fc06d1ced0",
            "29b6c334e47d4eb798f7efdcb66129cc",
            "c1513a750caa4bdebac15152df51b9ff",
            "89f630208a6e401a85f587e01df098c4",
            "40ca85e6ef664082a5c6c96dcaa940fe",
            "4965a55ee75146ccafab6a91fdf8809c",
            "2f0df9ebc8a34b5090ffebe71a8fadc1",
            "92d01fdd28fe4eb786d8fe4758d35c1f",
            "01726ce7565f45afa50a93a831a204f0",
            "88750a107e514cb895616a74e57032d4",
            "bae9f7f3a3bc46d894d7bd8a027b5f76",
            "065c8dc607724b6cb11663f8b2f0ec93",
            "b2e3ad48068f48ce9da34d52242c25c7",
            "add22625c63c448dba5d84778bb12d23",
            "d8e77fe5d3c4446b8c4a9abc09b71e2f",
            "9859b3f7424d4b4f8f923844a25c2a03",
            "e0d2215a43a1498487ed6ef9294d84f7",
            "417a406ed85b420eb4c26531bc5e4c81",
            "e4ed18e965f2481f8d60d85393f50618",
            "1220647c4b8b43d6b83cb79312ef01a1",
            "e3212510c9574e9c8a59be638126d8ec",
            "9fd25328d32948dda46571113679f8c0",
            "ef908ac519014484910958255ca85a01",
            "44f84e845c8e462494b64f4c60c837ec",
            "73d0768ef6044ca9b31d4946cb8910ae",
            "aa392df671c347709f063f29adb11551",
            "c2bcae3eaf7f4f74a04158da5e85b134",
            "535ed66f13cd4cc5bac606713e9ebb2e",
            "b3056e0695af4617b71578572bb64506",
            "46fcc37ce53b43d0b0f0ee9ab507be3a",
            "c3b267e93003437da30ecbbb2f3fab49",
            "b0bd1a6558004945ad5e2660384d6d52",
            "05730e710ea1485c9830eaca53b246cd",
            "e997b78d0b8a40c885329a9c1d83b27e",
            "aa0fb50e60d44bcaa953a46016012a42",
            "240a9eb41fad4a98a5d2ee32af592868",
            "cdf2e289a4cf4739b34d3a82adfcb3d2",
            "d24831c1c3544acdbe5ab5fe0ab17848",
            "2e9d31ff38b240a1bd1249191be1f26d",
            "c65785c708e246868906fbaf4684d316",
            "086e8365fd5643529702fd63eb225219",
            "cc095f7dc45841a780547b34dbf4e7a8",
            "aefb3543489f4c778bea7885ffc35d1c",
            "6a1a1ea0a4d9487484f0dd81abb4562f",
            "613995ce2261479d96d3fc3f2c43ade5",
            "cec22232d4e44933afb1e5dc19885c3b",
            "5ab66f8d40de42c5baee81136609b529",
            "70354af81f044e7399e75273e4e34ab1",
            "5538088467394fb5b514b9960d4e8f35",
            "31e5648d68d44794b077378080610f6c",
            "3ffb4ab06571442b9189b6c538f73ea1",
            "8d0af8850e434c9bb97a0aafc7e5274e",
            "bdbe632e013b4f5ba76de90b3932385c",
            "53a32987aab44655bf1a5216b42b18d7",
            "8ce7d475d83f4a9598adf9e9e191fcc5",
            "e955be5538d84dfdbac6d4ef3a6a2d18",
            "1c8665a9535a41098600faa8e3bfeca7",
            "780c2f9ed108402f8f38b0a650b7d767",
            "6a444002aa71462c88e42e15f35649ac",
            "4c20c1adfa9c41d19ec03deacadc1f6e",
            "ea4d57e9c5744a2e8895a1100c49ebb5",
            "138505d3340f4131b2dafadb98225010",
            "88691bd301e842a2b0a48d7e2fe8a2e9",
            "384bf46443e849bd98e7fa560e835829",
            "544a563c75024772b574d624726ae361",
            "6c01f3eca53c417199296520dbb30bf1",
            "e35c49b50ce74f648269908c5e5c3b28",
            "caeafdf085bf4a0289e78ead64bdc4b6",
            "0fce67af20164e5781dce060a2d79b62",
            "52b39dfd2864424dae89effe35c74204",
            "7bac6da021334ddd9cbbfe48b9020982",
            "c97ced179ef24d6dbaf429463bf9219d",
            "cad85347031c485dbbe2b293446c6dcc",
            "994b3def7e20412384ddd4d526ba5f68",
            "dd0685f7cccf46f7a46202239c8b1518",
            "d3c4a8e9c0ee490b8446739d3d79628b",
            "bbf0b63a3cec4564a0740e3aca31bca0",
            "91db4f919420445799cc977a3549ca28",
            "7466a9eb794e4123be03ea2642936811",
            "25c86a3f0c27453f8bc4e25c6b9e306d",
            "4860c352d5b6437991c17e3414c164e0",
            "960d02e8317140948a10d38bd3ffa2b5",
            "3152ec414bd14a4988a7e8458da18e51",
            "ba58765b265d4155a5dd790b624ff723",
            "bb99578339d1474caf61f617bb24d5ab",
            "fe4b00f8bb814c4985e84cb767817803",
            "4044cdab971e45948c4f42028f54788b",
            "3d998d21ac254b93b2a5acd18280b8e3",
            "61307676ab48498b9c2114a4c823243a",
            "cb6cc7e76f544472adb11e3d073f48e3",
            "066800cc343642d991cb519d841a7374",
            "4492d74608a84369a43f36ca03145c4f",
            "58ba6d23b6654541a42cd3a3f4975487",
            "17406adeed4541eda4acc498690c6c69",
            "6f747931a45f44fc967d3769c037c6ce",
            "b425fcc277c1488185abdcdfcdc2030c",
            "ac963158f5fc4bc19c30a2ab8b6b790e",
            "fbb9a613d6d549b586493f859a2ad25b",
            "ef91cdfe287243ca8365197062bd9c9c",
            "1c4b481a7d374853a2e9fcd162a4461a",
            "84a00ae50d4244e3ac45d0181248273d",
            "2f9bcbe9f3cd4c4ba521707c1882ad25",
            "52b88f0dd8ee4e269124744686809b90",
            "71f0e2b5b71042fe9bd25c17e206017e",
            "cc5f9c27b4ba4c42ab494e6d90e47ef8",
            "c18e158486b64bfcae6140e9fe03dc8b",
            "df5020c1a6c9411685f2994b98c61539",
            "a65ae2b8051f40a897c6c73fd8a2ac2a",
            "e2175d958a28443494dd6cbf16012491",
            "1e6fb1cf0e4d4a55b95507ef6ca0f11e",
            "f03207bc1dcf47f6a98fc5d017975b01",
            "3c3501b40a4a490482419890913f3bd4",
            "e73e6610ac9b4ae9b4343bc2b0eb669a",
            "080655a905e149f09e99af6dc448748f",
            "514479bd26f84ff3b94a37a23c662389",
            "89810c5e29724e3793b332fe3179b2e6",
            "2ad357afb9794da79caa196ab7b72863",
            "9482e0c04a3f4f5886bd4d3ea0e48e11",
            "c2d879b62565442ab0bd48a24d56f294",
            "4844b9d92d4e4329bfbcee2eee8f2b89",
            "d0d05eff112f49f48cb0c674efcd73f6",
            "ce66d748e0604df0a6a0981035a32f0a",
            "67e443fe34444224b1ebf27bd954cc2c",
            "accfdd3ee86f497aa5d9e87bf1761f70",
            "c8e56f4e896844968dd879b75c7faa94",
            "9ff23bcaf4174b7b8f03a11d896b01c1",
            "7d7b0c2753674e728f683035197c79f6",
            "6ccb75f806714067a3de472bff11a4e5",
            "8895f1cda7c342838377f40ae7495ee3",
            "3a80f5b7edd4451c9ec0ee87c3d6c1a4",
            "d9aa34be1f474caca4ac0a15aad28398",
            "5e935c5e3ab64ca8b3429a022126da85",
            "f1c2b5a1a1744a2f8824f6710ba024bc",
            "c94f2538e396499187522d74afaca178",
            "c61e45dd38af42f8a491683c000e053a",
            "70af6657d6384235a9a6425c05042e9a",
            "e6f601ab9cd74ebf9517e69e5f52e605",
            "58b9a523143f4039b6446902c6e9e3a0",
            "5e6282d6ba6443409052da5fba97d62f",
            "fa9190c53daf4346908b646f3ef12793",
            "d0fa4e59698e4e49a2eeecaa5baad9d7",
            "84a8a3120ad24b54b8a0f399ddc04cef",
            "983920357ea84f6b84f6aeff32d7587e",
            "9c8fab80fb974695a76585c8af1b1080",
            "3945dc35dc5e49bd872526299dcad5ea",
            "4884832f6b9b435a829ce7fb16196554",
            "03ba194e4fac4037a4bdb20c166e0bb1",
            "d345c9413a8c4563a399b4fb37c724b6",
            "60eb04f2bf4a4b7894a5994d2ef595f1",
            "e35bf47aabfe43e7890ff1c7f3473b6e",
            "e7dc544ddd324126bea34c2e9992a1f5",
            "8c3f349ee50a42cbb1d69900d3bc9058",
            "5a413eaa50864025b010a4781d13c97c",
            "ca35d79b5fa144b79ee51cb9f0712f65",
            "dd07da9779d94db5b20a9dc0f8a1786e",
            "bdfc8e25083c4fd798ebcfb0df3f32e4",
            "f8c5e96e74f84a00b6afce522f481e58",
            "cfd211db14f346adba449e17ebad59a4",
            "e11ed8b9507747d4a0c966776efd5c13",
            "8243f2cfdc0f4725aaf33b7794a275ee",
            "8002e04420254aa1adcea7728a7d4cee",
            "626a4d93f5044c9881fbb23f37d72bb3",
            "9c21f2e315ce45918b0a374022a534e6",
            "5a85775efe01423d992fe321be18b704",
            "cf78f89d055842718bfe92c78661161d",
            "94246a7d4bc249289ddb72defd1627d9",
            "c19627a8f9d0415597507b92817d83d6",
            "0116e824de9c43a8b7de3643472aa9d9",
            "48771b2add46465da661837a1e963bdb",
            "888c6d497a4b4ed0a1b6ecf7d9b3f191",
            "43cf314b226245b6a734a8faea5d81cf",
            "a9443e937f5f4beb9b4340725028f62f",
            "40f9c9e73fd74554beaa0a8e492ec714",
            "a7de346c4f2e46289ed3d82ca874a05e",
            "fdf9f517721b47338f4c1519839f1429",
            "a954a565c1c44b6abfe00edb00ec7a8e",
            "91e77618ba30483eb695bd274562408c",
            "53858bebb37443328f1fa2b9bc4010db",
            "9ca677b871e543ae8dc638777b9f4b6d",
            "a6b08a194e58482d9832fe73353d7237",
            "c49b0953afbc4f878797d80b5f997999",
            "5fede29f095041d2bee99371ad79d49a",
            "b6ebcbdd0ead4a39a9322cbdc334dbb0",
            "e6f30d0acd7e42a58d8f23f281df6cc1",
            "87859462553b4a5eba49d201d04dba88",
            "babcb827f0a24f4c9fa7172024b2fa5e",
            "404aed1c5d554ff68c3f033b204bc80e",
            "b9a638c69d5748009a8963eda77df4c1",
            "e12c04714b1841f185c6102ec42e8d5a",
            "88063be5087c4e2880f9d2c13fae2ab5",
            "d493dcf5ac86437bb6ec8c7efe6c9b51",
            "233aafbe449b4f05bb398ee4620ca993",
            "7cad2c2720a243af930914d49d1c8cac",
            "8ee452ff96574a338d14e613dd6d2837",
            "dce78da16a6743fd948b390cbc56ee0b",
            "692ca050bdae4e718e0bf4cc39d5fc62",
            "d5ca47103fc04e80b11880c95f59b7c7",
            "211a1f9441eb4ff0a3c30e9b26f79450",
            "ab3ca43dff9749d49056781c036fbb0f",
            "d71b55630596467d90ac0efef91ad05c",
            "5ccff6c992104561b1938c80d00fc8cb",
            "92885339fc044d879ee195a836a80675",
            "37aa1c2d350648c18e4dca8864b601f4",
            "36461cd9bbb44667b3b4e9617decf323",
            "43c6a6cf66664672974ab3fe61fb3a32",
            "30d6a377734f4161b437c36711857ddd",
            "fd3417a2541247f19786432e6f51bf4f",
            "4f9064d7dfb1461bbc1f26e45044f926",
            "5263d19793d04eb2b05399c60606d375",
            "7b0406574a1f4ddeb56d95c862e5f50b",
            "a58b04b6971e45bea4743f49df692103",
            "fdb1b71ffbb64aacab4e3e33f2808aa8",
            "bd72ab7e2d424e188d4fedc2e63e9f60",
            "fcb81c648934447cbee63c64157ce084",
            "864452a19bda4836bc9a36fdd4fdb570",
            "383315cc9bcd4b8daa1a8cb5cdbabf2c",
            "ee75445a59554157a5ea0cc43bc3ad04",
            "fb526c67c9f8448990607c1b40fa2eb2",
            "8091bca8037f46ee9ba413150fcfadf8",
            "65fb00c3c6f94d65bf121fb90e103eba",
            "2e39470a9f2a426cb92a47d19671cd2a",
            "b88cdeb3c00f49148862abdfe37d6a7e",
            "4e2cdf73d721419a8f9dc491a2635c81",
            "267a1fbf1cb44ae68dabce03a3373ea3",
            "6ef294b26b9c4e0195860083f339510d",
            "bf9a51d108984ba3af992cd12f12a54f",
            "8972255353a9413985cfcd8dfd5b6cee",
            "2971453e4f7b4f479bf593f87c36c260",
            "a88a72cf0e4f4c74ad7d3e031ec4f0b9",
            "025fffe4ebdb491381a58ab7f31c6e53",
            "2e49870fc0cc4f65beac609564d546c5",
            "2b19e4af28f54c6d8a2a4f4a4a83bf57",
            "c11753976bb643e28d724c27cd477b34",
            "7d36ad8d115847568bd08d133aa239f2",
            "80749c7c0cd143aca71b0659808b42e9",
            "c5df3562fc50476a981068b297bfab43",
            "ab2f4b2f5f294c3da55bd571c3760c18",
            "ed8c0ea764be4ba494a52deeb5ca2bc4",
            "bd5c9c6a38df4382a72df29bafedb675",
            "a0d42043289541d6958331659e12f78b",
            "85fb6c82ec6d48bd87664b7963013a2c",
            "e1f3b6bfde844484b925601605bc34f2",
            "b7223cc0e47c4be29fb4696787e333af",
            "629fb48d7b914f64b62255446bf6e0aa",
            "ad7b8a79069a48bd866ff83021289b22",
            "a46a54fdc51442c98440332e59959893",
            "6d8757fd2538404688e131ef750c7ff2",
            "08cb4108d99c4f7e9b4d52108375873a",
            "d39a6f7b255541a59ac0dd9da4d3a283",
            "f38bda6bd6364d88af93fb56c94ae5c6",
            "cab2dfa077134e08a850fb007ea83eb3",
            "c3d55257e0f547bf896a4cc04f2e8219",
            "1cd9a5f6594045a4b4fd7a235ae34969",
            "dfc85307c6cc4cc7ad2d9368e2c11b66",
            "08a5617e9e004c8eb666d17b97ef905c",
            "6e28b6153c724937a6d8727c26aed172",
            "8ffc0cedcf0345b0b169220f2a317d81",
            "f3acd18127d14c5e85922228dba178ba",
            "68b29cfc019745928afe862ad5e3e9e5",
            "2ed9b358730841eaadc549e555db9be2",
            "1cf211aee5c842be9fb0e18769a67f46",
            "ff9815609b7a48a4b6ac34f72e1ed21f",
            "b60b0102bdbf436bb1b66e5cf04b93c2",
            "78ab7af148c1466ebcb05df49d398d6a",
            "cbed63f9b629448e94d73c6b8f4e9f55",
            "78d3a2c6a5da412c9e106f37c7052a34",
            "8433b9a30ea141dea9915b305c0d08e2",
            "0417dbdc993a4ab7ab3c1ed1032993ef",
            "533e7668643f42b291adae029d364e47",
            "0354942a0298498e8938a27977bc9ce4",
            "c4b7865f8b8d4573b81c91aa86ff0aaf",
            "afce31b54e814eb7a7b1229991c7d9f3",
            "c45c61619f6b43ee955d26f3ed3d647a",
            "707f19822a524f92b1b2c0135e31dd8f",
            "3fada10d02814d66be3e8de9b14acf84",
            "aadae435e7824b1b9b5d9e8921983006",
            "836d205d7e5d4a8287b94204821d3f42",
            "5daf0766a734475bb097cff9fa2576c0",
            "55c3a8cff66740bc820a17f189896c2d",
            "8a063866d58d4681920f8a075c0c1872",
            "c5477962f5644c9fb7c8805d19d66812",
            "7048ec5bd87f4a2cb4a7c78aa50f50f1",
            "e7aa9624c81a40f696b0e6ec8e40a7c9",
            "703538a94b9f4f03bba5891ef330a390",
            "9c9cb9bd0195472b88e9c9b61bbeafe5",
            "457aa67102ae41b3a529eeff93a7e828",
            "da7529810f3a479e9a803d5405f49e3a",
            "37a9a27fd10b48f08cee845d0a23ebd5",
            "c182da781c2c4f28a088284fc5df2999",
            "4633b92a0a484711b6e1707cac6db092",
            "71a01146880b4f86903c5ca318f468e2",
            "cf7434fa64d5470785c1db002394debe",
            "feaae3dc2943494985566511e8febe5f",
            "e7034825f8d241099df6d1b33c0d2a8e",
            "c85f9c5e201a4275b5613e06c949b226",
            "649a594c5ac34f9a81d8e692aed3d87e",
            "148146a732444ab18217689079196f10",
            "62f86695639548978e3a426e6e6f9985",
            "1bf206527eb64e118930205c747c9fd2",
            "e48b93024f9b4a99ae522a6532185772",
            "2736b225c9aa4a85a17ff769855a15f6",
            "22b99535b0cb4dc0900f4c798afbc994",
            "67cb0ac48e0442f68e7cb64b967857d9",
            "f5503bde75444bb0bcfedecbb43c03c5",
            "ce6b6a893e8243a8984f4880dbec34df",
            "a3dc64c8a77f4827aa838f7c9b730c6d",
            "e4907110f0a844ec863176d0230b74da",
            "ab0722c270f04ef39dd1407f1cd2d513",
            "ea96232c13b94327a6919e83fbaf4b14",
            "65ef4b4567d24cc192b0a11059edd557",
            "e3df088e66b64e2c9ac8d9439d28be99",
            "9ccaeff5bc254800a13e4e567e4c34c8",
            "3e050c5bdcd54f18b0ce1d6567b103c4",
            "985445260b9b4299acef5ae98e9069e2",
            "88b7b1d968bd4d098961463066db84c1",
            "9c0b0a4aa0d348938e6421353c4f1ca3",
            "29f8431569f249749b3278d52de4e0fd",
            "965a66ec506345aea1609ff76b6ff94e",
            "5c6d4097538f4b79b3892b0d8a2e8dfa",
            "6ad5ee450dea4e3b8c0ce774a138f50d",
            "dfd591ad3bcc4603b544475f58586b78",
            "553a49cf3d12407ca6d8975803a04f0f",
            "5d27c988cc264639b0bb3432a66f7c60",
            "d1ba6cd5e2c44809aca4c6a18c028f92",
            "0dcf57b6d9d64bec99dc8eb1da988dad",
            "e6bb8b80342347f8b2b0387e32936750",
            "ef33ed22a4054435b878b00fc1210796",
            "44fb35d23194429d8c841798a600e083",
            "86e556ccea0e4f078577cc51890229e7",
            "b639857519ef421bbea08040a3f3d01f",
            "1ebe13e2d8dd4b9186ae11a1e133db05",
            "25c43a3a7b16405eb63f1cbd41c777c2",
            "38e61ec8990b4b608a8c060aa4f9c032",
            "7fe7909d807143a590676ea788dd6714",
            "8ca8946692c849509307f7f8b558d5d2",
            "b61108f1d5574678ad2709786d7512c9",
            "9fde0a7ce0974a87805dd9597f749ebf",
            "cab9debf0f9f41ac85578287b02ffc1b",
            "a7469ef9257d427687525261d3049da6",
            "229c85d547f44a77b8673988814bff1f",
            "67935d2651dc4c5da3324e8310c7801a",
            "f0171fee01ff45cb8d02f844d812bfee",
            "e17bf8bc38fa47c3a9029854b4a639f3",
            "dd6324f3c0764b1aa6e1f9380ad65d51",
            "d5503d99461f47f3bfbf9addf6bc427d",
            "f9a0a9ff718f47f3b5be7467151ec97d",
            "3a72e58a87434291b1424f725926b593",
            "ad505d8315ae4319a79f5edc69bcfe04",
            "0047d6897ec14044b74cc240f488f007",
            "a2afdba92bfe4dbeabaec1b9465fc15c",
            "a3a75a5c36164f7f81700fca9ebf5423",
            "79f8caae9c254bd08c4dffd662bdb668",
            "348f8442c36c4414b26d46acb2db3948",
            "fb52d6bc61554e67bbdda176b1197e86",
            "94b29443fb0647058b1bfa6964bedd3e",
            "13487023df1a4b98bdac49a36ad3694c",
            "2840d7ed44004fc6911d963ed1a05b4e",
            "1398de9f21fd434d8703844087c60ff9",
            "cecc08a5742d44cd96bc236c69bef409",
            "a2c36e4eb7c74a58b26cf8e72e105924",
            "a2f4fb8473304584905d67a781b46042",
            "b14ecd83928c4dd5a9017f26d83071e5",
            "37c1a6e43ff5463aae030445061e9689",
            "6a5a3e98054e4ad4929941c84e7fb902",
            "d06d545cf378467c95e31d33eaa2bf4c",
            "bfea440d6d264ca9946a39d53e6f9a5f",
            "9ad888fc8e3a4267aeb7180cace56321",
            "e943a91aa2554be3a1a430ccf348375f",
            "d676cdebb6704a7fb86c1285017b245e",
            "e0e7f21a2d7b445e9a3226c3d86312e7",
            "b4e0095b01fc4ad69372d543a0b97520",
            "4e6be5b507df49d4b9305f883a1deafc",
            "3f6438fc498d431d8b91e0289c9b5949",
            "3b9c00bb7be04daf8bc739a1dfec617b",
            "69e6e713ff20400c9c397767f726bae9",
            "2b1b66b474e34bac910ffd56da6defc2",
            "37b387f3a92549e3bc08a1857c77f005",
            "212034aea9164836a7f02c639300fd59",
            "7e94ed7635ea49329c01183b5590c41c",
            "dc606d31ba9d428599aade9fd97dab5e",
            "94ee873a017b442b9705368dfaa6e09d",
            "30225834459e454389809bf5ae15a7c5",
            "76a9309f7cca4f72b8747bb8a0f754cd",
            "9313a4e23946480fbe631341f074339f",
            "8d0343e0f22f4776ae9c0c29938f6125",
            "792beebfd7f645dab423bff7762ae17c",
            "6072691eaa564fb788bc4687d78f511d",
            "55fb319234b540318aec476a9614c983",
            "4b1af02cc8bf413ba7fc068ad28ef829",
            "7bd08976c68c4330be6b202e940509b6",
            "2242ca7e18d44ed3a5ac62abf6337e53",
            "31e2938be36f485c8385a57e68830a94",
            "634e38afd52b49a394ad22d03ece4065",
            "3ea6c9bd817f49fa85f62305c7abae5d",
            "d2d451da85674f55aa9b6bb48480d4d4",
            "2e628da9d95042fe8ac68d9203c4fd99",
            "311d1bfa09fb4bf88c53899fadbe5276",
            "8ab3b58affc94a05a2ba018a7ef35c51",
            "2a9341bf46834381805f57b41fa3e137",
            "66adf95f2fd34fd8a5e37101d9f1a784",
            "3d94fc38e62e47a9860f93db41ea50bf",
            "93c14171c47d4c76a0f389b49675d96a",
            "791e4730e19f4ef59498fee11cbf6ded",
            "869e2ae932d54a85b2f3e239fffe6cb3",
            "2e48c275f63a43c6b63f4776ef6e9028",
            "44a599714eac421c893e22c2c1c6371a",
            "6a96189bcc7447a69744e3153e741ec4",
            "f3edf2ff9ac34f3d83011721cdb91893",
            "03ae82157df545099467cecd4e316055",
            "76bf1ec15c144278adc72a2b2d7ee1da",
            "31f0bc4449e74932a0ace94e8e6390be",
            "6aaa7ed413704b4da2559a0c2337e4c3",
            "fd4ec95d991c4a5e875ff47775d900e5",
            "97ce789d4292497faf61f19d007f57b1",
            "659fd18dab1f498d91b11323b0b7642d",
            "c68e52dcac4b4681ac7e45ffa4517258",
            "0ab62cac4f634be597089d3d87e1c5f0",
            "dc3e099d6ac14aa9bf7247e734b76f6e",
            "7da97ebf341f4816b9e338ec24ba769b",
            "48445aa00c8c445c896b2dc9a31dce0c",
            "5ac8a7e1d48343118c6a4e5cc6720efc",
            "eb1a50be28d04e2d8a88277bf817a55d",
            "a277feff42fc49c9b8b88eda01c7010a",
            "be5a8ecbd4bd48748c1e0356470c284d",
            "50d727e0bb8949649072d99df6a3b97e",
            "0720bc047b91413d92c32dc1e482ef88",
            "ee4eed86dc5b4b7eada54983ac6fc715",
            "290646131be143118faa9acc04f85a6f",
            "b8747cb21be1401b8c491fb37aec12bb",
            "6bc09e1968734060a07e823f1dda656c",
            "ed6fff766090430cbd4b747b5cf355b1",
            "ba1b56cf49364941af7e4ce50f045371",
            "887eaf7f4e1e4bc697f4d8a5c9052a3e",
            "a70e755a050243568a593fbaabf42e90",
            "2fa8050ce74145888001b2f201b3a521",
            "64519cc84d264aea98d10c5dd4fe91a7",
            "5fafb568632247b1882fa1bff1f08862",
            "532b88be8fd5489ea6b9ed1719ca6ab6",
            "9670fc53146f469688f9a7788db59489",
            "563ce22255404a86b5a9b0f5e1ee44d3",
            "1714f0147fea4912b86a633d304f6009",
            "5a52c30563504c9493c6774abdc6639b",
            "b1581aa29a6c41d8a675bff6c7ce1822",
            "3f31b99c8f1e4ff996298e75d76d15e4",
            "4b7ab912679c40d79e16f7a11dd3c6e9",
            "283ef2b4d9bd4d9aa83bbea0fde31081",
            "c07d47b3989345c28787c868a9ddaded",
            "43e16ded7b4a44d6aa2e4a3a2937b62d",
            "b2baba62342341148275ed91d7fa3f41",
            "5c3e8c4d94494faab2dec02e9cefddc2",
            "3de0cca2be00499ca1868f7279c17543",
            "293960b71519467cb5f416b437d5cc98",
            "10d4be52dcb34276adb33d8af40a99aa",
            "7286248fda1d46919fa4ef15f290cee9",
            "a27aa2e859c84960b0d7c8d28d8a0302",
            "f6e1c69ff7ff44afa3d78c3ecbfeaf3c",
            "82fdaca26d664383820be591055590b9",
            "435eca67fede4f13a7a1821be7091018",
            "b490b6005580435e9d9d6fbd60bf4484",
            "aaff539ac34d44ca8ac7f61f8291d41a",
            "94b69eb282fd4ff0af40c5ec0ea089f2",
            "53904b9c652b4b5d85cdef52c89ed8ef",
            "3421c42df9364e2db1bfc14e26038ca9",
            "dfdfdd4ed2af4ee3884e2bcd0cf46479",
            "fa94cb888b26491db225aa7f2d53a9b3",
            "b603f8147bcc4f7e9b72627d83083439",
            "c7194ab768094b048deca7488f088353",
            "170f475a7f7d43769643c5bf92dc0f8e",
            "2a4d04fe057f400d8db631ed0090e391",
            "e4deca781472413594603fffbaaf0161",
            "8dcc8a5e221c42169dbbf1bf7476c0a2",
            "02c6369cce5042db8328f273fac87984",
            "980ae67c04714660877a8bb3c363f0cd",
            "fb3fef8d77e04352917cd0813fafbd85",
            "17afc1d08285495cbadf75c1d2424b51",
            "730002dab5c048fa8519d5a1bc001255",
            "13098fe17a49475e9489b0e4360aeaf0",
            "55c4ee66f0944fc89c84e60e1bfc201c",
            "a1da2692c71c4376ae1881bb8cb2a83d",
            "c3574191e84945aa8d7132b5532dfca9",
            "b714db6ab2a24f93aeb2d1df4d713679",
            "0194ad47e5414dd2b764239406b82bef",
            "13193b7666d141fbb508936e4c1a2b72",
            "a28b353af56f4c4c902f24cfb3394175",
            "50ad39d602ed426bbec5b4d738282473",
            "1e6eb50ab1f34136ae1c981fc2bfaafa",
            "0431cc7ceb1c46209c7598ed5ae099cf",
            "266dd570a4224e928987d8bf7e219b25",
            "37da8ce190414839be9c7dd9975062b9",
            "2be5902daa6a457e9fd1bfaa793d07a5",
            "4f84c1333b1d49e38d2145e35c10cb03",
            "94f6ebac50574b6c8370dc76cddc155e",
            "5ab1fc027f8248ff90891052436a444f",
            "4310f80c364546fc9d1956817e92966a",
            "d8fab0f562154bb2b539106de4b04bf6",
            "0f5e2f93177e40a78a813dad320ebb04",
            "d274d9ad42644833a29a3579193290a0",
            "9f67b94ee1ae43769c38fd31cd190de4",
            "aa535eed83bc437da4f7dacd51a816a6",
            "ca2dbefe3a094ee79de955ae06dfae0e",
            "022239f036644f69875bcb6fabcf9f45",
            "7e36600e0f8e457ab8fd48bcecbe7930",
            "c0e0cb7a255f4aba9e7512252f9f5373",
            "37dd99638a174aa7a577a0fd7c0f12fd",
            "62993b27d92347b7bd6ee464505d79e0",
            "40c5160ff75044d4b16e0eb985d60fed",
            "58e955a9725a462baf7b2307146f34f9",
            "14f3eae05e414be08b41946e9ff3dacb",
            "6d94141833864b41b22f118c324bd8c7",
            "fca222a7715a45d582d63910d7ba1950",
            "4e39fc5f2e2748fb8c00efdb1d95c1fc",
            "e7b2087c39a1431bb689b3d2163e115a",
            "9eff6b8d2478445e918296b66f164e23",
            "ec309babc15241648810ec5b07b33ca2",
            "de21f18489b94839938cd05d1efc8703",
            "cc4c1148dc9c45b8bb594543075a8c74",
            "c8940e3ffed54e769fa939a427c0610a",
            "9a4cf3b72f1944c79f61ecc5406b24e3",
            "e47d037e8d3f4f62837e94d3a85402e0",
            "dbd673281e244f6e98c34238c5d8ece4",
            "1e9d289a82e741a6b88101ab35d1da9f",
            "e8845ed911264f35bbdb1a1cde7cc9d4",
            "ad33bff617b545498aba1e0fc47ef24a",
            "a1fab829a48440b594b492a921954ae9",
            "b2b9d50f42994b338f047a111651ce74",
            "d3e1dd0ee08045e6b59c30d4114c7ef7",
            "027717d5f1c3443880dac32a73fec2cf",
            "adf70aaa55dc44ae8c4ad02c55e02d2a",
            "9f07ae513dfe4d809e0fbdb3d6773033",
            "1ee1e36beab64f4f88d3a4bbf56048fa",
            "038076a7941a406085af4081ea5b409e",
            "50e12ca6a6c241f19b25e376841ceb58",
            "fc6e97cbf80e49e0aa08cef0b6302aae",
            "cd711e60ee5c4002964656704513fe30",
            "b4149ffa38d34d86bfa8297f3f5bea4f",
            "8922f1acc2c54aa0a56f8f194c3bb32d",
            "815704f60d044a2baaac9215d99a729d",
            "f848fbfcf6374cb49ef408054efc61b7",
            "795fcacb9afd4046a140c9897cff5c6d",
            "ad794cfd157140ed808a8aa65968e243",
            "f2be0b9098474211ace62a37064474da",
            "37cec33284474853b476004c681f0ba1",
            "2f703901d41e482785ff52d0c76a625c",
            "d36b3518174d4b5fa1b94d7a68d932ee",
            "5c6ebd4d766b4ab2b7228ceaaacd6bec",
            "b45c53c4f31f424b99af1763d5ac5877",
            "2282bf90944e4d4ebf27a712d806e310",
            "331b12b698614329857c67c78ef49fc4",
            "88d9ade8a60448e995d565bf712c6416",
            "7b329eadd5a6416b98a091550e1af345",
            "cbe0408d0607493fb76ba0f86a26417f",
            "a230911a3177474f8afd99cf89c0d599",
            "f622f7e18a17491f9542a65143bf8ca1",
            "a2b84780bd504adf8d6d3cb3b3d0a99a",
            "bde6a4bd5ca042c1aac83c5379461a18",
            "5e31278cbcd44227a630002de0c6ba3d",
            "8161545cb249471dab3c8180753df466",
            "4a96b9f6cb6e4868af42854efacb77d4",
            "12cb21f362784fe59147a31f0b772688",
            "4c0af65d8c5a45f68f51be1be4497633",
            "3406fc0978a046039dc3a66be0ddb27d",
            "6716ac8dc9d04c89800bbb899b9441a0",
            "24fb7809b9ee4d12b6f408491d1abe5d",
            "923c05bdb8ca47eeb4b5edda2403486b",
            "5679ec00138e481d887266adf698038d",
            "75a2fdbaed6641ca882680dbb39ab75b",
            "4b5d1025b7784f68a21bb79610fd0bf3",
            "132a33d4bc2f4593bcd918b14d1500fd",
            "ab5492a59b1b4964a69d1bd0d9a34e39",
            "8b1b62f5a53e4cc1b15ddc3ce2833d99",
            "f1d20bab47d4432786f68c458910387e",
            "e74d26a48697487dab2294d210750c8e",
            "c3a5c57e183b48b4aabb4678c26c329e",
            "77198d0fb33c4294812e79c7745f7996",
            "803d5fa129b249b6a91b559c8302c387",
            "85088862fb0f4cb39f191109dad3b3e5",
            "e1699e8171c64a0284c81524463fd2df",
            "4a2bea26ca6b47bf9c89d2e588ab6819",
            "87297349d12e40479cd2cd343cb654e1",
            "128aea20dd9d427f9d3b30b256663675",
            "463aab30a48b4beaa452eccdeb61f74a",
            "3da168e007184945a92d8679efa39a15",
            "ccc9f1a4c414483391056c6993b32989",
            "efbe47cc897f47c9b412c87b081e5f38",
            "a6f7a79cd3394736a6a19a63908f4adf",
            "e42d7dfa52a6492ca12d84de2ae52efd",
            "220b9fcf6a3a47829d4d00e6d0dbaadb",
            "3dc0800ebad24848a69aa355e7bf3c7e",
            "74229462b39b45938f23f2d7d2c048fd",
            "1bc4b26fe2db46d38ace86ffd4701ea9",
            "59e5de271d5d4d729e6ffe8e5cac5c61",
            "2e8f3522bb4c494d867cb980cdf03488",
            "ac5b72cd425e4add8943f60405dad821",
            "338010c2d09d4e449ab87f49c7c265c6",
            "87c4dcf786c946e4b0f25f0fc48f5b05",
            "d5f32d835b1e464dbe1df9e4b9c95db7",
            "ffbccb62ea4845e8b48e1116e888869f",
            "65e7339802ff4d718a6a0c1f44a9fa23",
            "59fd87bc824243dbb4cef9cb181f2802",
            "b8e0d4cc54ba4ad38a660cba9b3f67e7",
            "f7b78b0ed4db4d7f9f86550fe3ac203d",
            "74254562ed334a8bbd15266691f16b3e",
            "f7758b637dda44149f01ca7ab0af4596",
            "b5b403a77a8e4805a123ecd086454f37",
            "986491a8199544e7b5c8163c5b10becd",
            "a754a189b5e846adb94e0db12c079014",
            "bc55820ea9a44d5d9939003c7c24c8a4",
            "54660afb78ca47bbb8e09569bfc702d5",
            "382ac2c8f6744ece9d2a759e79ddce48",
            "41e548d4eb6c46188c3674fe187fc0de",
            "f901096e214d42c3abc4d5c024c7ed38",
            "de92f33380cb48a69ca9fb49f1a68528",
            "05313a3a7d174209ae640c86c03a70de",
            "678da124ca684aa9962f93b5873e9b0a",
            "124f241e1af949f6977c3e958f165cf8",
            "0e496c88809f4f7c8facddfc236d03dd",
            "14c0e74c2f594ae891294aab6aeb8fb3",
            "e95979241b284374b11feae397332446",
            "b6f112600d554552ab1bc7fe224f7ae4",
            "02c9dd6bfb57478d939135c727235986",
            "8801105ab042499bb02bd7bb33257cba",
            "d95c1c95f46d435dbd626e9b5cbaf62d",
            "f11dab0e5afe422dbd49561e1a272495",
            "2115b681ecf54201b9cc985178a02466",
            "e3ddd57dc73044a6a687517584c37513",
            "15ed9164084348369492d657f039c08f",
            "d6a7dd7c95fc428c872b14da803d51d1",
            "c29932ba27914dffa7745a569b9a20bf",
            "8ad52fbf9de04d0c94ee74374ffd5c0a",
            "2ee0696c5abe4711af986b819479e02a",
            "c393a9185d9845888bd0eec7ac1bbb1c",
            "2506016ff73b496f8c9baa2cb71c3138",
            "b23c85fb582e4f6abccfc8910b6bb727",
            "ba2fcebdfd4d48f18e0e16c3b3593412",
            "69016d8453e247c287f536ee826e99f0",
            "8be671f9307c44e4be37ea45c7314782",
            "f53acb2f85d54624b6815534e5847ff9",
            "de73efd198d544c89f04f234e701e79b",
            "050b935c352547dfa7a0c1f4add91107",
            "13ebe7d9622544969f1619c036f93c9d",
            "9b65ba09ad164b2089a2e0c2e0d1732b",
            "f634aa661f50426a8dd5346b27cdf704",
            "80f0e4ff80c942838686f77cc0317175",
            "b04e9485249f4c74b0fc57fdcb5c8f77",
            "d388f800479d497398a642902cbb40b0",
            "eec5a690b64a498baa98c4a100897f48",
            "2d74b215b07647a28eae35551cf0e6ab",
            "39ab9c1ff2f24cf08f66ca6d9d569a75",
            "167996580ea34c4b9b08e9310f792028",
            "0bdf37972dce4b67ba815cc1a917a3d7",
            "8c50463679a74b13ac776f1c9a79eb65",
            "c1f322778c87432ab93fe7d08c76d87c",
            "e1f0edd7d6944e97aa9ef00832fcc8db",
            "cdbef09393e14a70a6fb55b9077f1ca0",
            "f8ce9a46a8314fc49a7e3a93c9e22824",
            "60a685b414c3477184bcd912fd332b61",
            "b692d52c1cf941e78335071e266c310f",
            "6f12439091884fdab76eaa5e78f300f9",
            "4c38daa1b6a146dfbd39006779e8741a",
            "1b76313fc35d4baab7b8438ccf80b2bb",
            "33ec737da5194a599699c2c2854637aa",
            "3644e30dafe741c2b83ee4c33a118c65",
            "4f8311c054f84f1f94cd8fe1a73e4d66",
            "0fca6464b8904337ab7a7bde6dc3b5f6",
            "74d402d67edb432da1b9f17b96a74a54",
            "ef6863b3f83f4df2a088155dfd2c6df4",
            "541668e8c6e84736a0d40f95ef5ea015",
            "28afe639f3404d0294c5bacbb6d188d2",
            "4d01b874507b431ea87772fa74d68029",
            "f7d32dd845524690955bc19d78fcbac7",
            "2fbac735bae1443f9a8d5686e58e057e",
            "3610f725ce1b47d7ad2cfda7e1f6424a",
            "aa4fcde796b34105a803fc62d7927df9",
            "65b54da6f1d64ba481afceba6b81e551",
            "502e3bb6b86e4e959b0028453bafd419",
            "1221d9a00941452c844ad9113eaad4d1",
            "812a4e08268e410eb57a601e823c6daf",
            "8ce2d1b695324043be40449647d67cc1",
            "535cf6bcf1ca4e968d32204bfe7e4d77",
            "09be1535aa7241e581a66980e4fa7a8e",
            "2a8160b3c5f9464384d0670c3706778f",
            "2d94f1ac13d1440fbe698b96a260a7b2",
            "ce21715a449949d4ab514a812d92154b",
            "da3bde6b561849258274a00192cfb0e2",
            "bb4ba9413e8d45dbb8817cf4a70356b1",
            "a3823654953f40d7a16f705191058b8b",
            "32138584a6994c67a760675a0a26c021",
            "30ab14b9d52e4bfbaf62f31011b031fc",
            "7ca3d131571f4cc68a27ddadeb413d76",
            "2b104644e7ba4f45a19f5a3cad46c58f",
            "a7be82be2bab490dbe8cca616b4b053a",
            "fd7b5472c5cd42b7bbfab4fde965d3dc",
            "2918ce4e731e4513ad28750c551a2b70",
            "58c0e4a24e6640bdbe497d34edeaeb51",
            "15de7f5ee4fe424c9dd39f323b9cc1a3",
            "3e97aa1a8c2142caa2faca7819b15fd8",
            "840fa024f4194057b6192e5d0ca65523",
            "983806dcf10d40d3b02e9cc798bcc5fd",
            "d71df6c99e6f41668e1d1abaa807a774",
            "78cc2683307449099ae7bb4dbc1ade0d",
            "fd09c4bace0d4e7d8323c971941d8771",
            "7118106766d54b22b1b939bf633f62a6",
            "1fea198513734669841187af4de293a3",
            "dc64695987a548ce9fd907c169cdb069",
            "7206d612e31e4eba84143a57220055d5",
            "efa4f9246f764f7db1c2978a1add57f0",
            "d9099aef075444e4bef2799995f69852",
            "bb7e99d0717d4c21a777ff755bdbf3c4",
            "a4dc4f324c6049d1ab92b36c33a5d948",
            "3040f51b16bb4c628e82818353afcb34",
            "da2b0642b9f646fba0c5feaf28fb5ef0",
            "7690b6e670b94aaf955a421c73a3cbf2",
            "ffeece9c1f86405a87073fe72f8a19a3",
            "93ca55fac53a446c86290b81cb1f5202",
            "bc6023f31db8445ea07211ca4927b011",
            "b296fcbefc014edca82916cba4915daf",
            "25a756baeb164219afea32edfb3abd94",
            "1edf1993f95640bb816259d202c67d43",
            "2b14caa2955b4ffa80e058e7194e2eb3",
            "174c50dc1ef943b8af5802b3a0f90928",
            "9c824f34f30d486c87042518b137d4c2",
            "2b6171491da2426bbd3cdfad03e4d0c8",
            "c8280a9afd944c8899461fc1993e9325",
            "47c38259ee8643fdafb4d6981150c0fe",
            "e09f80cce8d74cc09f3822861c7ce6c8",
            "91a1a957d86d4ada9d098bb3d82cac35",
            "0a2d9d0c401b40b09d1be873a98fab4b",
            "4e0b504da9cc4457b2c58231077728a4",
            "d4414b6780454c3b9040b247b7871bd3",
            "fcbb2604d1c2418d8454c3fa05d4530b",
            "fb275fbb617348d4861c119962998a48",
            "21b4a262ff8340dd9fad70b1dba710be",
            "6c0376374bd9457db5f947863e94bded",
            "30cdc2091d0f4ee594f15c82943bc622",
            "4424e2abfb504ff8a605fdeb0bceb429",
            "9cb732247e724947898467f93acbb190",
            "3910142184694e318f9b0247d81e901e",
            "dbce9b004c6c471f894d737e6844bb32",
            "ae11dcd15ff441f88ffd1aa290d8a08e",
            "926c47839ee448ebbe8e9bafe22915b1",
            "b48fa6a238784fdd8f0f72ba9a50e524",
            "299944f5fb634900ada1c47ef06800fc",
            "1ce32fca303f435e9831e026827d28f0",
            "2c9696c4697946bfab78fbc112285076",
            "dfd4be1e685746cbb4dd179fde5f1dd6",
            "8f18b0bc477f4fe8b26fea40c3b70988",
            "ca9979e46a7a4a17a2788b0ef9fcdacc",
            "c040a7179c56422b90ab34980988bfdc",
            "6a1b22e4ddae49d093d939b6ad24fed0",
            "ded3d585ed7d47499d53f7d08c279671",
            "952c73a8c1a649e29c4b2e83d2ef88da",
            "70d9772c2de1401593d208319d921c9a",
            "646ea87a5bd14e71a774677b27b0e872",
            "bbe952197c044643b621b8152e33c2a7",
            "1d190e82882d4327bf7ca9615ce0a104",
            "e79d3c769d094e17a2936fac3412bbc6",
            "a74853937161418e80bd45179c0c89a5",
            "14487162534a48da8178810ee22c67d4",
            "9650568cea1741d7b5906a2dd169283e",
            "a2cce69010c04271be9e84c81e8575cc",
            "91f50719d0914094af76ff908b12fcc7",
            "ec7609fd58c84dbd8fa22e502fb346e8",
            "f105e22bd46e4bc9b05ec97f3ea71810",
            "550f2f1d236d4b8384560d63941eb437",
            "4cf82a3266fd4d6f90ba65a21fe5716e",
            "78984f8c31fb45da901ad5ea7f5c0097",
            "5397f2f9ff2a4b9dad9f3cd102f73210",
            "c3952af997f04220b47219501a3932fe",
            "526daf23e9284ee7bcab556d9d056ee4",
            "91d8808ab5e546b2b65fd85271116b72",
            "956c7c253d71474c90a1d47f55439236",
            "62d548357f6d4ef88945631706369aea",
            "c068eb7bf0c646bda514b03a8e9faf5f",
            "7306f4bae35440d89cdbc1d74a32f683",
            "2e26abf0393f4a14a4a742a22ff5cf51",
            "27ccfa52685d4eeca9ea6a9ea6a0167d",
            "73866c27b2394cb3942b0ce8f07f0b57",
            "dc7dfa5ff952470baf9a39a215e3d75a",
            "75850b8b34904a4a8e686baf2fd0402b",
            "73cc26abf38a4e36a7a1b6c13a2430cc",
            "f36d6a9ed1a34d9eac6d24e5f3f9c559",
            "7b798dcc2258489691ebf32f5d070971"
          ]
        },
        "id": "UzIwEJIgsknT",
        "outputId": "600684ab-9e38-4e25-b0f0-e54a124fdb92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1094eff4b9534d579fe0dc46e005d1d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0/96 [00:00<?, ?files/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f1275e2ec456476aa0cbb13029aa9e4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/30.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce99d589257644e9a70d4fd51db16942"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dacb409420d2435ba0d2bf30f7904ee7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/56.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97d83c9f31994a4d8f9dee8593071a4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/30.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e818f16ef7b44c938d78ee13b934f727"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/55.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ee79be65645741a5b91efab1c5288dc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/86.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "523d425f5a5b4998a387b60d6d22e14a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/158k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d26cc66a6004062a9ae80625ac9f309"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8162e18885ca447098e1a0be81fb8890"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ef9ee196f5c54cd5a69f6affb484ca25"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2a7fc319d5343f7887061816d0862ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/31.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "638fc86f8b2c41bcb8c0270a4a8509b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bf00eef33324bb4a1c19f25671a403d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/29.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f617184e8cc4bb093e3c21369cc283f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/56.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a34571070f1c4b31846b77e45f5716bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/105k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d167ea0538134ad6bf5acf35be24ba79"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/396k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b65cd516f8e24ab698380b7836cb35d3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/251k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e8edb670d6b4cb3b80f040e0923954f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/58.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7329290ba7b4dd987652f6d9efd9a8e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/267k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f98a512989649d5a97bbb49390fe819"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/31.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cea15b6fab9e423f85d82d1f8b0d2e76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/32.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b15cb67c9a5487e8012997c1003986f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/396k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bb91fff619a44ace96e6c229d721f328"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/168k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a005ca3c3f040d9946bd33a8b77dfd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/162k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "941d4b87e2f246a09ad4d8e4b4074769"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/86.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8512dd9a766447f1bce70078ef1d39ef"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/276k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "58d806b04935410f9328f575aa5f2cbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/250k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bfb627245084412c94067efbfe89a1f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "86162979242447c9a01d884ad98c460a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/221k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "344c408390e84ef98d32afa2c28dbf91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "eaa6aee65bba42be821d8278da6cde72"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2821822b0f47428bb9300f5a477fbda3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/631k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74a1666b14bf46cd800662447b2c76e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/287k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dcfa211dda944bde9db0b8340dbe9a06"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "473b7f1784d546d3b4140d4f6ad27d68"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5abdd6f8a674bf1a7f0a1bb261b6176"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/3.30M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "863dc18b88d1488fb5d62b1091a1d429"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/2.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e29b80a61cb4f1eb17dd2fbc0cd7d5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/785k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1048cbb933b34a6ebd314c39a4fc08f7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/4.18M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e27864c6fa2e4554845074998adeae83"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/605k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5832022415854322aa0982df09da682f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/3.53M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7abce59d394c4ac99ce9c011ced776a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/632k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53470e565e0341e69e3b1665a4c9a47d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/4.48M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3e9a2e8818534c9baecdaf57a344a02b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/4.53M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6c920de5dc6e4d7ab99b0798daa7fd52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/4.62M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "241e5792e7234579a52d33f4a8edad9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/46.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d201296fe744466b8d090a0ec805007e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8a54a9851fa14a19b50bfbd38800273f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc7f5d5dd0bc49ac85c5122cd5185779"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "156690458ce24092bcfbe572ce55b2c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/44.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "09e4e70deccc4652835dece30f7c5a7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "29b6c334e47d4eb798f7efdcb66129cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/44.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2e3ad48068f48ce9da34d52242c25c7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/47.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "44f84e845c8e462494b64f4c60c837ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/2.44M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa0fb50e60d44bcaa953a46016012a42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/7.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cec22232d4e44933afb1e5dc19885c3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/2.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1c8665a9535a41098600faa8e3bfeca7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "caeafdf085bf4a0289e78ead64bdc4b6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/8.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7466a9eb794e4123be03ea2642936811"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/7.28M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb6cc7e76f544472adb11e3d073f48e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/7.12M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "84a00ae50d4244e3ac45d0181248273d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3c3501b40a4a490482419890913f3bd4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "67e443fe34444224b1ebf27bd954cc2c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/1.15M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c94f2538e396499187522d74afaca178"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3945dc35dc5e49bd872526299dcad5ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bdfc8e25083c4fd798ebcfb0df3f32e4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c19627a8f9d0415597507b92817d83d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "53858bebb37443328f1fa2b9bc4010db"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e12c04714b1841f185c6102ec42e8d5a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d71b55630596467d90ac0efef91ad05c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a58b04b6971e45bea4743f49df692103"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b88cdeb3c00f49148862abdfe37d6a7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c11753976bb643e28d724c27cd477b34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "629fb48d7b914f64b62255446bf6e0aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "08a5617e9e004c8eb666d17b97ef905c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/924k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78d3a2c6a5da412c9e106f37c7052a34"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/965k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "836d205d7e5d4a8287b94204821d3f42"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37a9a27fd10b48f08cee845d0a23ebd5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/983k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1bf206527eb64e118930205c747c9fd2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/553k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "65ef4b4567d24cc192b0a11059edd557"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/814k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfd591ad3bcc4603b544475f58586b78"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25c43a3a7b16405eb63f1cbd41c777c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/214k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e17bf8bc38fa47c3a9029854b4a639f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/108k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb52d6bc61554e67bbdda176b1197e86"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/84.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d06d545cf378467c95e31d33eaa2bf4c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/58.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b1b66b474e34bac910ffd56da6defc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/169k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6072691eaa564fb788bc4687d78f511d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/58.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ab3b58affc94a05a2ba018a7ef35c51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/29.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03ae82157df545099467cecd4e316055"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48445aa00c8c445c896b2dc9a31dce0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ed6fff766090430cbd4b747b5cf355b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/56.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a52c30563504c9493c6774abdc6639b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/109k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "10d4be52dcb34276adb33d8af40a99aa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/33.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfdfdd4ed2af4ee3884e2bcd0cf46479"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "17afc1d08285495cbadf75c1d2424b51"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/58.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e6eb50ab1f34136ae1c981fc2bfaafa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d274d9ad42644833a29a3579193290a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/30504 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "14f3eae05e414be08b41946e9ff3dacb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 30504 samples\n",
            "Training samples: 24403, Validation samples: 3050, Test samples: 3051\n",
            "Initializing enhanced BLIP model...\n",
            "Loading base BLIP model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e47d037e8d3f4f62837e94d3a85402e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1ee1e36beab64f4f88d3a4bbf56048fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f2be0b9098474211ace62a37064474da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a230911a3177474f8afd99cf89c0d599"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24fb7809b9ee4d12b6f408491d1abe5d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "77198d0fb33c4294812e79c7745f7996"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6f7a79cd3394736a6a19a63908f4adf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d5f32d835b1e464dbe1df9e4b9c95db7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model hidden dimension: 768\n",
            "Enhanced BLIP model initialized\n",
            "Starting training...\n",
            "Starting training for 7 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:382: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device == \"cuda\"))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bc55820ea9a44d5d9939003c7c24c8a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Avg. Training Loss: 1.6039 (Time: 324.67s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e95979241b284374b11feae397332446"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: in the image, a group of people are gathered in a park. the park is filled with people engaged in various activities, some standing and others sitting on benches. one person is holding an umbrella, while the other is walking away from the camera\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered on a sidewalk in an urban setting. the person on the left is wearing a black jacket and blue jeans, while the other three individuals are dressed in blue jackets and hats. they are all smiling\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter in a white uniform with red accents is mid - swing, his body coiled and muscles taut as he prepares to strike the incoming ball. behind him, a catcher in an orange uniform crouches\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2589\n",
            "Validation CIDEr: 0.0072\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2072\n",
            "Validation METEOR: 0.0910\n",
            "New best model saved at epoch 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ad52fbf9de04d0c94ee74374ffd5c0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Avg. Training Loss: 0.8117 (Time: 567.01s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "13ebe7d9622544969f1619c036f93c9d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: in the image, a group of people are gathered on a sidewalk in an urban setting. the person on the left is wearing a black jacket and blue jeans, while their companion on right is dressed in a white shirt and black pants. they are\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered on a sidewalk in an urban setting. the person on the left is wearing a black jacket and blue jeans, while the other three individuals are dressed in white shirts and black pants. they are all\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: a zebra stands majestically in a verdant field, its black and white stripes contrasting with the lush green grass beneath it. the zebra ' s head is turned to the left, as if observing something off - camera. in the distance,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2651\n",
            "Validation CIDEr: 0.0111\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2155\n",
            "Validation METEOR: 0.0986\n",
            "New best model saved at epoch 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8c50463679a74b13ac776f1c9a79eb65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Avg. Training Loss: 0.7218 (Time: 378.00s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3644e30dafe741c2b83ee4c33a118c65"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: this black and white photograph captures a bustling city street. the main focus is on a group of people walking on the sidewalk, each holding an umbrella to shield themselves from the rain. in the background, there ' s a large building with a\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in a restaurant. the table is covered with a white tablecloth and features two plates of food - one containing a sandwich and the other a salad. one person is reaching for a\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two zebras are captured in a moment of tranquility within their zoo enclosure. the zebra on the left is facing away from the camera, its black and white stripes contrasting with the verdant grass beneath it. its\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2739\n",
            "Validation CIDEr: 0.0126\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2224\n",
            "Validation METEOR: 0.1033\n",
            "New best model saved at epoch 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 4/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aa4fcde796b34105a803fc62d7927df9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Avg. Training Loss: 0.6625 (Time: 345.18s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da3bde6b561849258274a00192cfb0e2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a bustling street scene in london, uk. the perspective is from ground level, looking up at the towering buildings that line the street. on the left side of the frame, there are numerous shops and restaurants, with signs and\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: the image depicts a vibrant street scene. on the left side, there is a red octagonal stop sign with white lettering that reads \" stop \" in bold black letters. in the center of the frame, a yellow diamond - shaped sign stands out against\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their enclosure. the elephant on the left stands tall and proud, its gray skin contrasting with the lush green grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2756\n",
            "Validation CIDEr: 0.0152\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2252\n",
            "Validation METEOR: 0.1034\n",
            "New best model saved at epoch 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 5/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15de7f5ee4fe424c9dd39f323b9cc1a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Avg. Training Loss: 0.6189 (Time: 318.14s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efa4f9246f764f7db1c2978a1add57f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a serene park scene. in the foreground, there is a wooden bench with a curved backrest and armrests, situated on a concrete path surrounded by lush greenery. beyond the bench, the park is lush with trees\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of pizza and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their enclosure. the elephant on the left stands tall and proud, its gray skin contrasting with the lush green grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2763\n",
            "Validation CIDEr: 0.0124\n",
            "Validation SPICE: 0.1108\n",
            "Validation ROUGE: 0.2192\n",
            "Validation METEOR: 0.1023\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 6/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25a756baeb164219afea32edfb3abd94"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Avg. Training Loss: 0.5876 (Time: 522.64s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e0b504da9cc4457b2c58231077728a4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, indicating the time as 12 : 30. atop the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered on a cobblestone street. the ground is covered in fallen leaves and debris, creating an urban landscape. some individuals are holding umbrellas to shield themselves from the rain. one person stands out\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2765\n",
            "Validation CIDEr: 0.0152\n",
            "Validation SPICE: 0.1138\n",
            "Validation ROUGE: 0.2226\n",
            "Validation METEOR: 0.1030\n",
            "New best model saved at epoch 6\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 7/7:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae11dcd15ff441f88ffd1aa290d8a08e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-c928b694d1ea>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Avg. Training Loss: 0.5693 (Time: 313.66s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ded3d585ed7d47499d53f7d08c279671"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, indicating the time as 12 : 30. atop the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a red and white checkered tablecloth and features two plates of food - one containing a sandwich and the other holding a salad.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2774\n",
            "Validation CIDEr: 0.0160\n",
            "Validation SPICE: 0.1142\n",
            "Validation ROUGE: 0.2222\n",
            "Validation METEOR: 0.1033\n",
            "New best model saved at epoch 7\n",
            "Training completed!\n",
            "Loading best model from token_gating_checkpoints/best_model.pth for final evaluation\n",
            "Best model was from epoch 7\n",
            "\n",
            "Performing validation set evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91f50719d0914094af76ff908b12fcc7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, indicating the time as 12 : 30. atop the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a red and white checkered tablecloth and features two plates of food - one containing a sandwich and the other holding a salad.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "\n",
            "Performing test set evaluation (first time evaluating on these samples)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "62d548357f6d4ef88945631706369aea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: in the image, a man and a woman are standing on a snowy mountain. the man is wearing a vibrant green jacket with yellow accents and a white helmet, holding a black snowboard with a colorful design. the woman is wearing a purple jacket and a black helmet, holding a pair of red skis. they are both smiling at the camera, their faces lit up with joy. behind them, there ' s a ski lift carrying more adventurers to the top of the mountain.\n",
            "  Prediction: in the image, a man is standing on a sidewalk in an urban setting. he is dressed casually in a white t - shirt and blue jeans, with his hands resting on his hips. the man ' s gaze is directed towards the camera,\n",
            "\n",
            "  Reference: a police officer in a black uniform and helmet is riding a motorcycle with the number \" 2 \" displayed on its side. the officer is holding a flag, possibly indicating an event or celebration. the motorcycle is parked on a street lined with trees and buildings, with a crowd of people gathered around it.\n",
            "  Prediction: in the image, a lively scene unfolds in a park. a group of people are engaged in an energetic game of frisbee, which is suspended in mid - air between them. the players are scattered across the grassy field, some\n",
            "\n",
            "  Reference: in the center of a room with white walls, a group of children in red shirts are gathered around a table. the table is covered with a green tablecloth and holds an array of food items, including small sandwiches, hot dogs, and a bowl of ketchup. a man in a red shirt stands to the left of the table, holding a plate of food for one of the children.\n",
            "  Prediction: the image shows a freshly baked pizza resting on a white plate. the pizza has a golden brown crust and is generously topped with vibrant red sauce, melted cheese, and fresh basil leaves. a silver fork and knife are placed to the left of\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "\n",
            "Validation Set Results:\n",
            "BLEU-4: 0.2774 (27.74)\n",
            "CIDEr: 0.0160 (1.60)\n",
            "SPICE: 0.1142 (11.42)\n",
            "ROUGE: 0.2222 (22.22)\n",
            "METEOR: 0.1033 (10.33)\n",
            "\n",
            "Test Set Results:\n",
            "BLEU-4: 0.2788 (27.88)\n",
            "CIDEr: 0.0217 (2.17)\n",
            "SPICE: 0.0000 (0.00)\n",
            "ROUGE: 0.2262 (22.62)\n",
            "METEOR: 0.1045 (10.45)\n",
            "\n",
            "Validation vs Test Performance:\n",
            "Metric Validation   Test Diff (Test-Val)\n",
            "BLEU-4     0.2774 0.2788          0.0014\n",
            " CIDEr     0.0160 0.0217          0.0057\n",
            " SPICE     0.1142 0.0000         -0.1142\n",
            " ROUGE     0.2222 0.2262          0.0040\n",
            "METEOR     0.1033 0.1045          0.0013\n",
            "Comparison results saved to token_gating_checkpoints/val_test_comparison.csv\n"
          ]
        }
      ]
    }
  ]
}
