{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install pycocoevalcap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6vBGMivmNfUg",
        "outputId": "09b5ca7b-a5df-41b9-8f82-e1e9f9b8e402"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pycocoevalcap in /usr/local/lib/python3.11/dist-packages (1.2)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib pandas tqdm transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svjnZdmRjcAk",
        "outputId": "0d633bba-417a-4aae-81cc-3b646249e502"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2024.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# 1. Token Gating Implementation\n",
        "class TokenGating(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Gating mechanism that selectively focuses on important tokens\n",
        "    while suppressing less relevant ones.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, dropout=0.1):\n",
        "        super(TokenGating, self).__init__()\n",
        "        self.gate_transform = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Calculate importance score for each token [batch_size, seq_len, 1]\n",
        "        gate_scores = self.sigmoid(self.gate_transform(hidden_states))\n",
        "\n",
        "        # Apply scaling factor to ensure stability during training\n",
        "        gate_scores = gate_scores * 2.0\n",
        "\n",
        "        # Apply the gate - element-wise multiplication\n",
        "        gated_output = hidden_states * gate_scores\n",
        "\n",
        "        # Use attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            mask = attention_mask.unsqueeze(-1)\n",
        "            gated_output = gated_output * mask\n",
        "\n",
        "        return gated_output, gate_scores\n",
        "\n",
        "# 2. Sparse Attention Implementation\n",
        "class SparseAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements sparse attention by selecting only the top-k most relevant tokens\n",
        "    for each position during attention computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1, sparsity=0.8):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.sparsity = sparsity  # Percent of attention connections to prune\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # Linear projections and reshape to multi-head\n",
        "        q = self.q_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = self.k_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = self.v_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for attention computation [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            # Expand mask for multi-head attention [batch_size, 1, 1, seq_len]\n",
        "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(expanded_mask == 0, -1e10)\n",
        "\n",
        "        # Compute sparse attention by keeping only top-k values\n",
        "        if self.training:\n",
        "            # Determine number of tokens to keep based on sparsity level\n",
        "            k_tokens = max(1, int((1 - self.sparsity) * seq_len))\n",
        "\n",
        "            # Get top-k values for each query token\n",
        "            top_k_attn, _ = torch.topk(attn_weights, k=k_tokens, dim=-1)\n",
        "\n",
        "            # Use smallest value from top-k as threshold\n",
        "            sparse_threshold = top_k_attn[..., -1].unsqueeze(-1)\n",
        "\n",
        "            # Create a binary mask for sparse attention\n",
        "            sparse_mask = (attn_weights >= sparse_threshold).float()\n",
        "\n",
        "            # Apply the sparse mask\n",
        "            attn_weights = attn_weights * sparse_mask + -1e10 * (1 - sparse_mask)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back to [batch_size, seq_len, hidden_dim]\n",
        "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_dim)\n",
        "\n",
        "        # Final output projection\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 3. Enhanced BLIP Model with Token Gating and Sparse Attention\n",
        "class EnhancedBLIP(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhances BLIP model with token gating and sparse attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(self, sparsity=0.8):\n",
        "        super(EnhancedBLIP, self).__init__()\n",
        "\n",
        "        # Load base model\n",
        "        print(\"Loading base BLIP model...\")\n",
        "        self.base_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "        # Get hidden dimension from the base model\n",
        "        hidden_dim = self.base_model.text_decoder.config.hidden_size\n",
        "        print(f\"Model hidden dimension: {hidden_dim}\")\n",
        "\n",
        "        # Add token gating layers\n",
        "        self.text_gate = TokenGating(hidden_dim)\n",
        "        self.vision_gate = TokenGating(hidden_dim)\n",
        "\n",
        "        # Add sparse attention layers\n",
        "        self.text_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "        self.vision_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "\n",
        "        # Layer norms for stability\n",
        "        self.text_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.text_ln2 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feed-forward networks for residual paths\n",
        "        self.text_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.vision_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Flag to control whether to apply enhancements\n",
        "        self.apply_enhancements = True\n",
        "        print(\"Enhanced BLIP model initialized\")\n",
        "\n",
        "    def _enhance_text_features(self, hidden_states, attention_mask=None):\n",
        "        \"\"\"Apply token gating and sparse attention to text features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.text_gate(hidden_states, attention_mask)\n",
        "        gated_states = self.text_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.text_sparse_attn(gated_states, attention_mask)\n",
        "        sparse_states = self.text_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.text_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _enhance_vision_features(self, hidden_states):\n",
        "        \"\"\"Apply token gating and sparse attention to vision features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.vision_gate(hidden_states, None)\n",
        "        gated_states = self.vision_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.vision_sparse_attn(gated_states, None)\n",
        "        sparse_states = self.vision_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.vision_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, pixel_values=None, input_ids=None, attention_mask=None, labels=None, return_dict=True):\n",
        "        # First pass through base model\n",
        "        outputs = self.base_model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Return base model outputs if enhancements are disabled\n",
        "        if not self.apply_enhancements:\n",
        "            return outputs\n",
        "\n",
        "        # Apply token gating and sparse attention to hidden states\n",
        "        # Note: This is for inference only - the training loss comes from the base model\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, pixel_values=None, input_ids=None, attention_mask=None, **kwargs):\n",
        "        \"\"\"Generate captions using the base model's generation capability.\"\"\"\n",
        "        return self.base_model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "# 4. Data Processing Functions\n",
        "def closest_factors(n):\n",
        "    \"\"\"Finds the closest factors of n to get an aspect ratio close to a square.\"\"\"\n",
        "    sqrt_n = int(math.sqrt(n))\n",
        "    for i in range(sqrt_n, 0, -1):\n",
        "        if n % i == 0:\n",
        "            return i, n // i  # Return H, W such that H Ã— W = n\n",
        "\n",
        "    # If no exact factors, use power of 2 dimensions\n",
        "    size = 2 ** int(math.log2(math.sqrt(n)))\n",
        "    return size, size\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for VAE latents dataset that properly handles\n",
        "    variable length latents, applies normalization, and reshapes for BLIP.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract captions and VAE latents\n",
        "        captions = [item[\"caption\"] for item in batch]\n",
        "        vae_latents = [torch.tensor(item[\"vae_latent\"], dtype=torch.float32) for item in batch]\n",
        "\n",
        "        # Find maximum length for padding\n",
        "        max_len = max([latent.shape[0] for latent in vae_latents])\n",
        "\n",
        "        # Pad tensors\n",
        "        padded_latents = []\n",
        "        for latent in vae_latents:\n",
        "            pad_size = max_len - latent.shape[0]\n",
        "            if pad_size > 0:\n",
        "                padding = torch.zeros(pad_size, dtype=torch.float32)\n",
        "                padded = torch.cat([latent, padding])\n",
        "            else:\n",
        "                padded = latent\n",
        "            padded_latents.append(padded)\n",
        "\n",
        "        # Stack tensors\n",
        "        latents = torch.stack(padded_latents)\n",
        "\n",
        "        # Apply z-score normalization (per sample)\n",
        "        means = latents.mean(dim=1, keepdim=True)\n",
        "        stds = latents.std(dim=1, keepdim=True) + 1e-6  # Avoid division by zero\n",
        "        normalized_latents = (latents - means) / stds\n",
        "\n",
        "        # Reshape for BLIP\n",
        "        batch_size = len(batch)\n",
        "        feature_dim = normalized_latents.shape[1]\n",
        "\n",
        "        # Get dimensions for reshaping\n",
        "        height, width = closest_factors(feature_dim)\n",
        "\n",
        "        # Check if we need to adjust dimensions\n",
        "        if height * width != feature_dim:\n",
        "            # Use power of 2 dimensions and pad/truncate\n",
        "            height = 2 ** int(math.log2(math.sqrt(feature_dim)))\n",
        "            width = height\n",
        "            padded_dim = height * width\n",
        "\n",
        "            if padded_dim > feature_dim:\n",
        "                # Pad each latent\n",
        "                padding = torch.zeros((batch_size, padded_dim - feature_dim), dtype=torch.float32)\n",
        "                normalized_latents = torch.cat([normalized_latents, padding], dim=1)\n",
        "            else:\n",
        "                # Truncate each latent\n",
        "                normalized_latents = normalized_latents[:, :padded_dim]\n",
        "\n",
        "        # Reshape latents to image format (batch_size, channels, height, width)\n",
        "        try:\n",
        "            images = normalized_latents.view(batch_size, 1, height, width)\n",
        "            images = images.repeat(1, 3, 1, 1)  # Repeat along channel dimension for RGB\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error reshaping latents: {e}\")\n",
        "            print(f\"Using fallback reshaping method\")\n",
        "\n",
        "            # Fallback to simple square reshaping\n",
        "            side = int(math.ceil(math.sqrt(feature_dim)))\n",
        "            images = torch.zeros((batch_size, 3, side, side), dtype=torch.float32)\n",
        "\n",
        "            for i, latent in enumerate(normalized_latents):\n",
        "                # Pad if needed\n",
        "                if latent.shape[0] < side * side:\n",
        "                    latent = torch.cat([latent, torch.zeros(side * side - latent.shape[0])])\n",
        "                else:\n",
        "                    latent = latent[:side * side]\n",
        "\n",
        "                # Reshape to square and repeat channels\n",
        "                img = latent.view(1, side, side).repeat(3, 1, 1)\n",
        "                images[i] = img\n",
        "\n",
        "        # Process captions\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        encoded_captions = processor(text=captions, padding=\"max_length\", truncation=True,\n",
        "                                  max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "        # Combine images and captions\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": images.to(torch.float32),\n",
        "            \"input_ids\": encoded_captions[\"input_ids\"],\n",
        "            \"attention_mask\": encoded_captions[\"attention_mask\"],\n",
        "            \"labels\": encoded_captions[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in collate_fn: {e}\")\n",
        "        # Return a minimal valid batch to avoid training failure\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        dummy_captions = [\"dummy caption\"] * len(batch)\n",
        "        encoded = processor(text=dummy_captions, padding=\"max_length\", truncation=True,\n",
        "                           max_length=128, return_tensors=\"pt\")\n",
        "        dummy_images = torch.zeros((len(batch), 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": dummy_images,\n",
        "            \"input_ids\": encoded[\"input_ids\"],\n",
        "            \"attention_mask\": encoded[\"attention_mask\"],\n",
        "            \"labels\": encoded[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "# 5. Training Function with Mixed Precision and Gradient Accumulation\n",
        "def train_model(model, train_loader, val_loader=None, num_epochs=20,  # MODIFIED: Increased from 10 to 20 epochs\n",
        "                lr=2e-5, device=\"cuda\", checkpoint_dir=\"checkpoints\"):\n",
        "    \"\"\"\n",
        "    Train the enhanced BLIP model with mixed precision, gradient accumulation,\n",
        "    and proper checkpointing.\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {num_epochs} epochs\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "    # Gradient accumulation steps (effective batch size = batch_size * accum_steps)\n",
        "    accum_steps = 4\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    best_score = float('-inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"latest_model.pth\")\n",
        "    start_epoch = 1\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "            model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            best_score = checkpoint.get(\"best_score\", float('-inf'))\n",
        "\n",
        "            # Move optimizer states to right device\n",
        "            for state in optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        state[k] = v.to(device)\n",
        "\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            try:\n",
        "                # Move batch to device\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with autocast(enabled=(device == \"cuda\")):\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs.loss / accum_steps  # Scale loss for accumulation\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update weights after accumulation or at the end\n",
        "                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                    # Unscale gradients for clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                    # Clip gradients to prevent explosive values\n",
        "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                    # Optimizer step with scaling\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Track loss\n",
        "                total_loss += loss.item() * accum_steps\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"loss\": f\"{loss.item() * accum_steps:.4f}\"})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_time = time.time() - start_time\n",
        "\n",
        "        print(f\"Epoch {epoch} - Avg. Training Loss: {avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
        "\n",
        "        # Validation phase\n",
        "        if val_loader is not None:\n",
        "            val_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "            # Log validation metrics\n",
        "            print(f\"Validation BLEU-4: {val_metrics['bleu4']:.4f}\")\n",
        "            print(f\"Validation CIDEr: {val_metrics['cider']:.4f}\")\n",
        "            print(f\"Validation SPICE: {val_metrics['spice']:.4f}\")\n",
        "            print(f\"Validation ROUGE: {val_metrics['rouge']:.4f}\")\n",
        "            if 'meteor' in val_metrics and val_metrics['meteor'] > 0:\n",
        "                print(f\"Validation METEOR: {val_metrics['meteor']:.4f}\")\n",
        "\n",
        "            # Use CIDEr + BLEU-4 as overall score for early stopping\n",
        "            current_score = val_metrics['cider'] + val_metrics['bleu4']\n",
        "        else:\n",
        "            # If no validation set, use negative training loss as score\n",
        "            current_score = -avg_loss\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"best_score\": best_score\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Check for improvement\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            no_improve_epochs = 0\n",
        "\n",
        "            # Save best model\n",
        "            best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"best_score\": best_score\n",
        "            }, best_model_path)\n",
        "\n",
        "            print(f\"New best model saved at epoch {epoch}\")\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
        "            break\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n",
        "\n",
        "# 6. Evaluation Function\n",
        "def evaluate_model(model, val_loader, device=\"cuda\", max_samples=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation set with BLEU, CIDEr, SPICE, and ROUGE metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Count samples for potential limit\n",
        "    sample_count = 0\n",
        "\n",
        "    # Process validation batches\n",
        "    for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Generate captions with beam search\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                output_ids = model.generate(\n",
        "                    pixel_values=batch[\"pixel_values\"],\n",
        "                    max_length=50,\n",
        "                    num_beams=5,\n",
        "                    length_penalty=1.0,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "\n",
        "                # Decode generated captions\n",
        "                pred_captions = model.processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "                # Decode reference captions\n",
        "                ref_captions = model.processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "                # Store predictions and references\n",
        "                predictions.extend(pred_captions)\n",
        "                references.extend([[ref] for ref in ref_captions])  # BLEU expects list of references per example\n",
        "\n",
        "                # Update sample count\n",
        "                sample_count += len(pred_captions)\n",
        "\n",
        "                # Print sample predictions (first batch only)\n",
        "                if batch_idx == 0:\n",
        "                    print(\"\\nSample predictions:\")\n",
        "                    for i in range(min(3, len(pred_captions))):\n",
        "                        print(f\"  Reference: {ref_captions[i]}\")\n",
        "                        print(f\"  Prediction: {pred_captions[i]}\")\n",
        "                        print()\n",
        "\n",
        "                # Check if we've processed enough samples\n",
        "                if max_samples is not None and sample_count >= max_samples:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating captions for batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    # Prepare data for other metrics\n",
        "    metric_refs = {i: [ref[0]] for i, ref in enumerate(references)}\n",
        "    metric_preds = {i: [pred] for i, pred in enumerate(predictions)}\n",
        "\n",
        "    # Compute other metrics if available\n",
        "    try:\n",
        "        cider_score = Cider().compute_score(metric_refs, metric_preds)[0]\n",
        "        spice_score = Spice().compute_score(metric_refs, metric_preds)[0]\n",
        "        rouge_score = Rouge().compute_score(metric_refs, metric_preds)[0]\n",
        "\n",
        "        # Try to compute METEOR with fallback\n",
        "        try:\n",
        "            meteor_score = Meteor().compute_score(metric_refs, metric_preds)[0]\n",
        "            print(f\"METEOR score computed successfully: {meteor_score:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error computing METEOR: {e}\")\n",
        "            meteor_score = 0.0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing metrics: {e}\")\n",
        "        cider_score = 0.0\n",
        "        spice_score = 0.0\n",
        "        rouge_score = 0.0\n",
        "        meteor_score = 0.0\n",
        "\n",
        "    return {\n",
        "        \"bleu4\": bleu4,\n",
        "        \"cider\": cider_score,\n",
        "        \"spice\": spice_score,\n",
        "        \"rouge\": rouge_score,\n",
        "        \"meteor\": meteor_score\n",
        "    }\n",
        "\n",
        "# 7. Main function to run the training pipeline\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"SwayStar123/preprocessed_recap-coco30k-moondream\")['train']\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "\n",
        "        # Split dataset into train and validation\n",
        "        split_ratio = 0.9\n",
        "        train_size = int(split_ratio * len(dataset))\n",
        "        train_ds = dataset.select(range(train_size))\n",
        "        val_ds = dataset.select(range(train_size, len(dataset)))\n",
        "        print(f\"Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}\")\n",
        "\n",
        "        # For faster development, uncomment to use a subset\n",
        "        # train_ds = train_ds.select(range(min(5000, len(train_ds))))\n",
        "        # val_ds = val_ds.select(range(min(500, len(val_ds))))\n",
        "        # print(f\"Using subset - Training: {len(train_ds)}, Validation: {len(val_ds)}\")\n",
        "\n",
        "        # Create data loaders with smaller batch size for L4 GPU\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=8,  # Smaller batch size for L4 GPU\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,  # Parallel loading\n",
        "            pin_memory=True  # Faster GPU transfer\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize enhanced model\n",
        "        print(\"Initializing enhanced BLIP model...\")\n",
        "        model = EnhancedBLIP(sparsity=0.7)  # Adjust sparsity as needed\n",
        "        model.to(device)\n",
        "\n",
        "        # Train model\n",
        "        print(\"Starting training...\")\n",
        "        train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=20,  # MODIFIED: Increased from 10 to 20 epochs\n",
        "            lr=2e-5,\n",
        "            device=device,\n",
        "            checkpoint_dir=\"token_gating_checkpoints\"\n",
        "        )\n",
        "\n",
        "        # Final evaluation\n",
        "        print(\"Performing final evaluation...\")\n",
        "        final_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "        print(\"Final Evaluation Results:\")\n",
        "        print(f\"BLEU-4: {final_metrics['bleu4']:.4f}\")\n",
        "        print(f\"CIDEr: {final_metrics['cider']:.4f}\")\n",
        "        print(f\"SPICE: {final_metrics['spice']:.4f}\")\n",
        "        print(f\"ROUGE: {final_metrics['rouge']:.4f}\")\n",
        "        if 'meteor' in final_metrics and final_metrics['meteor'] > 0:\n",
        "            print(f\"METEOR: {final_metrics['meteor']:.4f}\")\n",
        "\n",
        "        # Print scaled metrics for easier comparison\n",
        "        print(\"\\nFinal Evaluation Results (scaled by 100):\")\n",
        "        print(f\"BLEU-4: {final_metrics['bleu4'] * 100:.2f}\")\n",
        "        print(f\"CIDEr: {final_metrics['cider'] * 100:.2f}\")\n",
        "        print(f\"SPICE: {final_metrics['spice'] * 100:.2f}\")\n",
        "        print(f\"ROUGE: {final_metrics['rouge'] * 100:.2f}\")\n",
        "        if 'meteor' in final_metrics and final_metrics['meteor'] > 0:\n",
        "            print(f\"METEOR: {final_metrics['meteor'] * 100:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main function: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "be82e7e94a244bb595ab18fc6de7a1f4",
            "4ff121aa1c6044d294afa5556c3363a6",
            "a8879fe423164c66b200b2a6b3c1d974",
            "6b08039d47cd4c5ea557a6ffcfa10372",
            "3110d53a58c147ceb1e986e567132695",
            "2d23a1a74d124fecb186ce72a3b09ea0",
            "7dbb85fc6d3b484bacfe68a4bbcfcb99",
            "c25cf240711e4f198e4191d25c377c81",
            "7a3bcf50e2a84192ac4405c09e56dd40",
            "5585ec6c203748f0a812b3887b27baa8",
            "4492615e9b864d4ab8f827c1ede0c886",
            "b49d378b306341d9aadea290ff484b5f",
            "4c4297aa90154272bf9d6423587c8f95",
            "b324053bd14b4e758166f4452058135d",
            "838e86418b0e44ee984091220a3b750f",
            "e34f12c3e3a140629c807692c54dafe8",
            "4452aa4d861c42e9b0a7010ec8134fc7",
            "3b503a9dcaaf4a2aa75ce22816de535f",
            "ceb7187bd21a4266bfbbafbbcf71e33f",
            "529ac21368f74ca58b4eacfd11494d5d",
            "40225540d39a4a11b26b4c5a1c1e3115",
            "f54851b2a78748e19a6d572679dedb22",
            "2166185e0a424114bb5d4e1edacabdc6",
            "89e998d0b84c4b959b807b3b18e3b517",
            "e3e900804c064382bd07f719e059f258",
            "7a2c9693b7c6436088f549d236a6e6ef",
            "9efd0e8f387047b7be8bf1fb65b4863c",
            "317b647eaf5b4b23b5c9348184ed3617",
            "00c2767807b841eca6ee6e5bc025f4ee",
            "34b8b101f79a43d7ad46cc9954d43342",
            "184ad8af073143e3815e7eed8c71d01f",
            "c71256bfb7bb4535bbebfe68d6859bab",
            "247a6672626b4e4d9c4e97d9abb59f3c",
            "024caf0eea0544c6bf38533981c077ea",
            "629926342a8e404d9f58b59acbe38c58",
            "92d3be8e08624858a3d531d1f363e382",
            "31bab19ee6d04eb2838e482e1f606563",
            "d7a3bbdfeffb46aaa035a6bd2caf5736",
            "361fb093e73a4360a79312270ef34c80",
            "ff75a6ab32e44e62a78789cba78750b6",
            "6d9a18dacfbe4cd6b8ff39312f92fe6e",
            "9a071b5ee6574487a9ff85c5727eeef6",
            "05aaef7c41d2426aad4906769e573f41",
            "4c06abf4aca64c8abe496481963322d0",
            "48aa6395c51444bf82cc38bd88dfc93a",
            "645b70d7a37c4c3dbbc198cad6a85e91",
            "28fd3dabf9aa441f8996e49998a40870",
            "caebfd9daeb7446ebaf11702edb26034",
            "1403b889d29746b8bd27686f4b4cbab1",
            "0890bf244a584cb6b5a1e9305dcda803",
            "9f22ca74eb3e4e01b2b6b0f537e34306",
            "b5f0ca53b4e840d3b63b11038d4164b9",
            "6effd5afdb8c41aaa92be9fc120ba72f",
            "4ceb2101a4a748688960c92793b80d84",
            "fe81d90060f04d4ab69f323dc4a94c3a",
            "c740c37c5f8847c8a0014fa9035d9104",
            "016bf7d9a4724e8b8d1adb64964a1ce1",
            "12d5416ec52a4cccacc3e7bf3d6b2fe6",
            "40a443a58d1745d2a2628a9c96f842f2",
            "5eb01d52d1634bcd956db06326c33516",
            "ac52e6acbe9347b1843fb466415de7dc",
            "f326bf71282445cd8e42d95f48f135a9",
            "d6950559b10d461897a915ef1bcfe5be",
            "f4a08d26090d46e8ad349c465f3108b1",
            "6ceae18970654766bdeef02f485285d8",
            "10da47727ac84301a67f9ed83067fa99",
            "a454a614766743eb8068caeb07d7986b",
            "845ac6a8c16c4f1d962a3be3103fc3c3",
            "355df4344a2c44aab31afc5c3ac18c73",
            "e8c81119273c465589dee6a175492b27",
            "71937e46cc9f44a3b12b050d4dc5c2d9",
            "6694a99894914a6fbb7639767b4e1cf1",
            "77b97bab373d435d823ecc72f0a82e84",
            "3bfc7401045047ee96eed3016f7548ee",
            "245650649d0844ca9a57690a135d901f",
            "fb61c818f3db43aa8b79d1e7cc71b503",
            "26b56a992067490c8c2c44b118e2af7e",
            "5c30c020d382480794f3abe92b79e3a3",
            "92f5424a233a4aa0ab0886aa74251917",
            "cbf2f858c7f34accb2fd254f08b30786",
            "b644550945974200b404efffbd48927c",
            "753f58dda9474deb8f39f3fa9a38de66",
            "c0c91441f3fe4b36b6dd5616d6012635",
            "a787581c4ab0453d9c717c924e9a1c30",
            "65d9e97ef6bf40d9a7e469a6fdf98a9f",
            "b1feadadb9b24fa5909a8b5ea8b2d6b7",
            "7641e53bc9204cc7bb360085debba688",
            "79e35d4d8de242099c64def0b761e529",
            "0ea82b3c47b84c8d83edda334999c5fc",
            "ae179c14ff23457882c2e017cf6db59e",
            "3ac368ecb3a24600a13823db50b92a35",
            "28f41b23617d4d93834af6f9a0f29c6d",
            "56c1f1cf9ece412b9c7754c96a6d386b",
            "c6a8324735584ee89afdef8961289128",
            "c3c9a0c9417c413aaf602bfde58e5931",
            "b36d45da757f4da1bf8701cb426b5919",
            "5f4f8cfd5cbf46d4b9729dd545e4c618",
            "111a675cec1c4307bd2dbe6589fcce26",
            "8608d9e9138c438898014c1ead6e44c0",
            "78bb0a171d7747438b6b08f9f5b9df46",
            "81090a9c722a4806ab5bd3164a4b1ed1",
            "2749313cd90343cfb60ff83da5469add",
            "80b699862857490889454f3a9279040a",
            "6fda629fe1da435b8c5f88d7e68cf856",
            "be60f3a657bb4e229956439a28d9d79c",
            "6265a170d3de44c2978be8f6682be4e0",
            "15ba1b2d4d6f4432b7b4f68915f4244b",
            "2b7b7589fe444e759ce8d3bbcabe9e57",
            "f09e73b288c643b1a4e54ea1d956894e",
            "54da8e8d9c334abb84c6dcd968e6f9da",
            "1e98aa9baaad4731bae584f7e5f4efeb",
            "712dc869ee484fa78f800ea647b90b32",
            "1c45fd2862384323b286099a34dbf137",
            "92a6a311edc54987a131cc1827a82e7b",
            "bd8612471252402eb166c56e282a83ab",
            "9cd2d669c10447b39a189301a4710525",
            "af242d4f234a4843ba57f638753cd0e3",
            "e0ff1d6ea75c403e96745a6cb7f85833",
            "7ff332f890e945a69fd665ec50fb9d98",
            "f03a51052c574c5a853c75adf655a269",
            "d3bb77daa8bf492080c923137fc09dc7",
            "1654776eb21a4a0ea83dd19013028a29",
            "6bd1b175c2d448f68642e3587f92517c",
            "4aa99d7c7bec454fb075cbc58832c736",
            "ac1d384d190a4a5aa35a1b204dc09854",
            "7cadae27536140c9bba74788d832e73d",
            "ed0659f6549a48cd97fc5eccf38b895d",
            "c0ca6b35ec2448649e56faa3ebaf1e3a",
            "fe41538e543c4b15b2f5c431ae43b349",
            "c87922386d564c098f73a94a746d9b70",
            "50cbaf492ad44c44acd2b9aef73732f4",
            "79cad7f9ebda4ccabefa2194b0632fc2",
            "3cff579d88dd4f7ebc27bbc5ec7d3b1f",
            "1fee414504ad4594b83fe0d7ff551580",
            "e375fac04e7a449c9ecdad282f0ae5fb",
            "72b02ae6c6054a139dd51501a022c387",
            "d8c1f9355ead4aa6885120aa4e709578",
            "898b66d673654b659ebf1da1da8f3d65",
            "90c3dffa5f014969b3b3dfc04204c447",
            "9599001ed7f944d18eba29720e9b46a6",
            "7991c6b9e9f246618eeb2b274f140d5f",
            "83b66cf56b4c4d4ba4a99919475751e5",
            "691e6460a5404a519e1cf5368696a455",
            "16b01293eb074494a2fa3144f16db3c3",
            "a25ce63fee0f461ea654ddc726692b9b",
            "3af9573deeed4d039d7a052c7702167c",
            "0dcb2f098f11494891955c2206db10de",
            "0ba90ae9774c4cbda87a09049b8d6f42",
            "d0e049948d6b4d9cb7b0c5cb807c41b5",
            "99ee6a9562db47179853a381778fc464",
            "36d2917a5fe145da8ebb5cb883d69509",
            "bce8708df4a64469aea1ff47839fa133",
            "da65c6e5790847acb415f7151d721cfa",
            "f8a7ac76f2d0456d9b2da2bbcc2a2b9a",
            "7f65fcc661ed4c42bee9520249f35dd1",
            "1033e85534624bbf85c49c11284c8d00",
            "731390b93bad487d9430a4e2d18649b9",
            "cecb230e33504fdfad6e955e26c68f45",
            "24322953db5f4c02ab28c4a47ab792ce",
            "242b2747df1c42309ff59d483ecabbfd",
            "7fd963b965be4a47b58a0feae37dea55",
            "76da198455184aceadabc935835b604d",
            "099b5c00920b4ab9a31239098fe2e891",
            "5711d66601fe41ad8b351e66ed8b8af3",
            "2f9d1b6368f34fdcb0f692eea5c42621",
            "f92a4edd5b584c54ba9d2376dd35f72e",
            "986052790bb840fd84b6b0f69a1a12e6",
            "e953938e16264dd29f11f30f60a7034d",
            "a16f8a378bb44f14b0c05fffd2236011",
            "ca7df89f48fa4ca59d4f338c7b004c29",
            "df2a5233e22047329aa828246effe379",
            "a0fdb5a10bcc416c9f7ee6b9ec3e7e19",
            "c4d4392363304c80b12e1a34e105f4cb",
            "5987e513fde344e88120eb3f06c91d08",
            "6c9d3f658e7b427fb27015b08d5a162c",
            "e86d38259d434c678154b50579f33aaf",
            "7668517c4ce24e63a5d790ee631df9ee",
            "819a7a5f12b04261b7f979e0d86eb352",
            "b2ea4c9557244895b9ce8f9398dad272",
            "47a0175bf44c4dc09c44d50434847d08",
            "66b5e9c218ef43f799fff9b41c3ce2e0",
            "460ec7a9dce24dc6a751a0e5c8e9815f",
            "29bbefab871444a4a053844a25c9733c",
            "066e800eb804412db4939d2e53713fb4",
            "7453bb9960b74ea7b4ebb6ee8b067dd6",
            "db3e72f9e34645d38647c4701c9b25d9",
            "9f1b4534b050464487cc0df98f45dccf",
            "a108d34d5cb6436f8724ed4879e79c84",
            "321650df66cb4950bfba5fd2c9790620",
            "399773ea00534ec7ad9031836365384c",
            "19e37aaf03334f5181b28fac8d9f91e5",
            "a23e63637e964e3d84cce360c9b707f0",
            "e539265018dc481cb9b76bac15d3b4b5",
            "4237979af587446a809e3f9c44cd5860",
            "e76d69d447114bc097f5a66b9227d30a",
            "75eca3cf8cf246418ca50896813b6184",
            "73e1796e65214699b3cde77b441e6175",
            "ef8fb2c23ff04099949f44e78149ad83",
            "ca0c045881ef4cfa96a321e0d86ebdc8",
            "4a7f8953649b4569b5bdf9614e25daac",
            "97a3eb75541e4ab99f911043b7360a39",
            "57f904f106104d309b6f1ac02a978050",
            "af50a6c4e2904fb095639f0727662799",
            "597b0af38863445aaf25ccb31029a840",
            "7772685ec9b6498dac9e79d2c91d154a",
            "130d105bf6684f1b9c8c71b7033b1af5",
            "6097f81fd387483884ba6c203c44cce1",
            "fc3d1b8742a0439fad142a0dab9e446d",
            "c7539bb11eaf43b996d86dbb9740f876",
            "e089c382207e43f4aa62ef2b130d72b4",
            "59ee8b7139cc47a998001038433ece8a",
            "84713afad2ef49dc86c88cd527688ada",
            "91d81c13c68243a9a92e34e4df8ff7c2",
            "4d78de6dbc2b4d5bb9f6bd74e3df285e",
            "8dc43426e1a14042b4018ce859d3d93a",
            "71622749fcf347738085bfa353e75ad0",
            "e2e66aad39bd498d979fac521b1a763b",
            "3193461fd88b42f2a7bceede8c3d6213",
            "69f5709199e54cff88c6330ad1933788",
            "6a41064cde104009bcc4e1c37a71bb72",
            "6ef82621139d470181898b718c6717cc",
            "27cbce8a9e1e4f98b2b10a21a3b52873",
            "2a10b97ab37241d3a0ce497c6652b58a",
            "08e124c7c060452282678ba059548506",
            "33143a852dfa470f9b5bcb537bd9f94d",
            "cca57d71f72d4607ad3b01d84be0a902",
            "2e2817466ff144ba8d6df99877d82ea2",
            "b080efd264dc4b4f98693694254da6da",
            "d5756cc2490e475f953e0e1a9ba8c2fb",
            "0c8f8c690a344ac993be5c61b496ccc8",
            "6909702c77ca4e8f9be4a83d708c7ed9",
            "a5f4cd857e0a4bf4bfa806e44c8c088a",
            "5547d14ce26a41318862edad9f115e94",
            "81060dffce404718af1e1d9bc0a0c023",
            "73b66123453d420cbb604e008c145595",
            "784b6dcfa0f44dd5887079de7248ae8b",
            "2d6ba6c6e9654112a7a349e417d33643",
            "d62d16d2293f493fb6e75e8fc75da95d",
            "b76ccc417b8645da9faf420c4537580d",
            "2c9df12ec9cd4268a9ea9be7e11416ef",
            "f7e49931fa8448d0bd8b94c8088fe4a2",
            "5167e90e7b2047b49e36cf0f29820258"
          ]
        },
        "id": "eBLmzQ1fsuh3",
        "outputId": "e64f2f31-c6fd-4c12-8d3c-925d6ac7fafc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be82e7e94a244bb595ab18fc6de7a1f4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 30504 samples\n",
            "Training samples: 27453, Validation samples: 3051\n",
            "Initializing enhanced BLIP model...\n",
            "Loading base BLIP model...\n",
            "Model hidden dimension: 768\n",
            "Enhanced BLIP model initialized\n",
            "Starting training...\n",
            "Starting training for 20 epochs\n",
            "Loading checkpoint from token_gating_checkpoints/latest_model.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:381: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device == \"cuda\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resuming training from epoch 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b49d378b306341d9aadea290ff484b5f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Avg. Training Loss: 0.7994 (Time: 351.58s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2166185e0a424114bb5d4e1edacabdc6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: in the image, two men are seated at a table in a restaurant. the man on the left is wearing a blue shirt and has his arm around the other man ' s neck. they are both focused on their laptops, which are open\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: the image depicts a serene beach scene. in the foreground, there is a small wooden boat with a red and white striped awning, resting on the sandy shore. beyond the boat, the vast expanse of the ocean stretches out, meeting the\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man and a woman are seated at a table in what appears to be a restaurant or cafe. the man is wearing a blue shirt and glasses, while the woman is dressed in a black dress. they are both smiling at\n",
            "\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "METEOR score computed successfully: 0.0898\n",
            "Validation BLEU-4: 0.2514\n",
            "Validation CIDEr: 0.0082\n",
            "Validation SPICE: 0.0683\n",
            "Validation ROUGE: 0.2020\n",
            "Validation METEOR: 0.0898\n",
            "New best model saved at epoch 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "024caf0eea0544c6bf38533981c077ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    Exception ignored in: assert self._parent_pid == os.getpid(), 'can only test a child process'<function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "\n",
            " Traceback (most recent call last):\n",
            "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "      self._shutdown_workers() \n",
            "   File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "       if w.is_alive():\n",
            "   ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ \n",
            " AssertionError :  can only test a child process^\n",
            "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Avg. Training Loss: 0.7138 (Time: 381.62s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48aa6395c51444bf82cc38bd88dfc93a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter wearing a white uniform with red accents is mid - swing, while an umpire in a black uniform stands behind him. the field is a vibrant green, contrasting with the brown dirt around home plate\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: in the image, a surfer dressed in a black wetsuit is skillfully riding a wave on a white surfboard. the surfer ' s arms are outstretched, balancing against the force of the wave as it breaks to the right and creates a spray\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a group of people are gathered around a table in a cozy restaurant. the table is adorned with a white tablecloth and features two plates of food - one containing a slice of pizza topped with red sauce and melted cheese. a\n",
            "\n",
            "METEOR score computed successfully: 0.0928\n",
            "Validation BLEU-4: 0.2576\n",
            "Validation CIDEr: 0.0101\n",
            "Validation SPICE: 0.0701\n",
            "Validation ROUGE: 0.2048\n",
            "Validation METEOR: 0.0928\n",
            "New best model saved at epoch 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 4/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c740c37c5f8847c8a0014fa9035d9104"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Avg. Training Loss: 0.6544 (Time: 390.25s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a454a614766743eb8068caeb07d7986b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: a tennis player dressed in a white shirt and black shorts is captured mid - swing on a vibrant green tennis court. the player ' s right hand grips the racket, poised to strike an unseen ball that hangs in the air just above their head\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: the image depicts a serene beach scene. in the foreground, two individuals are walking along the shoreline, one carrying a surfboard and the other wearing a wetsuit. further back, several other people can be seen enjoying the ocean waves. the\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a group of people are gathered around a table in what appears to be a conference or meeting room. the table is covered with a white tablecloth and holds two laptops - one open and active, another closed, and the\n",
            "\n",
            "METEOR score computed successfully: 0.0918\n",
            "Validation BLEU-4: 0.2590\n",
            "Validation CIDEr: 0.0087\n",
            "Validation SPICE: 0.0741\n",
            "Validation ROUGE: 0.2027\n",
            "Validation METEOR: 0.0918\n",
            "New best model saved at epoch 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 5/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c30c020d382480794f3abe92b79e3a3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Avg. Training Loss: 0.6055 (Time: 440.02s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0ea82b3c47b84c8d83edda334999c5fc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: in the image, a man and a woman are engaged in an intense video game battle. the man is wearing a white t - shirt and blue jeans, while the woman is dressed in a black tank top and khaki shorts. they are\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: the image depicts a serene beach scene. in the foreground, there is a small white boat with a blue stripe on its side and a red life preserver attached to it. beyond the boat, the vast expanse of the ocean stretches out,\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: the image depicts a cozy living room with a large window on the left wall, allowing natural light to fill the space. a wooden coffee table sits in front of the window, hosting a vase filled with vibrant red and yellow flowers. to the right\n",
            "\n",
            "METEOR score computed successfully: 0.0947\n",
            "Validation BLEU-4: 0.2604\n",
            "Validation CIDEr: 0.0103\n",
            "Validation SPICE: 0.0784\n",
            "Validation ROUGE: 0.2076\n",
            "Validation METEOR: 0.0947\n",
            "New best model saved at epoch 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 6/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "78bb0a171d7747438b6b08f9f5b9df46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Avg. Training Loss: 0.5618 (Time: 360.81s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e98aa9baaad4731bae584f7e5f4efeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: a tennis player dressed in a white shirt and black shorts is captured mid - swing on a vibrant blue tennis court. the player ' s right hand grips the racket, poised to strike an unseen ball, while their left arm extends behind them for\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: the image depicts a serene beach scene. in the foreground, two individuals are walking towards the water, one carrying a surfboard under their arm and the other holding a bag. further back, three more people can be seen walking along the shoreline\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man is seated at a table in what appears to be a restaurant or cafe. he is wearing a black shirt and has a beard. the table is set with plates, silverware, and glasses, suggesting a meal in\n",
            "\n",
            "METEOR score computed successfully: 0.0925\n",
            "Validation BLEU-4: 0.2553\n",
            "Validation CIDEr: 0.0087\n",
            "Validation SPICE: 0.0788\n",
            "Validation ROUGE: 0.2044\n",
            "Validation METEOR: 0.0925\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 7/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1654776eb21a4a0ea83dd19013028a29"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Avg. Training Loss: 0.5213 (Time: 434.28s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cff579d88dd4f7ebc27bbc5ec7d3b1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter in a white uniform with red stripes is mid - swing, while a catcher in black and an umpire in gray are crouched behind the batter. the field is a vibrant green, contrasting with the\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: in the image, a person is seen kiteboarding on a body of water. the individual is wearing a black wetsuit and is holding onto a red and white kite that is attached to their feet. they are standing on their board, which is\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man and a woman are standing on a sidewalk. the man is dressed in a black suit and tie, while the woman is wearing a white dress. they are both smiling at the camera. behind them, there is a\n",
            "\n",
            "METEOR score computed successfully: 0.0949\n",
            "Validation BLEU-4: 0.2594\n",
            "Validation CIDEr: 0.0098\n",
            "Validation SPICE: 0.0785\n",
            "Validation ROUGE: 0.2101\n",
            "Validation METEOR: 0.0949\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 8/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "16b01293eb074494a2fa3144f16db3c3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Avg. Training Loss: 0.4821 (Time: 431.34s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7f65fcc661ed4c42bee9520249f35dd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: in the image, a man is seated at a table in what appears to be a restaurant or cafe. he is wearing a white shirt and has a beard. the table is set with two plates of food - one containing a slice of cake and\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: in the image, a person is seen kiteboarding on a light blue - green body of water. the individual is wearing a black wetsuit and is holding onto a red and white kite that is attached to their feet with two handles. they are\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man is seated at a table in what appears to be a restaurant or cafe. he is wearing a black shirt and has a beard. the table is covered with a white tablecloth and features two plates of food - one\n",
            "\n",
            "METEOR score computed successfully: 0.0958\n",
            "Validation BLEU-4: 0.2613\n",
            "Validation CIDEr: 0.0117\n",
            "Validation SPICE: 0.0834\n",
            "Validation ROUGE: 0.2092\n",
            "Validation METEOR: 0.0958\n",
            "New best model saved at epoch 8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 9/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f92a4edd5b584c54ba9d2376dd35f72e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Avg. Training Loss: 0.4430 (Time: 440.04s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7668517c4ce24e63a5d790ee631df9ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: in the image, two young men are engaged in a game of frisbee on a grassy field. the man on the left is wearing a white t - shirt and black shorts, and is holding a blue frosbee with both hands.\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: the image depicts a serene beach scene. in the foreground, two individuals are walking along the sandy shore, each carrying a surfboard under their arm. the ocean forms the backdrop, with waves crashing onto the shore and creating a rhythmic pattern of\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man is seated at a desk in what appears to be an office setting. he is wearing a white shirt and has his hands resting on the keyboard of a computer monitor that displays a webpage. the desk is situated against\n",
            "\n",
            "METEOR score computed successfully: 0.0953\n",
            "Validation BLEU-4: 0.2590\n",
            "Validation CIDEr: 0.0095\n",
            "Validation SPICE: 0.0819\n",
            "Validation ROUGE: 0.2075\n",
            "Validation METEOR: 0.0953\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 10/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a108d34d5cb6436f8724ed4879e79c84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7dbc9fff0e00>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Avg. Training Loss: 0.4050 (Time: 384.58s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca0c045881ef4cfa96a321e0d86ebdc8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: the image captures a baseball game in progress. a batter wearing a white uniform with red accents is mid - swing, his body coiled and muscles taut as he prepares to strike the incoming ball. behind him, a catcher in a black uniform crouches\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: in the image, a person wearing a black wetsuit is paddleboarding on a light blue - green body of water. the paddleboard has a white stripe running along its length and is cutting through the water as it glides towards the right side\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man is standing on a sidewalk in an urban setting. he is dressed in a black suit and tie, with his hands tucked into his pockets. the man ' s face is lit up with a smile as he looks directly\n",
            "\n",
            "METEOR score computed successfully: 0.0953\n",
            "Validation BLEU-4: 0.2603\n",
            "Validation CIDEr: 0.0088\n",
            "Validation SPICE: 0.0819\n",
            "Validation ROUGE: 0.2063\n",
            "Validation METEOR: 0.0953\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 11/20:   0%|          | 0/3432 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e089c382207e43f4aa62ef2b130d72b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-606e51a04271>:430: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 - Avg. Training Loss: 0.3681 (Time: 363.47s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ef82621139d470181898b718c6717cc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: in the image, a man and a woman are engaged in a game of frisbee on a grassy field. the man is wearing a white t - shirt and black shorts, while the woman is dressed in an orange tank top and white shorts\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: in the image, a person is seen kiteboarding on a light blue - green body of water. the individual is wearing a black wetsuit and is holding onto a red and white kite that is attached to their feet. they are skillfully maneuver\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man and a woman are standing on a sidewalk. the man is wearing a white shirt and blue jeans, while the woman is dressed in a black dress. they are both smiling at the camera. behind them, there is\n",
            "\n",
            "METEOR score computed successfully: 0.0947\n",
            "Validation BLEU-4: 0.2568\n",
            "Validation CIDEr: 0.0102\n",
            "Validation SPICE: 0.0808\n",
            "Validation ROUGE: 0.2079\n",
            "Validation METEOR: 0.0947\n",
            "No improvement for 3 epochs. Early stopping.\n",
            "Training completed!\n",
            "Performing final evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5f4cd857e0a4bf4bfa806e44c8c088a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a bouquet of flowers in full bloom, including six white daisies with yellow centers and five pink lilies with orange centers. the daisies are arranged in a circular pattern around the center of the bouquet, while the lilies are scattered throughout the arrangement. the background is blurred, suggesting a lush garden or park setting.\n",
            "  Prediction: in the image, a man and a woman are engaged in a game of frisbee on a grassy field. the man is wearing a white t - shirt and black shorts, while the woman is dressed in an orange tank top and white shorts\n",
            "\n",
            "  Reference: in the vast expanse of the ocean, a person is seen kiteboarding. the individual, clad in a black wetsuit, is skillfully maneuvering a vibrant kite that dances in the sky. the kite, adorned with hues of green and purple, stands out against the backdrop of the cloudy sky. it ' s tethered to the person by a sturdy rope, which they hold firmly in their hands. the person is riding on a board, cutting through the choppy waters of the ocean.\n",
            "  Prediction: in the image, a person is seen kiteboarding on a light blue - green body of water. the individual is wearing a black wetsuit and is holding onto a red and white kite that is attached to their feet. they are skillfully maneuver\n",
            "\n",
            "  Reference: in the image, two women are walking on a sidewalk in front of a white building. the woman on the left is wearing a white t - shirt with \" swinging \" printed on it and blue pants, holding an umbrella that has the wilson logo. her companion on the right is wearing a pink floral top and black shorts, also carrying an umbrella. they are both wearing sandals.\n",
            "  Prediction: in the image, a man and a woman are standing on a sidewalk. the man is wearing a white shirt and blue jeans, while the woman is dressed in a black dress. they are both smiling at the camera. behind them, there is\n",
            "\n",
            "METEOR score computed successfully: 0.0947\n",
            "Final Evaluation Results:\n",
            "BLEU-4: 0.2568\n",
            "CIDEr: 0.0102\n",
            "SPICE: 0.0808\n",
            "ROUGE: 0.2079\n",
            "METEOR: 0.0947\n",
            "\n",
            "Final Evaluation Results (scaled by 100):\n",
            "BLEU-4: 25.68\n",
            "CIDEr: 1.02\n",
            "SPICE: 8.08\n",
            "ROUGE: 20.79\n",
            "METEOR: 9.47\n"
          ]
        }
      ]
    }
  ]
}
