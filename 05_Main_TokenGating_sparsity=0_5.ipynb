{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-df1nYj8Xk-W",
        "outputId": "46ed5606-0fe8-4fad-cd10-abcb548edd45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib pandas tqdm transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UbZgszAGXmSq",
        "outputId": "800bf513-0b85-488c-c16b-9b18705f9335"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Token Gating Implementation\n",
        "class TokenGating(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Gating mechanism that selectively focuses on important tokens\n",
        "    while suppressing less relevant ones.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, dropout=0.1):\n",
        "        super(TokenGating, self).__init__()\n",
        "        self.gate_transform = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Calculate importance score for each token [batch_size, seq_len, 1]\n",
        "        gate_scores = self.sigmoid(self.gate_transform(hidden_states))\n",
        "\n",
        "        # Apply scaling factor to ensure stability during training\n",
        "        gate_scores = gate_scores * 2.0\n",
        "\n",
        "        # Apply the gate - element-wise multiplication\n",
        "        gated_output = hidden_states * gate_scores\n",
        "\n",
        "        # Use attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            mask = attention_mask.unsqueeze(-1)\n",
        "            gated_output = gated_output * mask\n",
        "\n",
        "        return gated_output, gate_scores\n",
        "\n",
        "# 2. Sparse Attention Implementation\n",
        "class SparseAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements sparse attention by selecting only the top-k most relevant tokens\n",
        "    for each position during attention computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1, sparsity=0.5):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.sparsity = sparsity  # Percent of attention connections to prune\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # Linear projections and reshape to multi-head\n",
        "        q = self.q_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = self.k_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = self.v_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for attention computation [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            # Expand mask for multi-head attention [batch_size, 1, 1, seq_len]\n",
        "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(expanded_mask == 0, -1e10)\n",
        "\n",
        "        # Compute sparse attention by keeping only top-k values\n",
        "        if self.training:\n",
        "            # Determine number of tokens to keep based on sparsity level\n",
        "            k_tokens = max(1, int((1 - self.sparsity) * seq_len))\n",
        "\n",
        "            # Get top-k values for each query token\n",
        "            top_k_attn, _ = torch.topk(attn_weights, k=k_tokens, dim=-1)\n",
        "\n",
        "            # Use smallest value from top-k as threshold\n",
        "            sparse_threshold = top_k_attn[..., -1].unsqueeze(-1)\n",
        "\n",
        "            # Create a binary mask for sparse attention\n",
        "            sparse_mask = (attn_weights >= sparse_threshold).float()\n",
        "\n",
        "            # Apply the sparse mask\n",
        "            attn_weights = attn_weights * sparse_mask + -1e10 * (1 - sparse_mask)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back to [batch_size, seq_len, hidden_dim]\n",
        "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_dim)\n",
        "\n",
        "        # Final output projection\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 3. Enhanced BLIP Model with Token Gating and Sparse Attention\n",
        "class EnhancedBLIP(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhances BLIP model with token gating and sparse attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(self, sparsity=0.8):\n",
        "        super(EnhancedBLIP, self).__init__()\n",
        "\n",
        "        # Load base model\n",
        "        print(\"Loading base BLIP model...\")\n",
        "        self.base_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "        # Get hidden dimension from the base model\n",
        "        hidden_dim = self.base_model.text_decoder.config.hidden_size\n",
        "        print(f\"Model hidden dimension: {hidden_dim}\")\n",
        "\n",
        "        # Add token gating layers\n",
        "        self.text_gate = TokenGating(hidden_dim)\n",
        "        self.vision_gate = TokenGating(hidden_dim)\n",
        "\n",
        "        # Add sparse attention layers\n",
        "        self.text_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "        self.vision_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "\n",
        "        # Layer norms for stability\n",
        "        self.text_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.text_ln2 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feed-forward networks for residual paths\n",
        "        self.text_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.vision_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Flag to control whether to apply enhancements\n",
        "        self.apply_enhancements = True\n",
        "        print(\"Enhanced BLIP model initialized\")\n",
        "\n",
        "    def _enhance_text_features(self, hidden_states, attention_mask=None):\n",
        "        \"\"\"Apply token gating and sparse attention to text features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.text_gate(hidden_states, attention_mask)\n",
        "        gated_states = self.text_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.text_sparse_attn(gated_states, attention_mask)\n",
        "        sparse_states = self.text_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.text_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _enhance_vision_features(self, hidden_states):\n",
        "        \"\"\"Apply token gating and sparse attention to vision features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.vision_gate(hidden_states, None)\n",
        "        gated_states = self.vision_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.vision_sparse_attn(gated_states, None)\n",
        "        sparse_states = self.vision_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.vision_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, pixel_values=None, input_ids=None, attention_mask=None, labels=None, return_dict=True):\n",
        "        # First pass through base model\n",
        "        outputs = self.base_model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Return base model outputs if enhancements are disabled\n",
        "        if not self.apply_enhancements:\n",
        "            return outputs\n",
        "\n",
        "        # Apply token gating and sparse attention to hidden states\n",
        "        # Note: This is for inference only - the training loss comes from the base model\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, pixel_values=None, input_ids=None, attention_mask=None, **kwargs):\n",
        "        \"\"\"Generate captions using the base model's generation capability.\"\"\"\n",
        "        return self.base_model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "# 4. Data Processing Functions\n",
        "def closest_factors(n):\n",
        "    \"\"\"Finds the closest factors of n to get an aspect ratio close to a square.\"\"\"\n",
        "    sqrt_n = int(math.sqrt(n))\n",
        "    for i in range(sqrt_n, 0, -1):\n",
        "        if n % i == 0:\n",
        "            return i, n // i  # Return H, W such that H × W = n\n",
        "\n",
        "    # If no exact factors, use power of 2 dimensions\n",
        "    size = 2 ** int(math.log2(math.sqrt(n)))\n",
        "    return size, size\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for VAE latents dataset that properly handles\n",
        "    variable length latents, applies normalization, and reshapes for BLIP.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract captions and VAE latents\n",
        "        captions = [item[\"caption\"] for item in batch]\n",
        "        vae_latents = [torch.tensor(item[\"vae_latent\"], dtype=torch.float32) for item in batch]\n",
        "\n",
        "        # Find maximum length for padding\n",
        "        max_len = max([latent.shape[0] for latent in vae_latents])\n",
        "\n",
        "        # Pad tensors\n",
        "        padded_latents = []\n",
        "        for latent in vae_latents:\n",
        "            pad_size = max_len - latent.shape[0]\n",
        "            if pad_size > 0:\n",
        "                padding = torch.zeros(pad_size, dtype=torch.float32)\n",
        "                padded = torch.cat([latent, padding])\n",
        "            else:\n",
        "                padded = latent\n",
        "            padded_latents.append(padded)\n",
        "\n",
        "        # Stack tensors\n",
        "        latents = torch.stack(padded_latents)\n",
        "\n",
        "        # Apply z-score normalization (per sample)\n",
        "        means = latents.mean(dim=1, keepdim=True)\n",
        "        stds = latents.std(dim=1, keepdim=True) + 1e-6  # Avoid division by zero\n",
        "        normalized_latents = (latents - means) / stds\n",
        "\n",
        "        # Reshape for BLIP\n",
        "        batch_size = len(batch)\n",
        "        feature_dim = normalized_latents.shape[1]\n",
        "\n",
        "        # Get dimensions for reshaping\n",
        "        height, width = closest_factors(feature_dim)\n",
        "\n",
        "        # Check if we need to adjust dimensions\n",
        "        if height * width != feature_dim:\n",
        "            # Use power of 2 dimensions and pad/truncate\n",
        "            height = 2 ** int(math.log2(math.sqrt(feature_dim)))\n",
        "            width = height\n",
        "            padded_dim = height * width\n",
        "\n",
        "            if padded_dim > feature_dim:\n",
        "                # Pad each latent\n",
        "                padding = torch.zeros((batch_size, padded_dim - feature_dim), dtype=torch.float32)\n",
        "                normalized_latents = torch.cat([normalized_latents, padding], dim=1)\n",
        "            else:\n",
        "                # Truncate each latent\n",
        "                normalized_latents = normalized_latents[:, :padded_dim]\n",
        "\n",
        "        # Reshape latents to image format (batch_size, channels, height, width)\n",
        "        try:\n",
        "            images = normalized_latents.view(batch_size, 1, height, width)\n",
        "            images = images.repeat(1, 3, 1, 1)  # Repeat along channel dimension for RGB\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error reshaping latents: {e}\")\n",
        "            print(f\"Using fallback reshaping method\")\n",
        "\n",
        "            # Fallback to simple square reshaping\n",
        "            side = int(math.ceil(math.sqrt(feature_dim)))\n",
        "            images = torch.zeros((batch_size, 3, side, side), dtype=torch.float32)\n",
        "\n",
        "            for i, latent in enumerate(normalized_latents):\n",
        "                # Pad if needed\n",
        "                if latent.shape[0] < side * side:\n",
        "                    latent = torch.cat([latent, torch.zeros(side * side - latent.shape[0])])\n",
        "                else:\n",
        "                    latent = latent[:side * side]\n",
        "\n",
        "                # Reshape to square and repeat channels\n",
        "                img = latent.view(1, side, side).repeat(3, 1, 1)\n",
        "                images[i] = img\n",
        "\n",
        "        # Process captions\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        encoded_captions = processor(text=captions, padding=\"max_length\", truncation=True,\n",
        "                                  max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "        # Combine images and captions\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": images.to(torch.float32),\n",
        "            \"input_ids\": encoded_captions[\"input_ids\"],\n",
        "            \"attention_mask\": encoded_captions[\"attention_mask\"],\n",
        "            \"labels\": encoded_captions[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in collate_fn: {e}\")\n",
        "        # Return a minimal valid batch to avoid training failure\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        dummy_captions = [\"dummy caption\"] * len(batch)\n",
        "        encoded = processor(text=dummy_captions, padding=\"max_length\", truncation=True,\n",
        "                           max_length=128, return_tensors=\"pt\")\n",
        "        dummy_images = torch.zeros((len(batch), 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": dummy_images,\n",
        "            \"input_ids\": encoded[\"input_ids\"],\n",
        "            \"attention_mask\": encoded[\"attention_mask\"],\n",
        "            \"labels\": encoded[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "# 5. Training Function with Mixed Precision and Gradient Accumulation\n",
        "def train_model(model, train_loader, val_loader=None, num_epochs=10,\n",
        "                lr=2e-5, device=\"cuda\", checkpoint_dir=\"checkpoints\"):\n",
        "    \"\"\"\n",
        "    Train the enhanced BLIP model with mixed precision, gradient accumulation,\n",
        "    and proper checkpointing.\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {num_epochs} epochs\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "    # Gradient accumulation steps (effective batch size = batch_size * accum_steps)\n",
        "    accum_steps = 4\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    best_score = float('-inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    # Tracking metrics for saving\n",
        "    train_losses = []\n",
        "    val_bleu_scores = []\n",
        "    val_cider_scores = []\n",
        "    val_spice_scores = []\n",
        "    val_rouge_scores = []\n",
        "    val_meteor_scores = []\n",
        "\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"latest_model.pth\")\n",
        "    start_epoch = 1\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "            model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            best_score = checkpoint.get(\"best_score\", float('-inf'))\n",
        "\n",
        "            # Move optimizer states to right device\n",
        "            for state in optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        state[k] = v.to(device)\n",
        "\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            try:\n",
        "                # Move batch to device\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with autocast(enabled=(device == \"cuda\")):\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs.loss / accum_steps  # Scale loss for accumulation\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update weights after accumulation or at the end\n",
        "                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                    # Unscale gradients for clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                    # Clip gradients to prevent explosive values\n",
        "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                    # Optimizer step with scaling\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Track loss\n",
        "                total_loss += loss.item() * accum_steps\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"loss\": f\"{loss.item() * accum_steps:.4f}\"})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_time = time.time() - start_time\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch} - Avg. Training Loss: {avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
        "\n",
        "        # Validation phase\n",
        "        if val_loader is not None:\n",
        "            val_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "            # Store metrics\n",
        "            val_bleu_scores.append(val_metrics['bleu4'])\n",
        "            val_cider_scores.append(val_metrics['cider'])\n",
        "            val_spice_scores.append(val_metrics['spice'])\n",
        "            val_rouge_scores.append(val_metrics['rouge'])\n",
        "            val_meteor_scores.append(val_metrics.get('meteor', 0.0))\n",
        "\n",
        "            # Log validation metrics\n",
        "            print(f\"Validation BLEU-4: {val_metrics['bleu4']:.4f}\")\n",
        "            print(f\"Validation CIDEr: {val_metrics['cider']:.4f}\")\n",
        "            print(f\"Validation SPICE: {val_metrics['spice']:.4f}\")\n",
        "            print(f\"Validation ROUGE: {val_metrics['rouge']:.4f}\")\n",
        "            if 'meteor' in val_metrics and val_metrics['meteor'] > 0:\n",
        "                print(f\"Validation METEOR: {val_metrics['meteor']:.4f}\")\n",
        "\n",
        "            # Use CIDEr + BLEU-4 as overall score for early stopping\n",
        "            current_score = val_metrics['cider'] + val_metrics['bleu4']\n",
        "        else:\n",
        "            # If no validation set, use negative training loss as score\n",
        "            current_score = -avg_loss\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"best_score\": best_score,\n",
        "            \"train_losses\": train_losses,\n",
        "            \"val_bleu_scores\": val_bleu_scores,\n",
        "            \"val_cider_scores\": val_cider_scores,\n",
        "            \"val_spice_scores\": val_spice_scores,\n",
        "            \"val_rouge_scores\": val_rouge_scores,\n",
        "            \"val_meteor_scores\": val_meteor_scores\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Check for improvement\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            no_improve_epochs = 0\n",
        "\n",
        "            # Save best model\n",
        "            best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"best_score\": best_score,\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_bleu_scores\": val_bleu_scores,\n",
        "                \"val_cider_scores\": val_cider_scores,\n",
        "                \"val_spice_scores\": val_spice_scores,\n",
        "                \"val_rouge_scores\": val_rouge_scores,\n",
        "                \"val_meteor_scores\": val_meteor_scores\n",
        "            }, best_model_path)\n",
        "\n",
        "            print(f\"New best model saved at epoch {epoch}\")\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
        "            break\n",
        "\n",
        "    # Save training history\n",
        "    history = {\n",
        "        \"epochs\": list(range(1, len(train_losses) + 1)),\n",
        "        \"train_loss\": train_losses,\n",
        "        \"val_bleu4\": val_bleu_scores,\n",
        "        \"val_cider\": val_cider_scores,\n",
        "        \"val_spice\": val_spice_scores,\n",
        "        \"val_rouge\": val_rouge_scores,\n",
        "        \"val_meteor\": val_meteor_scores\n",
        "    }\n",
        "\n",
        "    history_path = os.path.join(checkpoint_dir, \"training_history.pt\")\n",
        "    torch.save(history, history_path)\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n",
        "\n",
        "# 6. Evaluation Function\n",
        "def evaluate_model(model, val_loader, device=\"cuda\", max_samples=None):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation set with BLEU, CIDEr, SPICE, and ROUGE metrics.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Count samples for potential limit\n",
        "    sample_count = 0\n",
        "\n",
        "    # Process validation batches\n",
        "    for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Generate captions with beam search\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                output_ids = model.generate(\n",
        "                    pixel_values=batch[\"pixel_values\"],\n",
        "                    max_length=50,\n",
        "                    num_beams=5,\n",
        "                    length_penalty=1.0,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "\n",
        "                # Decode generated captions\n",
        "                pred_captions = model.processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "                # Decode reference captions\n",
        "                ref_captions = model.processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "                # Store predictions and references\n",
        "                predictions.extend(pred_captions)\n",
        "                references.extend([[ref] for ref in ref_captions])  # BLEU expects list of references per example\n",
        "\n",
        "                # Update sample count\n",
        "                sample_count += len(pred_captions)\n",
        "\n",
        "                # Print sample predictions (first batch only)\n",
        "                if batch_idx == 0:\n",
        "                    print(\"\\nSample predictions:\")\n",
        "                    for i in range(min(3, len(pred_captions))):\n",
        "                        print(f\"  Reference: {ref_captions[i]}\")\n",
        "                        print(f\"  Prediction: {pred_captions[i]}\")\n",
        "                        print()\n",
        "\n",
        "                # Check if we've processed enough samples\n",
        "                if max_samples is not None and sample_count >= max_samples:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating captions for batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    # Prepare data for other metrics\n",
        "    metric_refs = {i: [ref[0]] for i, ref in enumerate(references)}\n",
        "    metric_preds = {i: [pred] for i, pred in enumerate(predictions)}\n",
        "\n",
        "    # Compute other metrics with better error handling\n",
        "    cider_score = 0.0\n",
        "    spice_score = 0.0\n",
        "    rouge_score = 0.0\n",
        "    meteor_score = 0.0\n",
        "\n",
        "    # Try CIDEr first\n",
        "    try:\n",
        "        cider_score = Cider().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"CIDEr score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing CIDEr: {e}\")\n",
        "\n",
        "    # Try ROUGE next\n",
        "    try:\n",
        "        rouge_score = Rouge().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"ROUGE score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing ROUGE: {e}\")\n",
        "\n",
        "    # Try SPICE with timeout handling\n",
        "    try:\n",
        "        import threading\n",
        "        import time\n",
        "\n",
        "        def compute_spice_with_timeout():\n",
        "            nonlocal spice_score\n",
        "            try:\n",
        "                spice_score = Spice().compute_score(metric_refs, metric_preds)[0]\n",
        "            except Exception as e:\n",
        "                print(f\"Error in SPICE thread: {e}\")\n",
        "\n",
        "        # Start SPICE in a separate thread with timeout\n",
        "        print(\"Starting SPICE computation (may take time)...\")\n",
        "        spice_thread = threading.Thread(target=compute_spice_with_timeout)\n",
        "        spice_thread.daemon = True\n",
        "        spice_thread.start()\n",
        "\n",
        "        # Wait for 3 minutes max\n",
        "        spice_thread.join(timeout=180)\n",
        "\n",
        "        if spice_thread.is_alive():\n",
        "            print(\"Warning: SPICE computation timed out after 3 minutes, skipping.\")\n",
        "        else:\n",
        "            print(\"SPICE score computed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up SPICE computation: {e}\")\n",
        "        print(\"Skipping SPICE evaluation\")\n",
        "\n",
        "    # Try METEOR\n",
        "    try:\n",
        "        meteor_score = Meteor().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"METEOR score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing METEOR: {e}\")\n",
        "\n",
        "    return {\n",
        "        \"bleu4\": bleu4,\n",
        "        \"cider\": cider_score,\n",
        "        \"spice\": spice_score,\n",
        "        \"rouge\": rouge_score,\n",
        "        \"meteor\": meteor_score\n",
        "    }\n",
        "\n",
        "# 7. Main function to run the training pipeline with train/val/test split\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"SwayStar123/preprocessed_recap-coco30k-moondream\")['train']\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "\n",
        "        # Create a proper train/val/test split (80%/10%/10%)\n",
        "        total_size = len(dataset)\n",
        "\n",
        "        # Shuffle the dataset indices with a fixed seed for reproducibility\n",
        "        shuffled_indices = torch.randperm(total_size).tolist()\n",
        "\n",
        "        # Calculate split sizes\n",
        "        train_size = int(0.8 * total_size)\n",
        "        val_size = int(0.1 * total_size)\n",
        "        test_size = total_size - train_size - val_size\n",
        "\n",
        "        # Create the splits\n",
        "        train_indices = shuffled_indices[:train_size]\n",
        "        val_indices = shuffled_indices[train_size:train_size+val_size]\n",
        "        test_indices = shuffled_indices[train_size+val_size:]\n",
        "\n",
        "        train_ds = dataset.select(train_indices)\n",
        "        val_ds = dataset.select(val_indices)\n",
        "        test_ds = dataset.select(test_indices)\n",
        "\n",
        "        print(f\"Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}, Test samples: {len(test_ds)}\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize enhanced model\n",
        "        print(\"Initializing enhanced BLIP model...\")\n",
        "        model = EnhancedBLIP(sparsity=0.7)  # Keeping your preferred sparsity value\n",
        "        model.to(device)\n",
        "\n",
        "        # Train model\n",
        "        print(\"Starting training...\")\n",
        "        train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=10,  # Setting to 10 epochs based on previous results\n",
        "            lr=2e-5,\n",
        "            device=device,\n",
        "            checkpoint_dir=\"token_gating_checkpoints\"\n",
        "        )\n",
        "\n",
        "        # Load best model for evaluation\n",
        "        best_model_path = os.path.join(\"token_gating_checkpoints\", \"best_model.pth\")\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(f\"Loading best model from {best_model_path} for final evaluation\")\n",
        "            checkpoint = torch.load(best_model_path, map_location=device)\n",
        "            model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            best_epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
        "            print(f\"Best model was from epoch {best_epoch}\")\n",
        "\n",
        "        # Evaluate on validation set\n",
        "        print(\"\\nPerforming validation set evaluation...\")\n",
        "        val_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "        # Evaluate on test set\n",
        "        print(\"\\nPerforming test set evaluation (first time evaluating on these samples)...\")\n",
        "        test_metrics = evaluate_model(model, test_loader, device=device)\n",
        "\n",
        "        # Print validation results\n",
        "        print(\"\\nValidation Set Results:\")\n",
        "        print(f\"BLEU-4: {val_metrics['bleu4']:.4f} ({val_metrics['bleu4'] * 100:.2f})\")\n",
        "        print(f\"CIDEr: {val_metrics['cider']:.4f} ({val_metrics['cider'] * 100:.2f})\")\n",
        "        print(f\"SPICE: {val_metrics['spice']:.4f} ({val_metrics['spice'] * 100:.2f})\")\n",
        "        print(f\"ROUGE: {val_metrics['rouge']:.4f} ({val_metrics['rouge'] * 100:.2f})\")\n",
        "        if 'meteor' in val_metrics and val_metrics['meteor'] > 0:\n",
        "            print(f\"METEOR: {val_metrics['meteor']:.4f} ({val_metrics['meteor'] * 100:.2f})\")\n",
        "\n",
        "        # Print test results\n",
        "        print(\"\\nTest Set Results:\")\n",
        "        print(f\"BLEU-4: {test_metrics['bleu4']:.4f} ({test_metrics['bleu4'] * 100:.2f})\")\n",
        "        print(f\"CIDEr: {test_metrics['cider']:.4f} ({test_metrics['cider'] * 100:.2f})\")\n",
        "        print(f\"SPICE: {test_metrics['spice']:.4f} ({test_metrics['spice'] * 100:.2f})\")\n",
        "        print(f\"ROUGE: {test_metrics['rouge']:.4f} ({test_metrics['rouge'] * 100:.2f})\")\n",
        "        if 'meteor' in test_metrics and test_metrics['meteor'] > 0:\n",
        "            print(f\"METEOR: {test_metrics['meteor']:.4f} ({test_metrics['meteor'] * 100:.2f})\")\n",
        "\n",
        "        # Save test results\n",
        "        test_results_path = os.path.join(\"token_gating_checkpoints\", \"test_results.pt\")\n",
        "        torch.save({\n",
        "            \"bleu4\": test_metrics['bleu4'],\n",
        "            \"cider\": test_metrics['cider'],\n",
        "            \"spice\": test_metrics['spice'],\n",
        "            \"rouge\": test_metrics['rouge'],\n",
        "            \"meteor\": test_metrics.get('meteor', 0.0)\n",
        "        }, test_results_path)\n",
        "\n",
        "        # Compare validation and test results\n",
        "        print(\"\\nValidation vs Test Performance:\")\n",
        "        metrics_comparison = {\n",
        "            'Metric': ['BLEU-4', 'CIDEr', 'SPICE', 'ROUGE', 'METEOR'],\n",
        "            'Validation': [\n",
        "                f\"{val_metrics['bleu4']:.4f}\",\n",
        "                f\"{val_metrics['cider']:.4f}\",\n",
        "                f\"{val_metrics['spice']:.4f}\",\n",
        "                f\"{val_metrics['rouge']:.4f}\",\n",
        "                f\"{val_metrics.get('meteor', 0.0):.4f}\"\n",
        "            ],\n",
        "            'Test': [\n",
        "                f\"{test_metrics['bleu4']:.4f}\",\n",
        "                f\"{test_metrics['cider']:.4f}\",\n",
        "                f\"{test_metrics['spice']:.4f}\",\n",
        "                f\"{test_metrics['rouge']:.4f}\",\n",
        "                f\"{test_metrics.get('meteor', 0.0):.4f}\"\n",
        "            ],\n",
        "            'Diff (Test-Val)': [\n",
        "                f\"{test_metrics['bleu4'] - val_metrics['bleu4']:.4f}\",\n",
        "                f\"{test_metrics['cider'] - val_metrics['cider']:.4f}\",\n",
        "                f\"{test_metrics['spice'] - val_metrics['spice']:.4f}\",\n",
        "                f\"{test_metrics['rouge'] - val_metrics['rouge']:.4f}\",\n",
        "                f\"{test_metrics.get('meteor', 0.0) - val_metrics.get('meteor', 0.0):.4f}\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        comparison_df = pd.DataFrame(metrics_comparison)\n",
        "        print(comparison_df.to_string(index=False))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main function: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a6853dad34ad41fd940f0182f607025a",
            "621cd557841144eaa37c8e7300284f87",
            "1480fcacb14d41958d8cc6375b25ea8b",
            "baa1ddaa66c643c7820006e671653a35",
            "a4c2a39c64bf4d78a969ba543af6740e",
            "d511b11682dd4dbfa48e2315ed42d23c",
            "25496d0ad1714aa989fdcbf308cdf6d5",
            "578b9d830b024479a9e6f071f268911e",
            "27e8d32c7a454cee8e16e689ef27be3c",
            "06f5d49a2a3a4e71ad214dfd544da4b7",
            "fce1d9d7841f41cb869091803493b04e",
            "affbbfd219ba4604bcfcb3959866c72c",
            "2dcc87668af942f7a1a12c71013fdd49",
            "47af3e9f6f81412e9c2016febb0505d9",
            "1dc3c79a4e5d4d929aad2ed768439667",
            "f7406051d55a4787b218d7d59db9a2aa",
            "c2033954b1d2455cb210e3d35d9b7acc",
            "aa2d2b9275ba48f3bf30ebe9c3920122",
            "ce7db211e42945daabb5f43bf44d6039",
            "48b5fc7979804e6c878bc78a1d4c8104",
            "d5cc2e14f6e34be4af0e92ba948d5a86",
            "11ffde1106a44dfa8b37ea701062ee02",
            "0aaadeda48ed4202bba92e37107f7a3c",
            "84f5d10b02394e1cb5173dc087c9a64e",
            "00ea1401f4de412689839db923176075",
            "db36e9fdb29d44e3a830f6bbdedc133e",
            "d78d8a270e4d4537a1dba1411fa3bd31",
            "5f1c10d2795740fdb010604fe922f74a",
            "73923ba67d2f456f8dbc94b7096696b6",
            "2e27728e46ea49889c544c02a20ef65a",
            "c6a24b0db4dd4924b2abc6fa502906a8",
            "dff8afb4fc224ee0bfb1c82e474ffa21",
            "6f480e6975ad43eeaa37c61627c1122a",
            "aba5db74473e4209a562ad4b9d49cf9b",
            "a8421b10f4e247e2b0ddadd250765d26",
            "9e111e15c5694009b57e6b46558ee0fe",
            "ba9f454f5b734bffa8d26ec1c13fce2d",
            "8fadd1a91476431ea471cc3814fbe550",
            "2bf4c2d5c28947f491b4ab78971e9f12",
            "c1c3bbe1f06f46e6b075110fbc36d296",
            "f176993476744de0b123a6bb17ad7969",
            "d5fc4fe681f4482282e516fbe2583959",
            "ae4fe756c1284d03b6588e7e6a062c80",
            "cc6f16e38b6e48ea81d142831d511501",
            "a2b268936a264017a2ebebf5bca9c174",
            "d948914edd444398b58759e0ea68e1f8",
            "51021fe8cf214dd2afa5e513d46e6294",
            "ceb4b70c5c5d4150ac4ffc88fbea1848",
            "ef26625f74644d409c89c0a54d6e8b85",
            "c168650c9b8d4461ac536669e5fc9d89",
            "673fd86aecf04eb58a3a03e0a3db4277",
            "99620308f30049dab01daa95dd6f83f7",
            "257dea9f744c4b7b99d60500bad446be",
            "7f2e79096b1f41e8b70f044ed10718c1",
            "c21e022630904032a889245921f13980",
            "f68fed903068428f8883af66580b16a5",
            "44ce6bcf73fd4c478052877e54c6c729",
            "89286767a9c04b95846dc48b1f67b2d1",
            "da14b2a699154840abe74066698e6ef4",
            "aa5ec0267e58462db67f9e88dffd7ae8",
            "ea06d61b20f4463580cc60644b4e2ace",
            "834434f4c9e04246951a85ff3331d709",
            "5b06b6c590b4402fb6aa135a971fda8a",
            "ddac6a9cf0e140e6b310945ca94e6969",
            "378f8363ac394086874300d01cc990be",
            "bf4fda5309b7440c80e52ff8f5825b1a",
            "6a99ddb1b1914d588b1be61e3731a13c",
            "9528de61f032446bb728aadb340ca215",
            "d399aebb1f0443f7a4bdc42ae207a30d",
            "f3fd27c07f4d4defa98507aa5d39d7d6",
            "d8111e4be1e24255ab3d5c479e78c91f",
            "a0a1c64fc8c0499e94ecb7023224d3b5",
            "9e938ec925cc48eaa3d5bcb9d4c89436",
            "ae510f37255441b982cade0ae60e1567",
            "b4921c099df748f185b68c0c21150d4e",
            "e87f60c588b643ce82e56125906d4e12",
            "69f5ad6b4bac4e48a27849eb6f4fe5eb",
            "2c0d7768c42b40b9ba44dda3c27edbc2",
            "be84716b953f47c2b0f4f197c288a36d",
            "9f793889478f4b569e408c49cb7c401e",
            "9fac4542f81c4033b1b71462b1d31ef4",
            "2861681d1e164b718ab0c4adfec10a72",
            "4192cf84834143b8b3259361b80bf830",
            "1ee42a1d7632455eac4f81c956f1636c",
            "b46e841a8a3243eebd08aa753fc6a603",
            "93ffc95e35504e1abb446a7cec5ea296",
            "f46ea49fe4ba4cfa8806211c0b7894fe",
            "1e173f96d008472abab4b82045b4ae6e",
            "b1c36486525147779667d62024c922e0",
            "8ccb2da531dc41eeac034928c7f2f270",
            "407e685be96947daa5f35ff5beda6a3a",
            "5d36bee249ba4199bee4a366369ff073",
            "fa14a2ee93fa4ae5896aaeae82749483",
            "2060e51b36d24b16bbda9a9675bc6a28",
            "991770e042d24c91b6a201cf4149e901",
            "9901346bdb2f4a1dac2b17ec7a06b941",
            "7f14538e4b1548edac8d97008031c275",
            "32ffb1827efc430990fca37741f1b082",
            "1e1f6d3a311e47e888143582f00918dc",
            "055dc5f2f67a4b29a24031ef153ff6e7",
            "582fd3732f674cfca739ebeae595206a",
            "d5fc8aa56d1045bfa799362db3e507ab",
            "8073f04617f24da4865d8ee37399bfed",
            "75a61209df2c4e4cb206c2c41f5822af",
            "0839598054fb489996de9974f57883b8",
            "8ea93e5b9e15414895a6780510074ab9",
            "9784382290d14c82be08f171f41dafa3",
            "f786c5abd2e844fa840ae3b430d7db67",
            "4a649e6498b44e9e8656dfa35f50240c",
            "d93c93e9e8e34f5cb2e56817fce47537",
            "2436fd6303664858bcbe2834ad750e31",
            "9666e7e0c25d475bbeaea2f998ea85db",
            "c63b67d66d9e4787b86ff587df8e21d9",
            "06ae422f8d9a49c1b5084156f019303a",
            "a850080083184b47ac2aaa203c64c75f",
            "7eba9e6b561f4aa89feb54d0b6b3cd7e",
            "3f08b54d21d84a9bbb3c8bfe41636393",
            "42558fbc209a45349471d151fddb51b1",
            "9c10b16cd4fc4d42b6f14c3c6b18b379",
            "a7b42b14e2b440c58b63af57e732b1e9",
            "51dc9666e5c2456eab8da8e51f78d8c7",
            "a8335542d4564e3bb83fd0cf87ca32f3",
            "5aa61b00bc59448aaf4c90baa85f5db5",
            "76d9e375a0604994aa37f94254fca31a",
            "b6ebc97f3ead416ea0424c3d331ffd04",
            "95cfbc46bb6840dab46ff0229c18933d",
            "ddb53fca67784d218ecc4c576bd0d1ad",
            "85c55e3fe8064fc8bec0cde154ce0617",
            "6c7b2775ef8e4695a2ff5409bc880e01",
            "a9d2c2f9b59b47ad8c8e0309d6e6f9eb",
            "11ddb406bc304e9687eacc8667d3b4f3",
            "eae556616b324c72b39d8dcf57640f69",
            "71891bf763ed474baaa456dcf9a195fe",
            "24be97dd31d442858b5d64bc0a02635f",
            "c76ebdce8f3148ab877ba41fa701dbbb",
            "deb4ec43fcfb4ab7886f907e6606b03f",
            "8d6e7f3e5a014ca79c7cfb595206fcd3",
            "4b82aced1b524283a01619533e7798a3",
            "14d9a0b386f44e30a1568f3374305c9c",
            "bff0a439672942bfb9cb1e47098a4782",
            "43408a52168e4a3e82153a15fcb7c301",
            "7bef5a95b36b4bac9928f522289adcda",
            "66cbbff271154aa7979593a1073002ef",
            "1f9b332a44f54a4eb5763cdfbc80114b",
            "c8ee76feaaa94a3c814cf62acfca6535",
            "235a64667391436884ddb7be8d247543",
            "fc5b23211a6a4a53a664f6682a984395",
            "e0245c762d1348959d33011d9f42952e",
            "f0554ff2e0a344e99b48adc77881510f",
            "42a83704b10948a18466c00ad7d1bcd8",
            "c87c653b378347ff9ae24d9b01764971",
            "e4b2de44612a45f6b74ba1b2ffcb7f9a",
            "57fcfa2ed66f4b0e97b25d63a87125f4",
            "bd1d5ee83f554596996774b6b04adb56",
            "6abf4e7bc7384ff58f35bcc4143c16c2",
            "4ae022b80b6b45939e8a1fa33cc15bef",
            "dbfb033928d34ef79ff9a9991d4e4671",
            "98e199ad0b90459aa895e1b8cb61ce5e",
            "1ad30bdd52054083981bbdc81979a196",
            "efa075f0ffb94fe296795eac6b38fabf",
            "08c82757dcec48d4bd47d7e59bc9ee26",
            "1e3596fbb3134a27bf34e93e4f7ce1a5",
            "58a545d6232545bfba7ed816eba432c7",
            "70beb1e2cae04a639e1c683b4e7fc358",
            "00a8a716108d4115851bfc8fc42fc46a",
            "5c97a76f35644e16b5b419e89afb1d93",
            "e14898235cf940dbbce16efab3d83502",
            "dd3cb1bc68774756895f13c1194ce2a0",
            "c254ad4a196f41a8ac0b7b86fb8aafb3",
            "61d47b72920342caa86fc4db46ba6174",
            "61c2185e540843618f5aa357502ed21a",
            "10f3eae79eb546bdaad25b1c11240714",
            "d388e19563f94bd480d26ed9c3c0e94a",
            "6034a7e7670b4572936309eea4f41cf6",
            "564d0b39f0d244b3989c52452fe97ce8",
            "5bbcf6fe99d646a082b584b296f10a45",
            "3cefe7983451445eb43716f54cc106ec",
            "6ec7c70460f64e818e8753074ac223ab",
            "195c7c1ade77435e97054ac6fd77b9d4",
            "01b4522ce1d3406eaa4517971e07de1d",
            "a988bf80786447248e5921a4f61e702d",
            "767ff99fcde14a56a7e118f2cf0760af",
            "46659bf3e2e2413c9afadd395fa235f4",
            "fc083a9434374e248be0213a653493d7",
            "bc1ffba55ba24b55b9f0aad51f3cc6dc",
            "76a0c771fc1d4283bd60c38cb316a007",
            "d0ac53bd43f5441591edb302f763dd0c",
            "1d2ebddd466c46feb92b179d83acae3f",
            "9ba9a1127ee949b18746dba0ce2eed74",
            "e843350d306b4b77b38ad96582cbcaf6",
            "e9f3cc1b5f844421acc36bcc1386dbd8",
            "699a4aecce454f94b67cc5437989cd33",
            "f21e0648fc5043d3ad21a9241a1c4ead",
            "ddcf628d43b143d2a0f3e53b3b2e7be7",
            "87119711769e47f0bffe6e77c8a4f762",
            "7f299bf698af4a219cc47d765cedc10a",
            "0ff6e8dd2e844dc98ede96d69b106abf",
            "37c83babb46b4fca9c9d1f63d92d42c2",
            "ea78f951117e49aeaee7e4ae163db179",
            "ab0289671e6f496387274f572a5d820b",
            "d441ccafec4d4c969b90c28e2d3a2c9a",
            "5470d3da9eb04d799872dcb31a9d545a",
            "e622f6db7de64f26bd89224ada78f919",
            "26e1b4bba6e5488abed12fd6177a174d",
            "b66c68ab0044439b9e0957c6366fa618",
            "87ee1a41758640629e7fce6b6de4cdb0",
            "4c64a83cec974ddfa537b474d191027c",
            "b14fcf1c007144919d12d86f7d121d61",
            "6b16a043cc8b4565b7f05b28e47060c1",
            "be69a8f3ea294789a295a8b9831d46b3",
            "00f92bda51d24ac79667be89e4e6b1ff",
            "f538b417c30b4a63b89ae53ba952f33a",
            "374ec2e900b649f793fa989b274b28d3",
            "20f515bdb3dc47219ef5a208e85dd65c",
            "4ce04ca3e69a4ba389d5ee4bd9ee2eb6",
            "7254030f2fd44871b4219f399bed566d",
            "91883fca52cc46299e58e59b5ed77a8d",
            "335e3b9744f64647be4b9babb184735c",
            "41ca36e036ec4428a58c8ec03ea78431",
            "2a3d523d794047828e984ef8eff4dde0",
            "537ef95ddd8e4346970f193d09c32407",
            "44da76da157e4432b235a8d109979440",
            "acc4c89ea5a7468585c7ae76d0369e21",
            "45f15474c2c04252b48305654c48292a",
            "47f7e6dfe89c44a785a32e7fcf185d88",
            "fee3240172be4a499c28af84cd0d10c2",
            "96d7d074bf404c7d8bb187685f893753",
            "47dad7682a264569ba068eae358a6e4c",
            "52cc24ed13ef4799b3cd8f910af827cc",
            "dcd8f5d131a741c3b74d866c21d301d9",
            "8829dd42e12c4592880bf50f79b8f7ab",
            "6a17f1ad00054e2fb2dfc8a56473956c",
            "5c7ebe490f054b2e85298f6d93e640a3",
            "2b431b0d61414f8d97079cd56317dd84",
            "5ebce685d2d7457eab68afdb4b26ed75",
            "db1be88e6156475f83a2b71f0af81284",
            "c4cecc4284d9410ca93ed94eb2749352",
            "e00e44d8daa549bf817f13ef1ca82cfd",
            "2ad4bddfabd24ff6bb0f428e4003d93b",
            "8624f9fa156b4ea4985fdb99c046e66c",
            "1f835c0135584967a9af7f1623c6cc3f",
            "c202dfc76c1a4e8bbd7bbabda13ad42f",
            "838d84facc1c44eab90a4d149020127d",
            "6aa83e84bb424e76989f7f8c6519f116",
            "e501541ca66749fe98ea17c03024cf1e",
            "ce27d9b36db5448199b566e246f68e06",
            "c4113e4bb1b147c1bdc07140694f4a51",
            "4851ec36cbd1429d99eab29b1e03ff5d",
            "7d7449553e404b53b7ee2b7b038b545c",
            "e2ce4ee3cec045fdaf97072df2fe31d5",
            "7de9e78db40542be979b4c80ca6c766c",
            "692fdfeab4f7405f954d0ade0ba5a8cb",
            "a5900e3f16444450909973854d72281a",
            "6514b58eab7b4294b63c2596244173ac",
            "52c41778c76647a0a4f19a92a844fd9a",
            "66df3a1f6a6c4fe1aa3ffb5414f0dfea",
            "e314e6b30b884e18b12ea734b340f08f",
            "72e85ebab48a4a02a2d21e92babed392",
            "5f247921c0ac448882cd630ab3484ad9",
            "dcc3b486e5cc4550a5d6ebb4f518f8b1",
            "d4fab5897f8e4dbf9a7ec7ce147ac6e7",
            "c6bdd7d7c28748268d50e769ccf1329c",
            "9c76c5411de741f6810f882623218470",
            "756da1bde4144d7fbf8c31aaa093d626",
            "1e03fc38f9a64e03926b17825240b39c",
            "9c935f9f9a034e9b9430a0de34ec5257",
            "c4732a157f0c421c9120fbcdafdd5370",
            "581321f01a34456bb0185108222d8cf2",
            "cc8035b8ea3e4a1bbe0685ea98aea88f",
            "0fc8d7db29734c42ad5ea73165e6035f",
            "5a30ace8085d41bc9a17652551c4b72f",
            "a23294498bff49458f9c2dbbc380f409",
            "ebbeefda384b41b7b62de875068dde17",
            "ec00f41b24e64d169352491ec2f3a0e5",
            "8c99126627244c8596e52aad0079fa10",
            "cdb79d2814844a26ba2d5825ff5e1e02",
            "f4bf091dae4b4cd79331d32499276aec",
            "db1169f98f94471fb3da2da1c85ce2bd",
            "6495f55f987c4257ac882c1491e0cf18",
            "85c3f6da2c44423daa18091b119f688c",
            "98e4090e11b8484f9c944c2ecc788a40",
            "9f5dd2500f5b4b6bacc29108bce97e80",
            "7db73bbb28e44c879fea065417337050",
            "3c257bef426349c595c9931bf93126f2",
            "ceb92bbe6b88426f9a9f9f26546eea52",
            "7c99460387ff4a2d882ad5521bd7b04d",
            "8dd6416502634c2b88ea7baf777528c2",
            "1d6f2c2878f3439ea5da10700a148031",
            "91b9fa01c1804d49afa72113a782dab0",
            "b289b1609d274d9b8ee49ce88ce0b5ed",
            "2dabde0593ae4bb29b380f2ef3b07b8e",
            "ca2c64bd0e74425597a81227bf6ac8b5",
            "4651cdc68c61421aae3f15bedc026418",
            "4613cf2c0a9447f3aa7f9c8bed5032c6",
            "9ef918fd36064a949728d26793d6d13c",
            "01a8da8e5fdf492a90ca81f8e61eea77",
            "d333f4d6ef1f4185815cbb1fc775afa0",
            "f20c829cf6484e719646483ea5a3754b",
            "044276b7aac843a491e17df1077e8ec2",
            "7669eb57df8b4a62b9b4f95a21b92885",
            "a696e8a6727f4b91b140246a4a223d04",
            "d99949a28310497a8f55d9184d58516d",
            "a3b2636190df4a46a5f7a42b7a6cc011",
            "9192a0988a0842babe6edb39f7465a45",
            "38e77be2f8434b4885ce2dfa0b7cb393",
            "2bfd5e0652164be297cc69c4b6be8979",
            "92cffb875f094ca599818fad4cc40cef",
            "734cac8184ef4808a6162ee568c3f5c7",
            "9c48bfb4aa4e43a18d1921b93c8317c1",
            "b17dd1429567444088f5f9e35e1b9fda",
            "a0a11cd662b848a291c11eec1daabb65",
            "751c233a2bd54f81892aab8e98d43a94",
            "204aad5254e3490b8b8732bce2254b1c",
            "73f8bbcdcba6412e8c5b2b453352ff05",
            "ddba253b807d45a7a4ec70e98585a3bb",
            "bdb2f378e83242a09fb770ad6d1e8bef",
            "a27a29a867c14e3f870fd0e689701f4f",
            "3ddfb4607e5540c690628ec6598f5d15",
            "b84c3c8ddd164eb4b3162d0cf2322448",
            "8d96cdb7fe534cb09f7e237b9b3362c6",
            "a74c7ba8c033451c83948ed619a13849",
            "aa911cf19fb14f299bd34fa6fe531c04",
            "434b4589ad604a96b7b184b561349913",
            "3bf86eaf3b404b12bd99790351c4929d",
            "c6a5727d9e3e4072b2ea9c3880763155",
            "b06502e0b5b24d878aed0444d7c09123",
            "39f90749b2ab4b22be092bf6a715fb2b",
            "6ffc259a4579462e88e21f10835e3543",
            "eafdc225ac3f4725a102eb892111b55c",
            "98dfdb7c9e164d26a99fe29bad8d1d04",
            "3cfb885db273457fbbb5e2a096fcbc3f",
            "7efdbe2edb7a47468b1ea4c465379ac5",
            "f68c800aef7d457484e2cf639d65a542",
            "5d4204080008491283c8316f8c6fbf3a",
            "24049ac0107a46aa9a5986d9cf8d2fdc",
            "ef6733de8a27447eb0a6cd98252dba71",
            "d7bc24b01b0b4d47ba2849879582c03d",
            "ab9ec84ffde043f884c47e44f2bfdb62",
            "3c5ab9bea89145f7a3927e0004d00eb1",
            "7c0fb3f4ea2b4e7c9205c1e4e11ba757",
            "c0e2a05c5aaf4e8eb82c78ef61f506c3",
            "c2400c400512427f93de66cb64cf105b",
            "5b36f24359ee48458397d27807370897",
            "b17af8835c9341a79cc763c69b085217",
            "11b45ba3aa184fe6910eb5c80f4a5576",
            "56326e3a5cd64d9cb6110ffacc5c1041",
            "78ea1decaf254088a0c92050036d2a73",
            "61453d5783e44366baa74cb7323aab39",
            "e2cb1e9298a74aefb35d5253de3244d7",
            "a45a22f349004fedb454405120fc4673",
            "fc60f300dbcb4956bd68e35236d7cb9e",
            "97036d83fc4044049b9b9d6a4605e525",
            "9fe3c4ec553640e28ad29d50f81ee741",
            "28ef887c5cba4946977cc9bf28de5677",
            "e06e3ec0b984408282e647146045c130",
            "95c5bda319204edaa4828dd46f24d2d7",
            "3c3f6bafc98e47109a4eba90398aa311",
            "cb520d9488e944a29bea783caa16d9ca",
            "8b859197017c4a36bfcbfb9c779a4e5f",
            "58b2b014d97b44f4856bfc4ee7bd798f",
            "ce6d7d2218a14f0493f5e97e10f08afd",
            "36e03f8179734cc096575e63d515b6ab",
            "59e9d6fad6b3474384a8a5795feefdde",
            "89bd7a123de8408f8824af01c8a199d8",
            "ce5e4a34c3404ee381d21548843408c8",
            "c5e5cf18f8f94ac3b990a903cd5bcf87",
            "2681246aa77746fd83118ff8e3e5535d",
            "24079b2feb634f76950749fb813c88b3",
            "2248c6dc497341878e6734d1e6476b30",
            "e1547f9b4dd04b7aa3360e98f79a06e8",
            "e63d589ecbff4389b18dbcc8c8a51866",
            "bf049897edda40f2bbeda1db48d7c00c",
            "90cdfac034db45f0a124e657a56309e5",
            "3a4e097747d94f7cbf2a5bc52757e752",
            "5a77e737bfaf461aa7c21ad70a5b3e1c",
            "9c021e71f59b4864bb32007bfc044415",
            "dab8f825e6c94358ba87b90ea3877065",
            "444dcf8317d2408e87d03e314b90bfc1",
            "5fea905b1092458aa75d646bd0ae2a51",
            "ebf5f0c8ade94574b2afdc796c74e63e",
            "201c5dc1c85241bb9795e86ff869f518",
            "c0493e78581c46e4953542ac9088fce2",
            "facc26bc35cb40138fb0d5396d72b90c",
            "fbc59dccb938418a89321e4514d85d92",
            "b80b327c97fe4572ae0ceeb264487b93",
            "85ebfca2950f4ea7bcf8805f6ddb7fb1",
            "fd89130dad1d4970b38696c527985293",
            "6135e0b2af7045cf894cde86533347c0",
            "3cc1cdda04f5474b9f4913cc709b251b",
            "f892940a3853450c9c14ef360c2ac316",
            "eae8dfc1ccaa45ce8adf1906d4f49150",
            "36e80b3ffb7249659086e87032f36935",
            "64caca64002f4ffda51d3f75c16f61a0",
            "bdd382db7cf24cad82d8c7484050f360",
            "d84465b2b2984183b2d9a20a85ca8414",
            "811364e4662446b6aed195ec39bf096b",
            "c0c5e72880494e10a2189f4d92df03c0",
            "1d6093641fb04fa28e1a4a1015a60d7e",
            "b52d977f5a4c4dcdbb45b092b07ef2bb",
            "e9130b9cfdcc42ff9fea8bfff2c4f9a5",
            "203f2372b9c5487289000d850f156132",
            "ae61bd11a08a4b16b448a6d51332658d",
            "b9ac742661df468a9efb0894c6f77ba5",
            "1adcbd7243f943b1846feaf47f7fa7f4",
            "d14f580cc664409387a23536d216dc9b",
            "4b3cd52c1b6d41859ba44431bb92507a",
            "3c66234c50244ff8b0d60eb94d4b9e0d",
            "e1f38d04a1d04b35bb69970e91618ebe",
            "caea51e695fa40b9bb366d6f7da71dcd",
            "dd3a0e4b41094856b2b91a14bcf82ed7",
            "ffdf20d38cff435f9ccc8aa7e317d14e",
            "07d367c957bb4112bc585a97f160804e",
            "5457fcb978574b7c8070096f55ce4257",
            "11b2f58743b84aef9869995f032cf83a",
            "ffe9d8d1e69c434ea14eba6d404efe6d",
            "0a4dad098c334e8582ea826780fdf5b0",
            "dcc24d5f15c54b919866e34c1f78eb80",
            "ebb34611499b45378c4464c0bdaf1575",
            "2918f804aca743a3b7b79d7714c57fdb",
            "10c2923824154996bd6ce1048956d66e",
            "eb2a79039f8b491cae3639b05ee32346",
            "dc16b1629ae748ddb4c04d373e83e458",
            "d8adb63c67b240e289f81820dc6ff8e9",
            "8a740d20cbe64de2a20578ffd6f238bf",
            "5e746a6cc8ef47f9a6ebc410bc4f5e60",
            "10fe5abc62824737bbe255ad569c116b",
            "9791d8ae86a44868953d6b59d17a423d",
            "83e592bb33ca4e16be1107b1561bf475",
            "b93ccdf2c0ec49549c68041b14bb6aed",
            "ba53df57581d4d42b0846082d649d0bc",
            "e2f8384f50fc467a89d75c58addadf2a",
            "41fc145e7f474a14b632733c250363f0",
            "e17091c7bec646a88da999b0e3cc24bf",
            "24ff44be8d18492eb83baad3fdf52ac1",
            "7df98132c7fa49979b981841822afaf6",
            "a2d35c21c18e46f49c40594b9bb3140d",
            "511233a4c20e4c0f9632886fc60bc9d7",
            "88d37de5966346d7bf9c55372b85d3dd",
            "7a6b02101096422a8e01537f20657d72",
            "355aecab40734660ba13d010486b6980",
            "42047574558f42b38d1f90f30a66c545",
            "527cb4ee714c48a883a255efad42cb9b",
            "2b87673d61b44971916bcf3eebb464b0",
            "17a9d69fb9ee4049972b7293b7121474",
            "b740d031ddb34e7c92c45d1df7ca4079",
            "a0630a7495d444058ef532e60552e597",
            "3c5e5eb8a0c14ebfb41253ef4a3e7c0f",
            "4c6d8810c749402181809940c62841f9",
            "d555a048d03e445780b21845c4a34e58",
            "57ccc2a136b4498188862feabee6c70f",
            "8647ff35804b4023b5b351c65c296ffd",
            "4a3203679f7243128b948932c2336832",
            "0dc3cbc0b0204cb7bc70ae2e2afae3f0",
            "5ab72a30c69d466e8379708c2b735720",
            "68f2c78d040e4cc2be60372805511d9f",
            "cc6b14a49ee3499aaeace4db07eda39b",
            "1dbf7d4f45e846d9af7222de8eed80e0",
            "f7979cc6f9184a32a265b50587d73130",
            "7f6b2242f11b454583c4fd661aef3e4d",
            "200667553db64f6e9a000aa9ed65a913",
            "f0f3e09cd753496abd43b445ec6e2895",
            "ed6c687c72e44c23a4724551fde129c1",
            "b00c166d08774b36bfbe8bcf8aea81d5",
            "d68d30a2d14947d495fad964e7c85e97",
            "087fe3c02b2d429f9b4e582a34648ef7",
            "565bc47adf2b4385b990a7486bf8e62c",
            "983d12487f6c47598df1a980d1083017",
            "bade233eb71e45eba6530a1f14a4be39",
            "3347a2eb44954a2ca62e558454b2edc3",
            "6ac84acf064f422b967a2be2718e29ec",
            "178cc1a5539b4d24afc32c1717640dc6",
            "c2f59d7fb81640688050a2c740277620",
            "f1e127571b034eac9e0b50a1d74600aa",
            "684a2ca712ad4a40bdfb36cfbbcda653",
            "5b787b68646b4df09d4e1be97500b0db",
            "8d053c27756d46339d04662a13ca3f84",
            "cc6ea7dc82f146b19a8f73bcb116fa96",
            "92c53eedc57944149d61ce8c7c20bcce",
            "5d0601eeda0846c19ea42b2310bc5895",
            "5bbf605f6f9c497cbbf6e75c8cce40c4",
            "ffad0b8a13074252906be2c13c4e7e33",
            "dbefaeeca8114e85b14d7eefe614bc5b",
            "7a005caa027a49d0bc57ca54e3627e4e",
            "d289fb240f7140688db6f4c0978db0fb",
            "e726aad84e3e4cd1af586811b8607a16",
            "5555f637916c48fb84dc98c9c06e1594",
            "76a83cf4b9594627b10d75eac3e61986",
            "13b57358a35a4bfb8bf5dde0bb1226a4",
            "4685fdffe3c641d390c2afc58aec3ef6",
            "f6d4b900bcb14baab63969c1e4393258",
            "ea534c4a384e4ccca822aebb70dd1fea",
            "d11ddbe15297448393dc7a7a50d3d398",
            "fd376f9353544bb4a4d2e4066483f5a4",
            "dce7c5f800ca4ddeae2c907846d75f86",
            "1495128da4ad4b65b9254578171123ab",
            "726789eec3a045d291d7162953b62bf0",
            "80197f35520547f792b3b486d69e2f05",
            "6597d9e3ae004b91a523f62b3fa8c39e",
            "37da1f8324894aec8529ce61c0778124",
            "9a83215755c84dcd8904a59daf787670",
            "69fcd2ad0a9f4dd9932c2855957529c7",
            "7d514aca0e4944b1b13216d4fa5423d5",
            "3c0586e8e45244798d573412bcaa9b8a",
            "940959c2c2d246dfac14ca63ce3cdb98",
            "a806281db04a436f9c3ffab3172ce670",
            "647ce497769c462cb82062fc6ebab9c6",
            "38c25ac4b832493b9af32d98e7ce326c",
            "ba20d9a748fe4a0f8ef4f11cb917515e",
            "18dec6d4f5314a6cab6cbbd6406cd0cc",
            "0ddaa7b5b763416db54aa67130577a31",
            "56498c8c6a78475299190cf59dd40476",
            "1fb6bcf100304dea8563c68e2d087994",
            "e122859c7e2a45fabaf728c824d97477",
            "6d1e325967f548fc9ce791e2cd10300f",
            "d55762ffeb0c4dacb9242f05dec18aac",
            "83f7e80e47e3436f9b24ceec8ecf63e0",
            "b9716fbe78f8474a8e07f6fe67e310e7",
            "963da411e3fa4d92b8672602e447c117",
            "4c7a8a70388a4661964fb414618bece7",
            "58546d5f60224bafa5d2f9b80f262c54",
            "0382aeba20b044b68f24ac48f9582e55",
            "a22aedd6195a4d929fedbfd3c3fb1d01",
            "fa06a6a5e3844ed0b494bb8f78bdc48d",
            "14901d18ec724960b157269e7f49820f",
            "41e59385f358493cb5a47e13f94a0e60",
            "6635007e664d4276bf56ad83c5ecede3",
            "fe844516dd5e4a5da97b8242d81e1f44",
            "7bfacd4516314b4c8357e169a7aa41c2",
            "e46a21ab186a40b5bc809d200dceb05e",
            "2e76cc7d83994efab1ccc3a77cea973d",
            "0759b50d72f343cf8b5a4c4ee07eea64",
            "72e9c9872964480990736b54d925b25e",
            "327e278abf7047cea7e52ab58b47d99e",
            "6d1126259be943a28408c0f905a305ce",
            "c405b8f3d0be48e3ab1c5a8b429c0623",
            "611971007c744600a444a3a70f01c6b3",
            "71dfb9926f79495aaa1bf05a2cc22eb5",
            "a023f48b70ac49e087418764dac82578",
            "a466c5153065404a8e78044b4ac78faa",
            "fc87f31b69d44664b2efb0d5b52b48ea",
            "903057c076374fc5a18379aa0f2d551d",
            "cd61c0f2d5a34d51a457231fdaf5b850",
            "1193b57c66804da39b5b0650da0ac495",
            "a0f0b9d02e394723bd308ee6fcac3f18",
            "0478127624854ee483d8142749082ca7",
            "755aa387297f473cb9bdd1ea2090f7ed",
            "4a934a297f5a4935a9ea0d8ec381dc07",
            "8e64b3db5aa74d90963c45fb58eb59fc",
            "5ba4ccc158a144c38bc2fb581783b77b",
            "03e5be8c7ac148b0b11e963a55451fcb",
            "883b03b4248a4db0838bd44694c537ee",
            "96e3d5c5f8884e1b865c48d0a02d5e8d",
            "1cfd0a5166bf4f33b89a24bb2a6f1c29",
            "14c34d7471d748bb9da4a83bc60aa9f6",
            "3f3b6dea4fce400288292a20033429be",
            "cd5667aceaf14a7c988fc86014633394",
            "0c6d293d84d24a5892646d0998660da8",
            "676499855bf64f92917bcf0b3aaac7e6",
            "db0820a63b6e4b23af6b9898226a0c66",
            "dea4183cf0e748d092e9dc9df458105d",
            "d31cd0fcd5614ad8a7e3768d9a355bdf",
            "2e9cab653284447283d346a4b9121697",
            "8de8febc1a0d458ca3131bec9df31e93",
            "dd9281aa13d8455387bfac2be135f703",
            "0353d6a7189b4fc095dcdae3c2c5a589",
            "2b021cf306ed4102a0d79bc24cc179ef",
            "3d2bf37dd09f4d0297fcdd17fde1d880",
            "e2329b2e660d470fbc33ccb231c4884b",
            "7a7e491eba6b479bb1d8799611c7ec14",
            "c9b48c5fef364790b34a0674d64248ce",
            "cb6f62401a8747b1857fe9de2c75b77c",
            "69e41734a5514cb392d1fcf0dfcede20",
            "350e38f58c9849e8834a63d1d3c9ac62",
            "42f2b303bed345dcbafff823cf6a89eb",
            "26b689b4aa2340188519f0a9d8d07570",
            "37072588d013495080645d2b02636d29",
            "00509b9d925b49d4ae0c879e43b47cc7",
            "21309b91081c46f38afc6395f1777e83",
            "78ed43f9a2414f069a7ce26a3f18615e",
            "cfcc2cfa60384d6081a3be2878677ed9",
            "1152881184fd4298b814fdf6dd69d69a",
            "c32a1fc430db4e3fb1c52d839ad868ef",
            "ea91eae23696410ca9dabe5da4d7e0fe",
            "55f3465cc55e4d12896a74442c751117",
            "ef89547e40854ea98625491f9bd960eb",
            "a7b22c0ba70a422dba960e8cff68b049",
            "c7469ebe5c7c49ed9272c7a5ff2ea0a0",
            "79db8cf78c1b4505b2e85c0e929edca3",
            "76a3ea0d7fca46d9bf6752ebd2f6bad2",
            "06dc8e6df6d941fbb164a30021f1801f",
            "294496d88b224d459e2b3316cd80a7c1",
            "8915f38f103a4c468e18923b4d009b9e",
            "d4368904ce284bcca91059c7a1b1d46a",
            "be27989b0b7d44688e6ebaf4b49b1c26",
            "3423ea05b941492ab5b730e2dd155d95",
            "482c1117ed2d4ea8bc1239607023fc0f",
            "50bbed134c974e2b880b203016aef5fd",
            "1f69b1625ee74f3c8bd57092febfe266",
            "6e69cdee09414c6e8814350127b8ae21",
            "0113f826ef7b47d6adaec2b6811ab96f",
            "fa1b66ba1c5c4d5494b3320c29c36db0",
            "69dc36a6aa934be4b6f23202d3d71683",
            "bfaf9fd04de248e7badbd057969dcff1",
            "62cd56b6f38d4489b6f045b80dee6125",
            "ee7d9ad8d8f84bc590abf2b725a687bc",
            "74d1bd59ebbb4b33bd0a60867e1db851",
            "585753d58f5b4c368230bd3f71dfd5f1",
            "7b6fdf391c7045f1bddef53775f43df4",
            "da2411f32933410a8fe0fedab0bfcc8e",
            "9a6ab986ba8944739ad0f63d90a942fa",
            "664a95eef9a5474aaaad114a07b1d0cd",
            "34f0733ccb5f4f14992580103452f1f0",
            "2b2c6f613a8e4c4c9072818c305b5557",
            "1e547be4ea0f4d0c9bee4f2367f992a0",
            "3fd4d128905941beb8741b5e2bad2548",
            "ec647a3f4d0349669fc202ec944dafd4",
            "e155f939e6bc4b598e06eb1e0b320843",
            "151badfc8fec4070bd7504e93cc3978a",
            "81f3b743df9542d38b81bcd92eabae48",
            "055a4e7442f74ef5930f9bc538276330",
            "7afbe5219cb34d89bdd8077384ab4b79",
            "05cadb5153f64972bfe550ca8eac199d",
            "9a2e5421531f4619b641078f5628722a",
            "8bb82706c60542e3a3f1958a69c4f6f6",
            "cabb66785c3443d9a5ff78c715869b0b",
            "25320b39796b424d9fd7ba0a5e245bb4",
            "42673ca8f6f14aa6b648be743de872d9",
            "26641d2116df4e8491414eb1fb7f4e89",
            "996a505b820c43459ad8282ad002b8c3",
            "e5d32abc4b3046a595352b992a977952",
            "724eb64945f248609232f1b32767cdab",
            "e3692bc6cac04714b4908082a5f41a80",
            "d7e97792ff3b458eb4320dfe061e4491",
            "85daeffa4d00465fb315a01c370f2ec4",
            "e92f61a0b0a641b1a96e5b3435e090f6",
            "342da5a5ac8440b190e5b29fe533790e",
            "b01406c9d5ac49179d08391919eaa7a0",
            "5ac0f33ee15b4b2eaf7d298d99f31075",
            "b1ffcdd2013d4cb28a29a6ca53b45644",
            "7a3e907d783945fc93a3214378a51c81",
            "9a8b9003cbca4dc6ba1fa18873657fd5",
            "50fd8e66ef2f491c8d1106c561213098",
            "0dcb9f18d6b54c388021b812b68d511c",
            "4285d95dbeee4afd9c3952e09942e02f",
            "303eaa940703465e8bbc92b90411fe62",
            "89318e94a5434a65a1270f50aa5e85a7",
            "362f6ce81e644e61bc4ad371e94093bf",
            "69ef29d13f444bd8ad18e16a032b2114",
            "8a9548f3fcf14ebabdc23b215eafabc2",
            "689aaf9bd6c74c8fb1ddc629024ac617",
            "c73eadf4574740cb8d47e84e37bae6d0",
            "c3a1939459ec481990150c8fdc3ac76d",
            "94d3a7675dfb4fe29026779ccfc8bbd1",
            "11196e9e5feb4826a5b3c3af151e7419",
            "fc6cb495db9c41ca8c099b5dfe4dde89",
            "5db3f8eab4bd452aa265e3a6d50ca737",
            "96af5b6f825b464cbfb46ffb96e7abcb",
            "45777fe3fb214f84ae99e9bdbb89dca1",
            "5c1179aa1da04b2d9e663f2ce053d11e",
            "5e309714d99d4abc88c3640f66ea55c1",
            "2c0795aa77e24711a899a9d1d96631d4",
            "50f4126f9bbc4294a072ca9291e19c47",
            "3b9cb9cf214d4b05976b104089978fe8",
            "2b77f5cfd9ff4835b99b511d897e7633",
            "d2f9b4112fda487ba9d000a1cca31857",
            "65bfc095d7fb42ffaa6303a1efd2e59f",
            "76cba1476f4c41c8b0f232a964a4c9f3",
            "1776effeb3ed463682e2d3d16eaf7dff",
            "c4699241c1ce4fa88f244fd4ff544665",
            "9f7947e84ce74d5f8671d29404d71ed6",
            "0e86d68128584c72ae6a416b7051bd1b",
            "585e767432be4616b96d4268a28b864a",
            "c6317e52f8ec4f3080d9240306d01e0f",
            "270d65eec61347bd913237f5b360bcf7",
            "65eb602d928242308bc991621c55d8f3",
            "90fc7bb57aa64199bf1b8f75374648ef",
            "8be0b65b7679487b83d375c2ef3940ce",
            "4869621e4d1542c6b991fbede6730241",
            "62ce9e35c22e48a9ab39bf2142a5e5d7",
            "37d43df5dc784bfe973f2148e791f140",
            "e1284869997e460ba3242093b25180fc",
            "00b4631189c1465b96de4a0013a376b6",
            "cba037cb99b440848be015a3658b7d73",
            "3d96a408dac642b4969a84ac27909612",
            "e0e49430af76423a8e90f88d9d367ece",
            "29e46202b66f4a90846ab2c1a2f2196c",
            "200314a351484126b1b7fc97cefd84d1",
            "452b347ea38c4a98a574942f37182105",
            "2bbf4eab6e42428ab6c212b5771970e9",
            "4619637bf4f14d3db67e16d8ede133e0",
            "26b0fbf55c7a49c189c72330131ad518",
            "ce85fb020ec34bb69745497c9d59df91",
            "97ecccd0684b4ebdaca065b570c37e77",
            "b178f83a2d524ab19da6d4d901068d3a",
            "2a078e18ca2145e8ba850a39d08a1803",
            "fc11ceb94d724529b0cb2b5fc5903ae3",
            "5de1c208e1644fd99404e7b7c640f8b8",
            "4c082ad503894939b8ccd5d9624b9d6e",
            "215040ad1b2d4193a13a1a56c5e2d58a",
            "c56bd5a4f5a34f08b8f59c1d439fb789",
            "b9b1105097be4d4785a87204978d434d",
            "6e296c71474643e4b1ac9f0bfddace81",
            "0080075a6d7b4a16a1d71b1f6db287c7",
            "5cccf08c4792469489b7823dad6ef34f",
            "b48673b8d7cd48388d4d951a8a899306",
            "78534ee7a0cc4600b1b2826e2a363f43",
            "cfccc52d480442cd933a5e32452fc0be",
            "d2ea48996cdb458b9223babeb923b1dd",
            "fca9b2a471334da08fe387a2892e2a76",
            "09002bc9a58c4653835ce68d463ae9d1",
            "53c54c5d1b3f464ab98f8ccf90f8b746",
            "8eb9c434b06d49579eebda1d6f4597b8",
            "5f5546955ec34f5084e701a735747664",
            "06aa8844298c47c1afd368fd1e20aef1",
            "658125d769924de3b60c48d06611ee04",
            "133435a4d0c84606999f7d020791ae2e",
            "d95d5925d4e5424585b566ba9440d431",
            "24e4f1bacc32400d9a9163cb5c5f2636",
            "35e473b0c7ca4be584273dfbcb583e2e",
            "9f38debef097448f842a78a216807c37",
            "71a53bac44ff4779a52e9169d4be6972",
            "2a40e458e4ce4db784e0ff35ac9fcff7",
            "eb0934e3582a432689bb0974aa368825",
            "c651474cde1c4be888679f32cf5c9cbd",
            "4f33bded28ad4966bee4a1a0f145b12e",
            "50a75e5787514ff5b908564553cece30",
            "f6e8302147df44ba94f8faf09cde98fa",
            "93330c4c887a40edabbd16f6378fb695",
            "cd3bcbf84773471081a4a4bf2de16200",
            "a0db077d90a0462b8fe0b69aeace6cf2",
            "ed24508ee4084fb591cf54ba868cd870",
            "bbfcbc967496444da279f668e1ef1e3f",
            "960a49af8cb0426a9b814ab633430067",
            "ee53c01db1464e06a92ce8c440a24397",
            "5344e3150ab34b79b9860f4ba5e739bc",
            "0f819a3f1e7346ac9c3a96f722d2f5da",
            "9f591930f121447ca2f9e8239d903b51",
            "19062abb873142c39da973776aa9d05d",
            "3ade750457f942889b8a09abcfc3ca33",
            "f7308e82a2144593b14a2669c929e2ff",
            "997df97fb4234753951981e8c3ccede8",
            "e0991c479d8a4eb6a096f90ff9e82fb4",
            "3adee00f5e654eae857bf8728ce88f03",
            "69a089132ed6453ebaf954e6cf48392e",
            "767353713dd1413ab0618713f97afde3",
            "1d468650c2ee44cfbab63ffec0d2b47d",
            "14c5368a51fd4d5fac158c2f792a65f4",
            "78cad60f10144a0f9b08b30bbf2eebb7",
            "e37404f33ca04e0983b38347204c9a0f",
            "bf2f593897954171ad316752a39e2d70",
            "616bf5b96ea348ad9355867e2a1377e4",
            "545ade69c88a4667b235a708db4c013d",
            "d36e75de6a7f40dc970c97bd3ba41ef8",
            "2ad5d4a0207d4754a2933ec406ea2dd6",
            "b666331e2042403c90b8af5f31bfbec9",
            "f0ac0a55c7f041e3834e3e80ae438525",
            "45f69bd70ca24d44b2de66ae5d869aa2",
            "0387eff0576c41d0863c46c90e2c7dab",
            "848ac1ce470c4c51a29c04ed24fb490a",
            "dc44b92bc2f843519f741f9e055bc5a6",
            "06d5d01e2e074c50b1b9c2e1b3c916db",
            "ea135bc79496497390929ddeb6bedf12",
            "b1afe099a1934f0fb4fdfe844bdd91c5",
            "da32464a498d4d5a860e1d44de2344e4",
            "ab3f18b6bf004feeab7a6c1c51f6e36e",
            "03075b3dd832488585fcdfaccfb044ab",
            "ca09af183b214395989265976d740f39",
            "74869f7866384adc9344bbe1d0787d13",
            "a5634102c2ce41caadaba82671ddd554",
            "62f559edefed471cabc2b19d62353949",
            "e08a2f4f9d654ad29087025f0d632201",
            "d28f5ffaa46945c2a977eb4c9ca9758b",
            "e7fdb31b5d4f4516a98fb3dbc040cee9",
            "870b067065684127886e0c0093b39758",
            "a01352a2d63b48c1b900674c97a693f9",
            "ee0ceb93c28b49bcacee422c818bf819",
            "28a4fb31377e4e3ca30b317607497696",
            "0066375200124f5c9d65976ae7b4cf4a",
            "3e008b576df348a68a4877774a8c20bd",
            "e0466e465c5a42e881579988498d35e9",
            "382cb5ebc5784473bedb1ba1ef01a340",
            "f5844b12784443d1ac5aeae5035b4a84",
            "0dcd9a0e279f4e189ed81872688bae36",
            "3869864ca2004aff90727e20b2563fbb",
            "ab2e49af74594f40a1922ec1f0e17836",
            "2b877e3bf7d24cd8a8619bdda2856112",
            "58801f5250b845f28f3df2803e73d09e",
            "0d4d968d41934165aa98a435480cbff9",
            "ac4c0d096f894bdeb55e8c214eb24ca1",
            "f0fdc7b509794f079e64de18b24a6197",
            "25b933505c7d4a0f9af74f863e17c210",
            "681d8d240f574b40a49a449033f838ae",
            "cdb5dcdd3eb24b5e95d4d16045b2f934",
            "5d8c35689ac04d07815a091314791819",
            "dd528ad841ee4d7583416c2f98f89a0d",
            "855b9aaed000462b8baf42616291d165",
            "523d8827eaff48c496f492a36924f9ce",
            "6bb91008a4034f4f8fecf748930af048",
            "924d724f26be475ab34f58587d416731",
            "77159b633cf744a0a35e162ab4eb981f",
            "165fd95dd1d3431897186d12f356faf7",
            "20227fc636f44d4a9728d38dd71ec4d1",
            "41de657a4d0244978a3098e4f8b349cb",
            "dd2b813633e74b4bb7446c54501ee486",
            "b72b03a804ff4f7d8e6864e4ab6700c5",
            "3b010e9d2f054730b41f68b1e59a420d",
            "69011da7019c4abc9519fa3447f122dd",
            "eac780391061404da13e95341b6bd7e8",
            "a2185d77d89244e2900244324f175ca3",
            "5806e96f755c4204a37b4e077bafb926",
            "4324d82d9d294448a3b9e4cbe2cc9364",
            "9024b4daf43d4c6ab629b3b664408740",
            "90fdc8271fbe4d93a2d214befb3828ab",
            "78815b90f3fe49feaa0120520d69ac66",
            "0657d28485934a1988dc19e40b42349a",
            "bb48f32468b240bb876cfa97533efce0",
            "788c690756ea4f7084b7d49b3091dcdc",
            "1653a916a2254295b067bda558bb29c7",
            "6520c73ab9f8452b86d9eac7d816b565",
            "f28b758361374b5b9686da894bfff94b",
            "82585fc349eb4a3797ea0dee60f3e7b0",
            "69e0f891052a437e93cc69550163d130",
            "6ba046e336574d909c195b9fb7707945",
            "46a402a67d7f46649f9193df06915c94",
            "58fe86c9c33d4738a51ef34b8c966831",
            "c2cef5c94ea148a08f7e5c521eab0e1f",
            "eda17103f53c4a43a0c69e47256ecb39",
            "642dc98a576d436989fbc30cd90d9d34",
            "c89f033936d44308b949f703a8feebdc",
            "a8c00415b1f5417fbc92c19d3c2b3c87",
            "70ed67f4461346e18e44ecefcd1bf7e8",
            "fa960109caf84eab947771107aba2cf4",
            "6021778378084051b85b0c052d3a4fe1",
            "997d65da39be49638796922b9a0979ac",
            "6f975667b7dd4fe8a3b391dd2d73d5f3",
            "8ceda18a40a844988f9f91e4add58236",
            "9b869a6185d548bf8be8daa274eadfe1",
            "bcd5f5b15fb241c2b011f2a39ed9e4a2",
            "3b3b210d399948b29b9d97499ef1e74f",
            "bef435ae5aaa4f2f8bc10e3e294cd855",
            "a9445100efaf4ae09597c9baf35b50a6",
            "eb64f061d9ff4a2fb0d522bd75721504",
            "cda6d5c7896648069b77e774ae85a954",
            "8b00515d077b44259a4b1e633b3b1fe9",
            "f6a3d68e8886477b8bd364cbcb39ffe0",
            "043fc8f2224746fd8b32b0930130057d",
            "7daa02273fa64919b579162d69d7b0d9",
            "7ea7879ad33d425e894b20e59f4bb430",
            "b92286480ec74dc4b27bb6f1e8395cb2",
            "55903606d2d04840b406779844da34a6",
            "e6e64faf86ea43b3bedeee4eb8891395",
            "b7dd1a6fabf34e048958fe7ef6d52cb0",
            "3e68bb3b5b0443848049690c6d2bc56f",
            "5dc2052d55394645b9344abc7e85131d",
            "c46d58fe32474745b3aa5713b75354f8",
            "cfc7da290bcd4feb8cbf8bb71426eabe",
            "a124c977eef14a51bd51ce24def31556",
            "4d56d993916243eea270b3c8d2a241af",
            "d15e7f423c7f4ad7ac18a6397729c6fa",
            "1ea6a7d5c93c40e5861604b78b1528d3",
            "44940c62929f479d9ceb50b84c0c89a4",
            "7566fb75535d4bfc80ee4c3ec95bc52b",
            "7ea0888c2d50428895e0dde4badad1a4",
            "cc3e87245379498693292a675636a765",
            "06e4734ba68843018a67071ede6bdb55",
            "70e8c3b9a58b4b4e8cdc72f89f9261fa",
            "dc4aef872679445fa495ee3574bc260b",
            "f92fed2c551a432f8322c241cf25d003",
            "e1e8b406a52a464ab96e4f88250edc09",
            "af5b7cce44024cc3b80d62923a4b5910",
            "3e3ef4f3929448c7b141776496fff4b2",
            "19da62185bb04d04bbde7d83dcba6210",
            "04f99cd1b71f4f73ad25d36a96c686ec",
            "edf6ad2f05914c4f9c13d111099dd681",
            "95a474f2910e4eb1bb194eba0286cf2f",
            "8f3308533ea74aada8a53adead4ba218",
            "71e73962e4064b87a0e3b31c3b319c4f",
            "c83111685d1b4967bcadaa28add52f1d",
            "3bd75c9ec97543d7ac3736e225585c17",
            "45ff2c59ea38412987a918ee0e256d5e",
            "74f6414882134021bcf18e89ed873f08",
            "b3228d019b2449659250af1c4f9ba8a4",
            "cdceaf16600e4e26951bd8064c597a2b",
            "15ca26b4ef69464bb6a08d6e835ffd2d",
            "52a213cb706640519d0b31bce4e314c3",
            "a2e5bf00a2b246c8bbf20211c073831e",
            "0e835f6731c34393a668916708f6a431",
            "2916a8f8663e42ccbe49ff668f29c210",
            "ae649383aedf4c29b3069371a1cc063b",
            "f714f1fda35a4c2fbcb72a70010f3c73",
            "23950fd53dbd405786d7bfe8c6b45507",
            "fa16778b2b4b42f99bab22fd7f416dda",
            "209750b207574037a4f45f8bd6c779a1",
            "36e9f3b5fa91427f870b89a82b0ae50c",
            "c3c001d1af1644b7a3a2d81844f3bc9c",
            "b4b4a6da2efe47d49e5d6d4c5b0b4581",
            "0b8e5d791461461eb90c9ade583fc796",
            "a3f48128cd704945b80bf51660a7fd16",
            "06b8b6d4f3ab44b0ad6c4e0981496c44",
            "0c2f71df560043d49d57c3336d138427",
            "2d6f852cd4444861aa363d0148faba6c",
            "817877825ba749ff8d433caedf006200",
            "b59304f6d1c44398a2701755762606d0",
            "17a18abed3234213b84213b97709bef5",
            "2a8c8ce7853a41fcbfc37ae4f6e6b27e",
            "4e8161e566784da2937723ac0c034279",
            "f0de0be56a6b4b0f8b7585eb9d86a5d6",
            "864f7ad729d0438d85d115faaedf277a",
            "3276374604914700b6f02af4e2a692bc",
            "d0ef4d83991b4481b7647623964b967f",
            "86de02dcd75042d19b1d565e6ccceafc",
            "8fdefd834f78475cbf8f2aaa423e0136",
            "f594b06307d9445383df51c00657641c",
            "c5b68ca95ea0442e82954b1cd22b5ab4",
            "e0484e3c158e47a094a9cf1c18b5aabd",
            "005f2bbb32504eb5a9a2de8b5dd4f902",
            "3b6353db1035422b9bbc157d66fdeb8f",
            "ec9edace1b9046e4a3e1effd6b55f3ad",
            "c5f5e51a7f7f4139ae53e5e1779288fa",
            "6d9e5d903257425ba0ce01eaf18ba6a6",
            "88c90e071a5649119c0e49ca96c22cc7",
            "e2bd3e39b58d423d8210d4132d73aa9d",
            "3b807f8a48794484b03f9a0fd80efa14",
            "1554d81329454b2f809e9f5447ec04ca",
            "1a3dbf6f67674ef7adadf9f124442f0a",
            "71a16fdd71c34ddea0902b9929917060",
            "46db95e1266045898340b1ee7170d7a9",
            "e28bfc5db1fa48b281433f0569c997d0",
            "c62b64bf4b1d4d26be360e0721228db2",
            "4aa0183fbb3b4e2dbbaac05b00772ea4",
            "a00f7e83839d464b9abfa4dab5b14350",
            "0861b5d7efd64e9581c5b5806cd4abc1",
            "8ad0db83bd3f44cd9835575fb03bd251",
            "f6e87efc53f44475ac0859a52289d11e",
            "45f2e0f4669f42669fbae32d7064e5ef",
            "cd6ba1fb7c2a474d9769204f8e299be0",
            "02ad51c9b51649e7b13657ac892ffba0",
            "28770836b04c41ba902ccaf7d3d1c879",
            "8a0a6d145fb542ecb2b249175ee297e2",
            "ade3e3b38f6048fe888f6209a7d3422e",
            "63c6e36ff5fc4596afd6001d340c4dc2",
            "f47043baf50a4a77b51eb15f6065b069",
            "ec85d88c55584aeba6dd1c35048eedc4",
            "7774865326554756b683b28278979c2c",
            "9ffc222a378842e0b5e767f71a9b78cd",
            "862ffff9c8b04adda6d95a47244e642b",
            "ae1b7973aa9b498c91e6d26ff362446f",
            "d28a43af84db4a5188473d43983ef767",
            "d1bb72a538e340e5999adf4f3f0ec022",
            "fa93bb094d6c4701b231c6f8caa22ce8",
            "7399ae9460084e99aeacf4767b56a2f4",
            "cab47f9217254770b4fe323677b28399",
            "25ccc1ec31b24e619c24f3b21a8f0932",
            "203dfb2ed7864dc3abd7a9be96a19698",
            "463dff5abd6a414d9c296f52493ad0fa",
            "8ce39888165644ba99aaffeda32b00dc",
            "330a1bbd9e2f474494a8b04f576814cb",
            "810b04e90dd8491a9711cf6bd9c5e856",
            "ad289bdf24a748539108ee921dfcbd5f",
            "26d9077252a54a32b23844daef5d3280",
            "465b11a2c0d04196ad1000b479168f4b",
            "d20634da5b894274a1ed31fdfe164c84",
            "8954a7e7759b4c929243d041386f2a64",
            "582b035695344bf8a9026fb7f17aaa70",
            "fbf9d78e6c01465c9f601e897e7719cb",
            "afd39113f3e34e349a788a3a265546c6",
            "7fe4eed94eee498d96a8ee3e823c124b",
            "49e0f291029b4e04bf697e5cab57b092",
            "ea485e52a20e4d63a7bb7380b6d82dce",
            "e04359fda0904dbd9be598ecfab67349",
            "adced9f5620247faa5ebc8066390ea39",
            "2645c2b783344ad29cdf924722b94390",
            "f4933b4fc0274c0fa3ff86e36d635df8",
            "fe551157772a4181a78eebb424a08b9d",
            "a0e8b05e62cf44cea9aec4b43b1880e0",
            "f4c5cdc055034f5a86b4d8b2d443719f",
            "f30124280111413fb38e6878035908b2",
            "4cd41c7b222f4e9e99f41e6c932430b9",
            "0d0a76ae30314d15815e5d506d8c150e",
            "61428f5e4d70434394c3311280ef4888",
            "21fa2b32c48e4fd19b1e041214782945",
            "ed93f6db97f64c068d977124c3c336a2",
            "37f0c5f39ce84f4e8e4cda5ce890ad9f",
            "1a08b1f0db324ed79113f37d9d59f009",
            "13ff001b2ddb444db8d6f407bc553bed",
            "98a156f712fe4f78ae5040f53625e5f8",
            "8c5adf0b56bd4d41997806769178949f",
            "297c30a5a27244a98ee699176c450dc1",
            "bd3be5d1cd9e494e98f38f8be888d071",
            "fc5ff269d5064d4db788e19b9a4be9f4",
            "5f205b92934749a0bfc76a6aed4f64ab",
            "b159cd0951234287a223bb7fa6d57595",
            "ffb2d410c7ce41ac94cc36b91a764efe",
            "a78aed04654c4d3eb28d8de9fad08a1b",
            "d0721025c725415a98f0d63fa30b368a",
            "32115676975b4d3eac7fcd2e90c7cb55",
            "7b08ba6bf2c140659f1e90a0ed4c0aa2",
            "72389a2de6dc41feaac2b915e3bd42bc",
            "b57f1773d35545edb38f00b350e30884",
            "c9c5d1f0fcbf4c29ad478657dca6315f",
            "8cb169bbda5a443dabddf4b9ea12f7a2",
            "b47345116b2b467b91ff4dfb42e473c5",
            "d35cd271cb5046bf8d3fb39e34981103",
            "c11198ac7af9436d8e0de3399f9f35d6",
            "cf606c3e36e4416091d6ce61be91252e",
            "844cbe5e87f94af1a138dfcf6d131ff1",
            "76fc8f4cdba849479f8bdec7c1f22efe",
            "aa028236c10f4e308cde2178a3a863fa",
            "1f54ede9665f4d7f9160fc11bfe5d8b6",
            "8ecfa33ec4294b13bf9a6ac7c661801f",
            "0865fed88a7f4089ae9faff345c1f574",
            "d99192977cf84a23ae580ee97918e353",
            "4265c5a7ca834a039aae2b66491e7d30",
            "832fedce3ee54da6ad28c99574237e64",
            "9ad2522f052c4adca865b45dfd3f9ea1",
            "29b4dd927299407f95167ccca0cf3ba3",
            "404706a0c9d941e28c2fea4d833824c9",
            "c829e47d6ddb4d7d9c98ec9781c86941",
            "072e73bac51a471da09cc2bf15073864",
            "5526c7ca2a6b44dfac503d401ce55718",
            "655565c9ba6a42e0a3b806c20bd10eea",
            "cf0a8b98fc4f404aa3a780c4f46cd30c",
            "c23ee9d44bf54351bde302cd1eca407d",
            "d8e02e4942414a01b2e2d8d16190620c",
            "2851c5a25d584445ab8b270976d2a7e6",
            "b2296235d7b742398ceadc1349466da7",
            "43a9172e3bb14d089b0c00c39d1133ca",
            "76f995c6eec04398870684ff941655b4",
            "0e5c5dfc56564a3c81b6443a19a3fba7",
            "540e5a64e9b243b18ce42367da99ec32",
            "5501f120b26045c486c11089482adf0d",
            "8fbf4287563c46b0b137d783487dd89b",
            "e0a7feb963014974b20ff60c29f4c8af",
            "e501cc4702024836b156d4ddfee0fa23",
            "fcebef01c62f40ed8aa3b631744e1b57",
            "0197945cb04148768b31687b7ef0a9b3",
            "8b7dcee5f70a4f05893abc6d3afd4572",
            "0a82f30247144f15965230697476d09e",
            "bb337c4abf0e4afa8f5a52edcf0e24d5",
            "51521648d80546e1ae77abe307a84e91",
            "be2d33ce61df401cb27b51e4e0a7be5a",
            "67eadce4b5054bb2ba9741427f5aeb8b",
            "b01a095ec3414cc98e6e1576e3fe6ac7",
            "da304d3e8f0d466b975fde2b0d192c86",
            "5a68a38e03e6489eb8a7747941baeab4",
            "518b9522532e4edc86fb12ae42cf9a24",
            "1321b0dfdc1942208aded2a66c95d874",
            "bd12adeb63da4351a8c945b8522f9f57",
            "c9c6dff0df4d4e3188c6ed13f31d23e4",
            "7a7697277cda49aeb368b85900e7e596",
            "7e809410ff574a9eab21f8f59cf07bc3",
            "6cbb30c52fb44293839a8bba5893f48e",
            "c8ce02da4cf04e648ce5257780e4d328",
            "af64105c1dc4411f83fc46413e05b7b7",
            "edbc0eda4f014e9c9d5d8f1146ae2980",
            "03ca0efc204d405cada3f6971b535323",
            "2d398aa4c4f242ed85f5eff3ea7307cb",
            "3cd9d70d10cd4f89931657502305cfcd",
            "de323a90afe5460ca6ad397e64f14910",
            "b5c8ba97c82b4ecfb64599501a4039c8",
            "08911a02085c4a0cb5df151de12ff90f",
            "79fd0224372743db83a139860ddad99f",
            "db502e0420124060bf31865f5001606a",
            "40e9468224e449d29ae2b3705c455e0d",
            "3d5735af3d1d4d1bba80b59d6a9d9289",
            "dbcfdc3e58ce48858bb85a5b38548576",
            "46a6b552c0924e25ab6fba731ae8f880",
            "2a190a8132ad49f28d7f96bbaeabdb54",
            "795a271a5d6c44bb946d2f7c28010068",
            "39279dccf01849969205937330219779",
            "0c196a54d9ac42be875b5de204fc2bb1",
            "5662ffce0679419dacd575024fcf2d51",
            "14ae56a8eca944a2b55fa24540c04f57",
            "48e59c6c112e4c17849b4a5f384b91a6",
            "986efd0af06a4f508d3ff4d16fa2b2f3",
            "704b9a59cc9b42b78f6abedba39fa7fb",
            "8fa6e8db275d4aa489bd796408b081cb",
            "51704195e3ed48029900b3f4f7edb59f",
            "71329b62bb304b8290c3c5eae0c47782",
            "103196e93abe4459b8ded22fed651511",
            "c8f3325678e34984915a47698346e57f",
            "f39b5794daae42e4a1a6160eec391a1b",
            "1a878edb7d3a4171be583fd2abff9166",
            "5673dac2fdaa46e8850b0e568f8ebc55",
            "eed24e5269a54faaaced11d88cdf502d",
            "cc2b2a8998d241728ba17576c2f0323b",
            "94e82d8419e9439ca20a3b078f815b9c",
            "1dae155ead7d417b9710d513d5f397e5",
            "f53d667df5ab4b3da96bd75aefbc1622",
            "af9ffbdaa2374049901a2d3532d0d629",
            "049e0fef13fa42f49cae4d2be8559d2e",
            "ffeedbbcc35f476188875bad80e651fc",
            "e55f3e4731bd4d0f934d56b621e66823",
            "7b577cb5b3274244b39690c8fed960c4",
            "97b7b5a21f6a473884c8596d7a154b9c",
            "d21e0bcc8e9a4601be607de91ddd6ff3",
            "c5682ddf40ff4293be0373281c3f1dd1",
            "366549d471924f91958e082b9275183a",
            "4c02ce8161f84967a55eb896501c5de9",
            "66797fd01aa7451c9eceda5bef0ccad9",
            "e7e5df11f82f475ca3238be780103de8",
            "6bf435558d79478e8f3cb3c2f9de2056",
            "b54c92138c7044babf7042aaa93d6631",
            "ba45db70fb564c149aaa6c22aae76a94",
            "12c19a8b7f89456f9607ad834c810f91",
            "c54534305d6147fe9de8ef6131a5dccb",
            "eec8d5a0509e4b07ba27211f25835b1a",
            "62d2e1a34e104b398f8db6abc1f3b201",
            "f948256e91bc4229a9a94d1f475facfd",
            "7ef88999325b4b2892a1451cec24c04f",
            "f516d1ff03a740bd89684ba9d12e6041",
            "e083950614f04bb0bb578c4f14ca02df",
            "f44ab1fc7eff448e97fb78f6e9b12932",
            "84e1a69a3e214badb5a40d10824c6b62",
            "9278c1158f4e4507b3162e4af4b409d2",
            "aea9eb6bdfff48c1b9a71a9f99f4eebd",
            "a124c47b57f8420380b4a2aea4b96ca6",
            "39c33c5dba57480d95a07e23381dc85e",
            "e580234c4b8b400ebdc2d8610af1d888",
            "bf7785003a004ee898ff217c4c86203b",
            "b3643ad2c3d64ba09defe7fd88344ddc",
            "3aaae09ee8c5450e91613279d12d9096",
            "2fa4dded393147d4953bb0fc9000770d",
            "20c0e6b4c66b46fbaa78d87fb8c1789d",
            "7989b6223a534e0bb8ce398726d13d83",
            "78918a14c9eb4903bcfbeb72ed069cd7",
            "2f214c5063f04ce7ac4070a1a7e0b6e4",
            "c3f6f46965ff4d4e8abc3be97114381c",
            "45a128016e9940b6ad972c50f31e169c",
            "93c2b6fc2791438c93345e7951a25add",
            "5785387b3c524aca920a3c7b96a5602f",
            "9aa69460fb404dfeae845069dddfc76a",
            "8e2e94c5b083459aa58365616870a644",
            "c5ce004568b2458db955e83549489f74",
            "e5e6f9b5d8e14a529e4b42e12c28bce6",
            "89f86035994042a4a8ecd32c1b1e1776",
            "a8fc1ff04f5148bcb5a5b3967fda6298",
            "59c3ced42df14c04a01e665e336a714e",
            "06fdbe147a51456098580c25348649ca",
            "510d97c80d534285a33785e32c1eeb7b",
            "e7aff2144e724341a483491a3cbd76d9",
            "72cbadcc2f724e31a28d9e5891cc4acd",
            "7b7059d1c03b44b59bf417dd9e8c1c2e",
            "5df0ad9522cc4538ac6d7637864e5903",
            "5d4aa3dc77344674a9d33c421403b5b7",
            "979e8cecbf024148b6fd0d32374a4a46",
            "d346869a15a14f63af696d1f8637f73e",
            "5070e4c04e234afab74c38bfc4dc5779",
            "cd3b4c9bb5c845c2acaade06d838b53b",
            "0b010e3b1c1c4711a858cf48585bafde",
            "209008a2bea9459ab69a4fb9200434d7",
            "68ae49e3312d443d9a388a3b6fbc3286",
            "7c9157fecc8f4445a230b5d92741916c",
            "58b6bcbe93374845ab4109e037257f3f",
            "7cda9ff493d345b588528e5926666775",
            "bac96591520e4a71a173c1dfb52b0560",
            "f8f80bad46244c70b4f6809ddc8c5c84",
            "592050d745884b479e7864b12a70812b",
            "d7b3587895534f31a1d0a1b7f9645998",
            "b8b00dea9df44cfd9fff3649f8e8327e",
            "9ff8d01c779e4587bb21ab1ca8e59d7e",
            "794c0129242d40bdb16abc89163b57cc",
            "76af291c5c944b5090ddc9d839c1305a",
            "c85ac4ae40cc435f8c320489b64482cb",
            "d47ae4c2bf9342ae9bd6e4458dc22737",
            "b90ab1cb7f20479da4b18e6055378e31",
            "1b567ced3c184906a17280944ebb5016",
            "03d7677af31a46ba88c3d870a9b36a8b",
            "43a1a526e78d4fdb9031c900425e3934",
            "92364e64479c4a4dae88639f36a3279a",
            "c3f7ea7500234635b3bcb748015ca587",
            "b2d0164a72f544028c8d08be88ca4470",
            "4da648e9f44d441d8227eb9f32f92641",
            "effdbf0e447445d7b81d104af3885889",
            "ffd9d00e5fb848748b2f550187ec268c",
            "c04e7ecadac6482ea961025091b6005c",
            "611f28cf96744c5a94b05faff77b5b00",
            "4b432ba5ec4d442d97908840f67e1b27",
            "63174fde212d4eef822daf8b0b0a7bac",
            "31906277c1ac4bf0b39282f73bd5ecc5",
            "39e816d4337347669eaf134e28067bf3",
            "87b064063ce040239d62acfe65ddf209",
            "05da7d105ca040daadde1ceb18d329ba",
            "e11541194129487194ccd3fb68c23884",
            "b3a1c2d9d3f34e4da55d4ca406cfb8e7",
            "5e6bd7469a7b462cbb693fce2de1674a",
            "25c0251b924a4460833bec37a31a2498",
            "d807f9b0a9e047a3a4c3fdee3d168b0e",
            "da2eae29925b4c06b83caa590c142477",
            "c1d22b15a73e440e82a375c20936759d",
            "6787863a79504365aae9d812b9e7447a",
            "6926aa7048244362961452770e3b0d3d",
            "29d197c810644679b0fa65d5dcdc5a0b",
            "01b4a954a9814d9f9714676525abeed3",
            "886d9fa5282b4f80971492dbbdcd9bfa",
            "6eb7a5a3f6b74411adde1dbde5e113a2",
            "311c6f42f198450794c7331d25e43560",
            "7822372719584611bc2d75f52ed2dcd8",
            "e097c452ea124dffa40d9dcd22d43da3",
            "18e1be2e1e9d4323bb74064377d7702b",
            "a53be606964c40b7b14bd27a19a60413",
            "4d5f9ed3a48a4a5cb39f3e90546484d9",
            "7128e5730e964f06b36b31f2ffe0f8e8",
            "5e0dd44dcb254a8fa1ed98f38017fc65",
            "15e44f30e09c4b7aa9627aeaeaf75c5e",
            "47ce45cf8b514a599bacabcf2f488f4c",
            "c3889dd5f80f454aaa5399a5efeb2d2e",
            "05baa0b38b084045bd49e52c2d2bdb5c",
            "2a3225f5aeea460ca7e9046fcb3daed5",
            "2d3111072d464da4aed192995aa13e79",
            "71b0ac0aa09a41de98ee3f95bb122c63",
            "0f320668e4f64af186195d890185da7f",
            "cc1c3b4476f64df18729a0a30129cf22",
            "a1de833ce23d4c0d8bab1ece3ecb6574",
            "9e418997f5894fc1948fe9f808ae51c7",
            "65b5b8ff31534724bcabddbf40bf1952",
            "c9d02e1e8d504f829f169fa72a85c3c7",
            "c9623af8220447ee9093687ab9ed778d",
            "b1883b0e5ab74d46b5ab17c91201a6aa",
            "4cc4794cc3cb4a568761d5fec03863b7",
            "ca0f44d201014db987131151434e25b4",
            "1531532d75264062a326a567e54359cf",
            "5a141a2bb7de4843ae66ee9cdeb4631a",
            "fa66fa12dfa74a9f81b0bd0511af9081",
            "de0cef96df4a44608e865ad8a7b46556",
            "7800223a0aab477aa0adfbc9051cd619",
            "c7c778c6adca4587b81471b9fe90e099",
            "3011c52ad8d6456998f138d57dd685d9",
            "592d73190a7749d092a81dc8227a7321",
            "9972126748e0444ebc67d2f8d9eb6108",
            "bca3ea4d34914469b3f74fdd2bf08620",
            "77b0e07eaa054452af8878c430413ddd",
            "c47b861e021c41d9bd24c4df75eb6b3c",
            "f0923110f2974f06a9eb6a1ab8cc5496",
            "2fe72772ef1a4d13843f4d3f72aee42e",
            "baed585b17174fa38d57fc56d9fd3e08",
            "91a04da9ada345e6aba8e5e9b162b6de",
            "3159974f515748ad83d5ab7de116a4f7",
            "fb43ca18b1814609b88ce8723f238e41",
            "4bb771abda264b6d9a733e2e69cce5a6",
            "6496cb0a1bb14f579903857c456fa4bc",
            "02c7a0fa0c7949c8afea74a902755e18",
            "bafef0a639f54d20818478d30f15f531",
            "c818fb1aa8654a03b046b7d6d0f926fa",
            "f929c72b1d784339a8db010d7e8ad441",
            "fa0cbf37bbea42f4a0baaf186cb15e66",
            "bd30baff8a0a4b459c442b34348bb58b",
            "cfe89845272c48d697b291c9e4887c5b",
            "95b6a77444844d76bb742ef90ed88a85",
            "c5762010c75a42f6a410ec6e0d5a0d81",
            "3986f07a8b8d493cb6b373d84ac10c9e",
            "454d0471a4db46d391c39240c2d3fc48",
            "8cfe8ab8383746618dd6d2521ebfbd38",
            "19c4f5c76702457180f0c0805aac2038",
            "e44428b555364e318e0ff016d7332b4b",
            "6bcabeaa1d0d4c0aabd32425ce663447",
            "ef62c5be2170493298c824bf5027e6f9",
            "5269f07babd049feafad3cba84539f3c",
            "09c936eefe8d477c85ef6f67c6208dc1",
            "64109761c84a4addb16a3e0c914c10c2",
            "31cceaec99934fa2b192b120d0e16bfe",
            "d877aba35d884f989a08a4af45d94fda",
            "5769c9ed9ed84a888dd22b453c7ac863",
            "74908a3ef96c4577b3eda49e00409724",
            "0e8a03a155134eea8917a31b4e368337",
            "5c0d9efc932f4c7b8645e22c1a34e9f9",
            "c1dd9ccb430943f3926cdc277b955edf",
            "5c28a96616354f9da290a0419a51c864",
            "a4703ddb80aa43afad389a409bcbbd06",
            "34e1a699cc3a4ef1adba9f1f60196bbf",
            "ede9f6374aef4beeb7fc47bbe09d9c3c",
            "2f714f6b29da489d9ce444f03aaf055b",
            "524698cd5eb6403fb7054e9f0f694f3e",
            "2aa7c543f4934fc2a98fa7faddd24ecf",
            "c7a663888ace4efeb6ead9ac9f7d0201",
            "2575ac31cdd54ac286a266a0ddad65d6",
            "7c2c13bc258c4ed38cbeb167676c0e4e",
            "203bf4264e0a436a919560bfa5a5bd80",
            "7d897489b0254047ab5c940c124526cf",
            "70349828bb634bf79ff0dd2303c42efa",
            "3937dcf7956f480788a851f8ae3a9d3c",
            "2125e5c62f4a487e87f7992081da3558",
            "c2ee718028ca4f24bc7083d5033628c3",
            "ef746d45ddef45ca90244b82bafabfde",
            "5b1784b5d4514f88a1ce996a9e3dd6f4",
            "5badedbe646b4467b7da9cbd8f101543",
            "e7f145b8f4814ec5afe54b00176bc48d",
            "fb16357ee407467cbcbaa1a6f183b9d9",
            "8883994b3fae42899654900d960434b2",
            "4b0a8b11be6d4e57bf0c9b6e42a77faf",
            "38832741c42b4a24a8ebe2338e67c36b",
            "e59873a1de57499db3e8a0545c0863e6",
            "92ee57bfddbc46adb15470e775ef471a",
            "8b97ffb466bd4da49e48d332287419a4",
            "7bd4964539014832a6a7622a0e2fb4cb",
            "c65ea7178aba442892e073ab6013f248",
            "cb402c8d28c3426cbd6d472d805ed7d3",
            "235378c61fe6466a9ceb9e0d6dfd97bb",
            "9068f6e1006d4233981d375ff997f850",
            "a5d75ce709d445fa99f360a2133ad641",
            "49e0e907d46f4db78ebbf2295cafa31f",
            "cc28732b7ff64c86ac8cd5124beb6915",
            "7a8fb7eddee04589a7986a6a65e18613",
            "a37e17c9e2e74144aa07b730f86c378a",
            "e9eacdf91081400f92ad42fcbfb2046d",
            "4afc4c9f55da4b76a9800e2a3b9e6228",
            "dfa796871bd64dacb48051304fb54230",
            "a9cb89b882bf4c549b82e7d83a0a24a6",
            "a661d99bce004ed28581177cf9f55e11",
            "3177c84f58ec40bb88718bb2dc9f1aed",
            "2d53faf8aae74fdab66d6312b0e92c4c",
            "3181e10a3e79471cb0c7de9fd08b3384",
            "d0ff3f41ca1941e089dc07ebb7f92119",
            "f7bbe4380bc1477795caf47c9ab431a2",
            "125ec5e591ad45679b11066111f080cf",
            "32c0d799ac5942c9b087adcfca73486a",
            "5c952c52140b414685d69a4719b9bf5f",
            "a1e167efe2fb4fd8ba003a2c1def6873",
            "781b33cf6a4541e2ac42067176353400",
            "d2219ec7340f43628ae2c579718ccf51",
            "0b75bff0c040426abf20d160bbc414ac",
            "45d40f1e418f427f81332c20f3e4cf23",
            "e578b60b853940b9bb0c1ce58d860d0c",
            "0f85743fd75e47a89f69f36f85776177",
            "be96fd7d090f45eb9ddfd9507037194e",
            "6740b15bd4f44eec99db50ef23eb682f",
            "b285e650d84c4f2bb17d952c747a77ba",
            "4ee5d713eadd487cb385a273370eef0e",
            "e58d887917264f7fbd61802310772d42",
            "0e3976b0dfbd4cb28617197f15ffe474",
            "37c7f84b72d347848cda88c68ff7a261",
            "efc2912b3f064d3f95a077d499d4eccd",
            "05605ff5e8f14738be5e3a5aa86598ae",
            "268801ab956c4cebbc8176a0b012d49b",
            "72fe5c8a10964a20949419f5822c3454",
            "1546407e0fdb4a259354355e30640637",
            "836f2afee36b4cf1b1c9789cb5364499",
            "6043eb01c1c34f00aa7a508fcf29cbc4",
            "82658c6fd89443e3ac4d9f4daef3a3e2",
            "79b76799b4a84956b22c91e3ee2ad341",
            "378abe224fb0410e9cbabcae7dd4027b",
            "e1ead08590914321babee83d015ec42c",
            "4443480414ec43ed802b6c67a8beea73",
            "b6a41e3179e14251b0cb6e8787b7711a",
            "cd570047a76a4672b19921a134a11d79",
            "f8a29409027542d2b867ec829743452d",
            "06d1adf450c74f1cbf8901b8ce8ea779",
            "e2ebdd346686491ca24b3d049b9937ac",
            "368bcba675c54ad082412375f9984809",
            "0adbfe600a2a4c39b017f1c982a5c77b",
            "50c3879ce4cb4f4eb15d542b8bda9ef4",
            "7cbabb76e61b4ad1b4827ea9a7fe7fe1",
            "f87517cc713e4e62ac7becdaf0d48946",
            "1981378ad28d4e33be6850e4454c3419",
            "735ac9eafc6f48d6a7dc29157b735899",
            "3fc41fe929714062a4b556d476f04e3e",
            "1ae8316330f445ec84572bcb09d5ee1a",
            "169ddcaae52f4c328939a1900eec8faa",
            "6fe9ed34bbf547218ded4d433dcdcdde",
            "78c553c8bd5e42c886dbecb28452d8a5",
            "a172577f416543e697820cd3d8c0f2b5",
            "9a5b0d6f5acb450da4337caaf398063f",
            "bf0157e249c1424c889cce4d491b0734",
            "e9dec14e262f4131a0c25ac90f497591",
            "c797416e75554c41b7be8f7154a504e3",
            "a518158d316b4241a7a35c7a5d7ee22f",
            "78d8d2be32b2460b952a00b97e51aec7",
            "a4c1c43db5a5444db2a61e267f2420f1",
            "4ff076ea4ff04a449a4830837418dc7a",
            "53583a908f814260a18b2d7e35e041dc",
            "f6e552a178ad47388201d880efe3cd7b",
            "bb3a1a9c02f14c7d80ece4c3826c0381",
            "edec37ba8ba54d07b779f51a58e8c5d6",
            "3d0f58b9533a46e3b3c8bff2c8061628",
            "27bc028e49df4f05b0694aa7fff4c0fd",
            "373d4de41e314b5fa9558c7f6168d3a7",
            "e6e98107a7e149a9a912580ca4496dd2",
            "59ad5a2010cb4090a638154b1bb9776e",
            "343a99652ea348bca5a8ae00748bb44e",
            "4280e3e26f324845bb25ed1a87e4e905",
            "9bca5218593e458b93e26d21ef847d07",
            "95142aa10689494ca83b41a86480c36f",
            "2e61279443764452bc69648190560636",
            "7172e4736b414130ba58f2e158b6403d",
            "ec63b32a09fc4d56b5d6fc44440b96b4",
            "565fed9ca32e47299f237023897cf62a",
            "230fbd74e433405b81610f845dc2d4e7",
            "03cf76956a1945a2a40dab064ae7d628",
            "67741abd445e44909e8bb9210fe8e80f",
            "8b8f8246d28245d194f4472242214ec6",
            "4486f80e0bf1424b973b0e0528d6675a",
            "426bd1c9e32a4c098f0f6d987c695e8a",
            "f42a8fff75df486f9e93d3dd441c9550",
            "e23bf47c69f74fe7b84533928422b1e8",
            "fb51bfe333ea4346bd45d23e2cef8389",
            "a72c6b4c947e42949f4900331d60efea",
            "4af63344bb2d4460ba02dc72f78590b7"
          ]
        },
        "id": "wxuJaM64XpNH",
        "outputId": "6da375d6-9c36-419f-accc-b39339e4e35c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a6853dad34ad41fd940f0182f607025a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0/96 [00:00<?, ?files/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "affbbfd219ba4604bcfcb3959866c72c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/105k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0aaadeda48ed4202bba92e37107f7a3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/55.9k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aba5db74473e4209a562ad4b9d49cf9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/30.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a2b268936a264017a2ebebf5bca9c174"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f68fed903068428f8883af66580b16a5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/31.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a99ddb1b1914d588b1be61e3731a13c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c0d7768c42b40b9ba44dda3c27edbc2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/287k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1c36486525147779667d62024c922e0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "055dc5f2f67a4b29a24031ef153ff6e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2436fd6303664858bcbe2834ad750e31"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/158k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a8335542d4564e3bb83fd0cf87ca32f3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/56.6k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71891bf763ed474baaa456dcf9a195fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/86.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f9b332a44f54a4eb5763cdfbc80114b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/32.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6abf4e7bc7384ff58f35bcc4143c16c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/30.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c97a76f35644e16b5b419e89afb1d93"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cefe7983451445eb43716f54cc106ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/29.7k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1d2ebddd466c46feb92b179d83acae3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/58.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea78f951117e49aeaee7e4ae163db179"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/56.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be69a8f3ea294789a295a8b9831d46b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/396k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "537ef95ddd8e4346970f193d09c32407"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/396k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a17f1ad00054e2fb2dfc8a56473956c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/31.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "838d84facc1c44eab90a4d149020127d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/276k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6514b58eab7b4294b63c2596244173ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/251k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1e03fc38f9a64e03926b17825240b39c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/168k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb79d2814844a26ba2d5825ff5e1e02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/162k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dd6416502634c2b88ea7baf777528c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/267k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f20c829cf6484e719646483ea5a3754b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c48bfb4aa4e43a18d1921b93c8317c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8d96cdb7fe534cb09f7e237b9b3362c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/250k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3cfb885db273457fbbb5e2a096fcbc3f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2400c400512427f93de66cb64cf105b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/2.99M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9fe3c4ec553640e28ad29d50f81ee741"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "89bd7a123de8408f8824af01c8a199d8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/631k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a77e737bfaf461aa7c21ad70a5b3e1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/3.53M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "85ebfca2950f4ea7bcf8805f6ddb7fb1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/632k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c0c5e72880494e10a2189f4d92df03c0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/785k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1f38d04a1d04b35bb69970e91618ebe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/605k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2918f804aca743a3b7b79d7714c57fdb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/4.18M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ba53df57581d4d42b0846082d649d0bc"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/3.30M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42047574558f42b38d1f90f30a66c545"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a3203679f7243128b948932c2336832"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/4.53M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b00c166d08774b36bfbe8bcf8aea81d5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/4.62M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "684a2ca712ad4a40bdfb36cfbbcda653"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/4.48M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e726aad84e3e4cd1af586811b8607a16"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/46.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "726789eec3a045d291d7162953b62bf0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38c25ac4b832493b9af32d98e7ce326c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/221k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "963da411e3fa4d92b8672602e447c117"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/44.5M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e46a21ab186a40b5bc809d200dceb05e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fc87f31b69d44664b2efb0d5b52b48ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "883b03b4248a4db0838bd44694c537ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/44.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2e9cab653284447283d346a4b9121697"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/47.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "350e38f58c9849e8834a63d1d3c9ac62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/86.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "55f3465cc55e4d12896a74442c751117"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/2.03M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3423ea05b941492ab5b730e2dd155d95"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74d1bd59ebbb4b33bd0a60867e1db851"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e155f939e6bc4b598e06eb1e0b320843"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/2.44M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "26641d2116df4e8491414eb1fb7f4e89"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1ffcdd2013d4cb28a29a6ca53b45644"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/7.50M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "689aaf9bd6c74c8fb1ddc629024ac617"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/7.12M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c0795aa77e24711a899a9d1d96631d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/8.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "585e767432be4616b96d4268a28b864a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/7.28M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cba037cb99b440848be015a3658b7d73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b178f83a2d524ab19da6d4d901068d3a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b48673b8d7cd48388d4d951a8a899306"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/1.15M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "133435a4d0c84606999f7d020791ae2e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6e8302147df44ba94f8faf09cde98fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19062abb873142c39da973776aa9d05d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e37404f33ca04e0983b38347204c9a0f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc44b92bc2f843519f741f9e055bc5a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e08a2f4f9d654ad29087025f0d632201"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5844b12784443d1ac5aeae5035b4a84"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cdb5dcdd3eb24b5e95d4d16045b2f934"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dd2b813633e74b4bb7446c54501ee486"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0657d28485934a1988dc19e40b42349a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c2cef5c94ea148a08f7e5c521eab0e1f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/924k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9b869a6185d548bf8be8daa274eadfe1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/965k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ea7879ad33d425e894b20e59f4bb430"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d15e7f423c7f4ad7ac18a6397729c6fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/553k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af5b7cce44024cc3b80d62923a4b5910"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/983k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74f6414882134021bcf18e89ed873f08"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/814k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fa16778b2b4b42f99bab22fd7f416dda"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b59304f6d1c44398a2701755762606d0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/109k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5b68ca95ea0442e82954b1cd22b5ab4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/84.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1a3dbf6f67674ef7adadf9f124442f0a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/108k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd6ba1fb7c2a474d9769204f8e299be0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/214k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ae1b7973aa9b498c91e6d26ff362446f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "810b04e90dd8491a9711cf6bd9c5e856"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/58.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ea485e52a20e4d63a7bb7380b6d82dce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/58.5k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "61428f5e4d70434394c3311280ef4888"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/56.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5f205b92934749a0bfc76a6aed4f64ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/169k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b47345116b2b467b91ff4dfb42e473c5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4265c5a7ca834a039aae2b66491e7d30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/58.2k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d8e02e4942414a01b2e2d8d16190620c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/29.4k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fcebef01c62f40ed8aa3b631744e1b57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "518b9522532e4edc86fb12ae42cf9a24"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/33.0k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2d398aa4c4f242ed85f5eff3ea7307cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2a190a8132ad49f28d7f96bbaeabdb54"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split:   0%|          | 0/30504 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71329b62bb304b8290c3c5eae0c47782"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset loaded with 30504 samples\n",
            "Training samples: 24403, Validation samples: 3050, Test samples: 3051\n",
            "Initializing enhanced BLIP model...\n",
            "Loading base BLIP model...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "af9ffbdaa2374049901a2d3532d0d629"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7e5df11f82f475ca3238be780103de8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e083950614f04bb0bb578c4f14ca02df"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2fa4dded393147d4953bb0fc9000770d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c5ce004568b2458db955e83549489f74"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d4aa3dc77344674a9d33c421403b5b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bac96591520e4a71a173c1dfb52b0560"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b567ced3c184906a17280944ebb5016"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model hidden dimension: 768\n",
            "Enhanced BLIP model initialized\n",
            "Starting training...\n",
            "Starting training for 10 epochs\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:382: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device == \"cuda\"))\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 1/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4b432ba5ec4d442d97908840f67e1b27"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 - Avg. Training Loss: 1.6039 (Time: 339.36s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "da2eae29925b4c06b83caa590c142477"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: in the image, a group of people are gathered in a park. the park is filled with people engaged in various activities, some standing and others sitting on benches. one person is holding an umbrella, while the other is walking away from the camera\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered on a sidewalk in an urban setting. the person on the left is wearing a black jacket and blue jeans, while the other three individuals are dressed in blue jackets and hats. they are all smiling\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter in a white uniform with red accents is mid - swing, his body coiled and muscles taut as he prepares to strike the incoming ball. behind him, a catcher in an orange uniform crouches\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2589\n",
            "Validation CIDEr: 0.0072\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2072\n",
            "Validation METEOR: 0.0910\n",
            "New best model saved at epoch 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 2/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "18e1be2e1e9d4323bb74064377d7702b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 - Avg. Training Loss: 0.8118 (Time: 571.15s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71b0ac0aa09a41de98ee3f95bb122c63"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: in the image, a group of people are gathered in a room with white walls and a wooden floor. the room is bathed in soft light filtering through a window on the left side of the frame. a man stands next to the window, holding\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: a young man is captured in the midst of a powerful tennis swing on a vibrant red clay court. he is dressed casually in a white t - shirt and black shorts, with his right hand gripping a yellow tennis racket as he prepares to strike\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: a zebra stands majestically in a verdant field, its black and white stripes contrasting with the lush green grass beneath it. the zebra ' s head is turned to the left, as if observing something off - camera. in the distance,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2641\n",
            "Validation CIDEr: 0.0105\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2149\n",
            "Validation METEOR: 0.0977\n",
            "New best model saved at epoch 2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 3/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1531532d75264062a326a567e54359cf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 - Avg. Training Loss: 0.7213 (Time: 427.00s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c47b861e021c41d9bd24c4df75eb6b3c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: this black and white photograph captures a bustling street scene in japan. the perspective is from the sidewalk, looking down the length of the street towards the horizon. on the right side, there are two buildings, one with a sign that reads \"\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in a restaurant. the table is covered with a white tablecloth and features two plates of food - one containing a sandwich and the other a salad. one person is reaching for a\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two zebras are captured in a moment of tranquility within their zoo enclosure. the zebra on the left is facing away from the camera, its black and white stripes contrasting with the verdant grass beneath it. its\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2738\n",
            "Validation CIDEr: 0.0121\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2219\n",
            "Validation METEOR: 0.1023\n",
            "New best model saved at epoch 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 4/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c818fb1aa8654a03b046b7d6d0f926fa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 - Avg. Training Loss: 0.6606 (Time: 336.96s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e44428b555364e318e0ff016d7332b4b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: a majestic giraffe stands tall in a verdant landscape, its long neck reaching towards the sky. its coat is a beautiful mosaic of brown and white spots that blend harmoniously with the surrounding greenery. the gife ' s head\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: a majestic giraffe stands tall in a verdant landscape, its long neck stretched upwards as it reaches for the lush green leaves of a tree. its coat is a beautiful mosaic of brown spots that blend harmoniously with the surrounding greenery\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2736\n",
            "Validation CIDEr: 0.0161\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2227\n",
            "Validation METEOR: 0.1016\n",
            "New best model saved at epoch 4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 5/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c0d9efc932f4c7b8645e22c1a34e9f9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 - Avg. Training Loss: 0.6126 (Time: 311.89s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c2c13bc258c4ed38cbeb167676c0e4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: a majestic giraffe stands tall in the center of a verdant landscape, its long neck reaching towards the sky. its coat is a beautiful mosaic of brown and white spots, contrasting with the lush greenery that surrounds it. the gi\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a vibrant red fire hydrant stands on a sidewalk in front of a brick building. the building is adorned with a blue and white striped awning that adds a pop of color to the scene. a tree stands tall behind the\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in a verdant landscape, two majestic giraffes stand tall and proud. their bodies are adorned with a pattern of brown spots that contrasts beautifully against the lush greenery around them. they are positioned on the left side of the image\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2762\n",
            "Validation CIDEr: 0.0148\n",
            "Validation SPICE: 0.1116\n",
            "Validation ROUGE: 0.2213\n",
            "Validation METEOR: 0.1033\n",
            "New best model saved at epoch 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 6/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fb16357ee407467cbcbaa1a6f183b9d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 - Avg. Training Loss: 0.5720 (Time: 314.72s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9068f6e1006d4233981d375ff997f850"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, flanked by two smaller clocks on either side. atop\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of pizza and the other a bowl of soup. a\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in a grassy field, three elephants are present. the largest elephant on the left is walking towards the right side of the image, its gray skin contrasting with the green grass beneath it. two smaller elephants can be seen in the background, one closer\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2752\n",
            "Validation CIDEr: 0.0133\n",
            "Validation SPICE: 0.1150\n",
            "Validation ROUGE: 0.2222\n",
            "Validation METEOR: 0.1035\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 7/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3177c84f58ec40bb88718bb2dc9f1aed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "    Exception ignored in:  <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0> \n",
            " Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    if w.is_alive():^^\n",
            "^ ^ ^ \n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^Exception ignored in: ^<function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0>^\n",
            "^Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "\n",
            "AssertionError  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            ":     can only test a child processif w.is_alive():\n",
            "\n",
            "   Exception ignored in:   <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0> \n",
            " Traceback (most recent call last):\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "^    ^self._shutdown_workers()^\n",
            "^  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "^^    ^if w.is_alive():^\n",
            "^ ^ ^ \n",
            "   File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "      assert self._parent_pid == os.getpid(), 'can only test a child process' \n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^\n",
            "^  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "^    ^assert self._parent_pid == os.getpid(), 'can only test a child process'^\n",
            "^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^ ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "^AssertionError^: ^can only test a child process^\n",
            "^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 - Avg. Training Loss: 0.5379 (Time: 331.31s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0b75bff0c040426abf20d160bbc414ac"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a serene park scene. in the foreground, there is a large tree with bare branches reaching towards the sky. to the left of the tree, a wooden bench invites visitors to sit and enjoy the view. on the right side\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the dry grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2806\n",
            "Validation CIDEr: 0.0150\n",
            "Validation SPICE: 0.1158\n",
            "Validation ROUGE: 0.2255\n",
            "Validation METEOR: 0.1050\n",
            "New best model saved at epoch 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 8/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "efc2912b3f064d3f95a077d499d4eccd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 - Avg. Training Loss: 0.5112 (Time: 311.35s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4443480414ec43ed802b6c67a8beea73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, indicating the time as 12 : 30. atop the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the dry grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2778\n",
            "Validation CIDEr: 0.0166\n",
            "Validation SPICE: 0.1149\n",
            "Validation ROUGE: 0.2246\n",
            "Validation METEOR: 0.1044\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 9/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1981378ad28d4e33be6850e4454c3419"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7a41d25699e0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 - Avg. Training Loss: 0.4922 (Time: 333.22s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c797416e75554c41b7be8f7154a504e3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, indicating the time as 12 : 30. atop the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their enclosure. the elephant on the left stands tall and proud, its gray skin contrasting with the dirt ground beneath it. its trunk is extended towards the ground, perhaps\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2790\n",
            "Validation CIDEr: 0.0147\n",
            "Validation SPICE: 0.1169\n",
            "Validation ROUGE: 0.2245\n",
            "Validation METEOR: 0.1052\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 10/10:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "373d4de41e314b5fa9558c7f6168d3a7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-28d561391d05>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 - Avg. Training Loss: 0.4815 (Time: 316.39s)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "230fbd74e433405b81610f845dc2d4e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic clock tower standing tall against the backdrop of a clear blue sky. the tower is constructed from light - colored stone and features a large clock face with black numbers and hands, indicating the time as 12 : 30. atop the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a man is seated on a bench in an urban setting. he is dressed casually in a black t - shirt and blue jeans, with his legs crossed as he holds a phone to his ear. the bench is situated next to\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the dry grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2784\n",
            "Validation CIDEr: 0.0154\n",
            "Validation SPICE: 0.1164\n",
            "Validation ROUGE: 0.2252\n",
            "Validation METEOR: 0.1054\n",
            "No improvement for 3 epochs. Early stopping.\n",
            "Training completed!\n",
            "Loading best model from token_gating_checkpoints/best_model.pth for final evaluation\n",
            "Error in main function: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<ipython-input-3-28d561391d05>\", line 780, in main\n",
            "    checkpoint = torch.load(best_model_path, map_location=device)\n",
            "                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/serialization.py\", line 1470, in load\n",
            "    raise pickle.UnpicklingError(_get_wo_message(str(e))) from None\n",
            "_pickle.UnpicklingError: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n",
            "\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n",
            "\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n",
            "\tWeightsUnpickler error: Unsupported global: GLOBAL numpy._core.multiarray.scalar was not an allowed global by default. Please use `torch.serialization.add_safe_globals([scalar])` or the `torch.serialization.safe_globals([scalar])` context manager to allowlist this global if you trust this class/function.\n",
            "\n",
            "Check the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.\n"
          ]
        }
      ]
    }
  ]
}
