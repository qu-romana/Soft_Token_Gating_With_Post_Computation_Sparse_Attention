{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install nltk\n",
        "!pip install pycocoevalcap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vq_fHELmOmb-",
        "outputId": "d352d16b-c52f-46d0-ae6c-302d63da5a36"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.1-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.4/491.4 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
            "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.1 dill-0.3.8 fsspec-2025.3.0 multiprocess-0.70.16 xxhash-3.5.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Collecting pycocoevalcap\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.11/dist-packages (from pycocoevalcap) (2.0.8)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from pycocotools>=2.0.2->pycocoevalcap) (2.0.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=2.1.0->pycocotools>=2.0.2->pycocoevalcap) (1.17.0)\n",
            "Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pycocoevalcap\n",
            "Successfully installed pycocoevalcap-1.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install matplotlib pandas tqdm transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxa79aO2Opde",
        "outputId": "2c6c6ad3-2bff-4214-8dfd-d1337ba8131f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "ef6ea094893d480081c4f2bae29895f7",
            "4588ece013424cee96092e47f9db41ed",
            "5da26bd1ed4f417aad8a61bc43982e49",
            "690932c863684b4aa7867fafa57c073e",
            "72686be4f802448cb4123e030a4bdd9a",
            "1b433a6c1c9f43b08d45833c3f029f0f",
            "b8dee7d1e3d64dcaaa79eb7e86f99816",
            "2be196bf35e7481988c59fd449463a85",
            "e97c8afb032142288b3c6c06e8b1ac9c",
            "53db8951e89f4abfadee769423bd265e",
            "086c224a9dd04ef2b93fc67c249b3d30",
            "16168ffdbcfe44a6b4e5bcab282b2670",
            "26ce26ecd0ee4f4c9c354e58816c7712",
            "62c2fa8ec478452cac1573025deddb6b",
            "eec60e3c30ba418db9204912f022ffda",
            "3f7c0956f84a486c8485fe868b27d991",
            "48e3f736151c46eb8e1f31c05d3d3506",
            "f6ed54f773654c619724967d152be1e5",
            "b5c5764a634e44048ce37427ab060565",
            "08e2ab9c75524bf8aee1dd99e77ceb6f",
            "c84f35ce40af4feb9baa47754eb9c3b8",
            "cf22b30cd12f4f778cadff55dab532c9",
            "9cd4081e59a24e8483ef117183da0af2",
            "60fc68d5195d4008b4a9f25fa4693d5c",
            "0490ded7095b4436b738c6f695d6a127",
            "6cf35cd10ff94a06ace49e560c0451e8",
            "e67566a27b184ea3a085077254ac40b2",
            "f31320991350452ea2cb2fe5d0f93db7",
            "b0d6d1109abb422282b3c9da85040a11",
            "68e907010fb84eca8d89a1aabb14e064",
            "f9664aeda6d54ec4899cccfd5f46e71b",
            "6352b97e09ca4593bf583084451c8331",
            "0567bf68b3a6489d87fb6027361b7e90",
            "b9a7e0aa4d7743c88a2a9f1b3b0b06f7",
            "018f0e30bde64ff9ba7d5dcda30b08fe",
            "ea5df515b90b4bcea78487e36fa5517a",
            "6779a4bffd4548d6ab1656ccb0b81098",
            "61c2868e51544a759297ee8adadeeb66",
            "fc57bb0731354ac0869f6b80b8c859dd",
            "9343142933584ef2a8a1a591196527c6",
            "286c5e66d6fb4b1fac35f45872bcab3c",
            "85a76d5c9a0d405982d65df33bdef40d",
            "5777ee15cc534477a88c239d39a28abb",
            "d5f7c6e9a47c4d43b0632d5a3edc91ac",
            "5921d010219949bea5d05e3e4d1897b6",
            "ca1c00079d594745a7125110ab177c0c",
            "7a78578f2b95447ab1d4f5e4bf7d6206",
            "b6c7c51dfa4b4b01af43935462eb25ca",
            "8d6ba483baed4fd084d5a6dbe8e1cfc0",
            "3f3ff491150a4e0e968fa856f53f4e77",
            "d451e3fc1b1f4349ae36e316d37ba6eb",
            "9b53d5e2e02c4149b83277f69a118fa6",
            "a94b0966db7e4e799446a67072880154",
            "268b2ba4c0234c0a9d961c69c4a8d784",
            "707e8ce80b93487f8848d82f6273176e",
            "9d19067095ab4d6982bad812f1b5a466",
            "f6ffba5a90d54e24ad3553411038ec27",
            "830ac2817a1a4cba8521de0cba545539",
            "c7f4e8ad1e324a83abc3554986f5ac86",
            "7b9f6101c2fb42959ff11f2428e970e5",
            "36c3f4106d5744f082849f9cf2eeb164",
            "1347a6138b634d5ba8ce6811ce243d16",
            "6bf1bac94c814a5b8c358be9b2dfe3f8",
            "b28b0f91d95f45b9838c52a75bbc5ae9",
            "92bb85773b6b4feb9fa371fc3654dfe9",
            "a0bafcd26e3047e38f0f9a0d59e927c4",
            "58e2bd44aa864abe8801b4fc38a14580",
            "78ef1a193f12485fabe2d4cb25ce2349",
            "1e211ef784ae46d59911a6f23eb2d562",
            "087283f46c2d4fb6a8c10700c387ba0b",
            "f3d9e45145204ad295a4f23b5523afc0",
            "75850b6e9fb442ccabf34ab295596518",
            "23ab731447cd44048b806c5a5db8a934",
            "e8d18f7ad95d42c481efcf88d6b49605",
            "390f2d28f4774ac58256249f0c4d580f",
            "f4752d9b4012424dbe9de955145e45ba",
            "1eed6ba0582b458cb6e92ebe848df79d",
            "513c46dfa68242958b83d533bcb016cb",
            "e78e9e1917f54418b0dfeb6d20f8209d",
            "3b17106189214ec3b17ced3802aa635d",
            "8f5a009df0564003993d0f723ce16ca5",
            "db83df55748d4a51b6907214925fdb0a",
            "01a95cffc5cc420a92914dee0918f999",
            "12a1df86950a4c1ea2c0be8ec7d76324",
            "9721d1e0bfc144f3b314c053617801a3",
            "4ff21e2842a0439480db289d676c88af",
            "47994ec912784501934e598137fa44dc",
            "e48d5d6d75c6468d91521ab4d928a706",
            "a11916ea2d4342379ae29371ab859ef6",
            "03333fbb956342439672d7d8a07e1813",
            "c03a9e163a2f41fa924361f4fa71b5ce",
            "06f8f21c68a3488f92607e920a4e70c6",
            "d3485a6c4a9047c5959df3de3700188a",
            "b1bef4f793a54ed1b1efcb07b5b1e17e",
            "d888e3c23fef4348aada6a313f802aeb",
            "871d2fa66ff2465c83d2b936250450b2",
            "d77901e76e154b59be5f8871f27c7b78",
            "9207e2d789e64ed4baafc904f9d17135",
            "3551c836fdd543359e095b8998579ae5",
            "0df8d324c29e40f8bb736f40fc804e59",
            "fef23bfdb3184702aa85914a55e09cf2",
            "b0171779d19142868ed07d1889a5982e",
            "3e9a7c06c5d94b5fa79a40bc199f9c23",
            "28ca90f49097465f8032da9abef3b18b",
            "5963b3e8860f47c4905199fa78dd4a57",
            "bb944132007d4287b4b8b6e827047e44",
            "a37648ab472d42fb9aeb24a7a04f7e93",
            "8965a1fee6fb4a6692235559bf41acfd",
            "14a956f659c44265a4fecc2cb7422044",
            "887b42bab1f045ef9d6e0fdd7a73bc56",
            "9e4aa82e98e048ef8246d0a091c8f197",
            "e4a11c9ea0fe4cf39db5012c92641f6b",
            "996e2e9eecf149d1856bb0d500d69e7e",
            "63007a00adb34ed19c105065ce78992d",
            "345337f690cf4c5db81158e8c99c275e",
            "746b91e61a9245389d4532979e529ae7",
            "a8fc8810d4ea4128ac2676ad94663642",
            "22b9932e21424bde84749a0c777d366f",
            "01a77e6e4dd449a8b0b63bb9f9931216",
            "0150294ee12f4dd995193599340691d7",
            "b9034cf53b1a4d49b5ec9f71392fb1e3",
            "5e4ef31b97ae470cbc7f0c7fe0ed4263",
            "694327a27ff14df3a68070ddbf27fda4",
            "10f3d94d3c5f48c49c45937aaa5c2db3",
            "1bc069927ae44874956459f44a6e5cdd",
            "69ab688390664f89b333d484e3656107",
            "aa7766a2eb1948129667c55102843738",
            "4578ecc69bba4ed2a625c7f9f3f48e35",
            "e2922f79270f42e3aa352a3dcf00cb5c",
            "cdc268281b0d46359c1cafa72bcc24c8",
            "8ee1bcf6b1cf4381b69852c4ffc2d975",
            "7937dc9ef26b4836a0909337ae642adb",
            "61a9133ac9ca4679b890048f42b5a71d",
            "ed049a8b55f148c49f7d210159f770f7",
            "500c1a73ad924df087aefc210b41de49",
            "5f2f9cbf7a134fc1bbc67d92112bf8fc",
            "ebeed42560884192b2ba4d3746495f0d",
            "87e50724e0894c1e8c8d8a5fec767b9d",
            "a111f78c07564533a3fa566bf54b57f9",
            "ea3ec77c550f46ebbe49692b9d411d75",
            "a2c8ab3a121b49bc83c6c52a1307c54a",
            "d9c662966d5e461eb2a43cd8b218543c",
            "32496ebbec244f3a9cf4df54f7153239",
            "56f66d5a5f3a4aaf941adb340cb05d00",
            "4baf00bb60d0448f82fc7976675fdaaa",
            "caf35fa494cd49939601eeabdd33f504",
            "e023660d08064c8eac35d5b7a28f3a5a",
            "ad08c7f82be34caea2471e76e98bcae0",
            "62046dbfafc547558bc56e49c20e9d1d",
            "354ec24851bc47b2afcba2d27d9b30c1",
            "e5b8ed3278254c94a0c2fd34df4d5b41",
            "29a3a3a248724b978285a58b34a7d818",
            "06e880a2e5f24decbab85574a28207d1",
            "ba52723abf8b4849b3cc4c7896fa07f5",
            "988aa424912f486b8d2b3c7130d1c255",
            "e085eb2c348943528d1651d5c11aceb8",
            "a4b19bbdc3e4404eb9abb785e3b00d17",
            "61b01c22dd704156b8bc292474f529c4",
            "a00b81cece874524aa11068b0a1e7bfa",
            "ba7a9f7d40664e369749720cb65b0679",
            "1e4f36346b7f404e9a4537d2c20d2497",
            "acfad99ea7af4464a1cceda3c76c77d5",
            "8afc30e77e3042d29bef66b7f0028b4c",
            "f5fb509f9a264802897f041a4451f825",
            "c1b746756d1f46c190d2728059b1abd7",
            "4c11b09c942e4836bbeb550a1d08b1e6",
            "b4b26422fff642a3ab0598f4a74ae114",
            "cdcc2b86925d47e9a6e140b2cbaa4fd1",
            "be3b16eff9804d98950502bcac7018e4",
            "f6b17f52c557487bb47f76658bfaa5f9",
            "9b1d85fe0f2e473790ef9ff2bf948731",
            "f264f7fd2e724581969a60fb5a9aae9d",
            "302044ecb422423fbf3c78a9349f61d8",
            "21b61888deae45ee87ab7383c764ae8d",
            "47ff838bd43545ad9ded2fbe29de9d43",
            "87420eccd92642168b7253206922546e",
            "92382f886d644096a9bf8964c96f082b",
            "e0169f97a64449d196e23bd07eb66b3e",
            "83b1e979bb7543f8bc41d224373e3ddc",
            "d8a347373b9e4764aa5f95f885897e69",
            "d4c11dbb6b8e452580f1270a6c485d53",
            "a6d622c75d0c444bb4f8ad4207c2f83b",
            "fd7edd292cf24cfebfd406a3c0a333e1",
            "345457e3500a4c2b81eb0b798b037961",
            "213e9d89e51746aab9d50cd8a43135c7",
            "dc1b8d6503454e0b8f58421d62d5d66a",
            "e19d2af819cd413fa327fd5b5638f3b1",
            "9632b9a5ad344987af4ce1601a0a33b2",
            "15b68d528064458596beeb94270d8159",
            "ad6a2921d2d74c8187cc0b7ab3ddf7ed",
            "a74369933a574dd1b60810378a9c47e1",
            "32acb41245a645f1a3a1fdd6c061816d",
            "e2b1a15b1b1f4952bd62071d417f441c",
            "24120b13d6474b6f8a4b94908f57083f",
            "10d74e151cc44a6c96ca83e7d85923f5",
            "cc98e7c1675840ba8ab6e9a1833e1622",
            "8e748f228db74ba689ad38f9e88f35a8",
            "e1c5470d8a8d4c70b55c630ba6ba289a",
            "3b71b88770f74b6cbe0d4dd18cf5e010",
            "b5a264b1de4c418a8beed5a1e5d7d6e5",
            "9b63ff3a34fe43198d93d3b8db367948",
            "6a2faa2b4337444fbce769692be0aad5",
            "5ba7066b918246a7bbce907866d89f9d",
            "560f7e9c1e4544ec84df25276bbaf6a0",
            "7e36855b5a134aca8fbe8a4248f6628c",
            "fa68aaf96e784b9d858d15c4dfb38b1b",
            "efca619a4541407db5d8d4fa0b2c617d",
            "03857bd56f4f42448cc9ff5fa906345d",
            "054dee40f0fc4c3dbe0cea1d385d3d2a",
            "2674c7a345d3439d906862bd0a7c6655",
            "cac0dc24c4234800a536574ab1ad0c56",
            "956f691a1501435ebe28bd2f1ce25954",
            "b3b8b8a87dec4e7dad75aad7f952aab8",
            "61b4c76152a34ae8ab43a12758d816ab",
            "0766c10a9de3498bbfeb59fd56761757",
            "4022533c52b347ff904340c4dfa541f6",
            "9c2d8c698e0b409dba8ff46aa9519370",
            "ff2690bae19e4055a4f55cbdb94d08fd",
            "cd4021b08c7c44df88d46ea69fb1d1a3",
            "c0bfc99384314e80b30a318e495a66fa",
            "76c472d8ce1d4c4a9b8bd7c607537d7e",
            "812b759e4db84717a83fe05978c2a690",
            "7816960424064b4284efc101241bbaf7",
            "89b3aa212ebf4d948bdc829157a9c990",
            "430d7e9eaea04e00b4036652ddc78c57",
            "5d2ad7c8deba4e9bbb8de5c265c193f1",
            "91ff1dbf326b410293ed73b21a6e3267",
            "6655add6aa864c5b9a2c2d91127e8536",
            "fc34c5952393449ba35b3d01720f6f02",
            "2bcc705b435a499ba8ed697e9e524553",
            "4535748e1aba45d1afb33f1789b4eb10",
            "84952555f69a4eed9d2d155c7be177d7",
            "7e11351214ed4ed4bda87322e6ce1d78",
            "460ca98daee14cd1a9c06a8639280076",
            "b826bb2fd23045e9a0feeaf6f1170cd2",
            "f141629e809b4a419fa986bacdaf908c",
            "a58e29f840714612b16f020c462fc6f5",
            "aae29e04cb6945518f735c676c2b2c24",
            "972648b6d1d948a192a3d169460c66ad",
            "b628024ed59540a28a8eb6aaff5542f4",
            "d2d188a0c99a4d929613b0006105c15f",
            "e5893fc206f44f34a0d67fe5e2e9814d",
            "f72106747fb04ce9bb35b4078699358c",
            "5a6c01c2b385487aa697c0b6caa78546",
            "90b7ca509d67460dab5fc1e2a9d9608f",
            "d4ad167aa1b14c2497b14533f637d327",
            "255a8999f4f7439497aabad67b6556df",
            "0e206e77625641888d5db6b672e07e44",
            "d63d84f270f14e66800caf1b6057e750",
            "9fa87c1ed4ed4f3b92f3fc18285c6245",
            "0e32e66c5e064b62827be9ee9c4c9045",
            "d73b662f8dcd41a0b0ffa2c424d6200c",
            "df1395ce1a084abca5407ec23660334d",
            "b8360a9a438947a7b547aa5b73c1e81d",
            "53abe0264fdc4454a44f4009435153e0",
            "84722e22d4d54d269cc25a1577ab2239",
            "4957bcf0ed544cd692be745b13e46f56",
            "e0b270f89cfe47f18eb9bc62f522056b",
            "14c1c019523842f18af5efbaa9da184a",
            "a2f8133d2e394b85afdd56ebac9afd46",
            "27a281cf576a4871bae19c31074797ca",
            "737936f4388d4b70957503f61c052db6",
            "0d8ac9daad9b4a4a89cc50c7fdd5cee1",
            "d3db14d4a42f4f599f7c3fbad18e3c8d",
            "c413fa716a40447a85068244ee0ea393",
            "37f87d5ec3a44171a534cec3222970c8",
            "72782d9e165c46c893e1c112add6c544",
            "d7d4c3046dc1486cbd015037650808de",
            "ffc75755d02549589c578cd145b4a586",
            "9b09a823111d4b129f09bb67b76a34a1",
            "ed62c994cc0d4739a3309a31536d454e",
            "6c687b971fb74927ade1c8e0a71fb684",
            "a3ec2f3a1aa443c2884711044e41785e",
            "9606697f8be44b708de2ea50d863af19",
            "12d8f8868f014c5bb3c84354a925dfca",
            "feb261227611404d90333d0d741ce560",
            "4132802fa9f34e16bb8ebd37d1945bf0",
            "0c3914710df74effb48a5737a6eaf59d",
            "68752bcd8bb541be8b7984510b243bd6",
            "7e7e6f9c12634ba1b41ff1f96b2d5ed1",
            "de3ee2949cd84a759db4e5bbaf185d61",
            "c33f60356988454285fc643d82c12689",
            "f1bcf54d4931450c8c88f53ab1485be2",
            "d859fd7d0dee42ff8f32a5a19055f825",
            "6e004795f7f648bba23f827e3b11a1db",
            "89f9e8f0a2d541f2b9c179db7419bfe6",
            "1fcac7977dd54c318330f2793fb5c302",
            "93ea9f9d2a5741f7b03a82bc7f7b7c28",
            "c451a4d2e1784d9b95edb2fcce8b7337",
            "a622b64c3a5744acac122d8b66407f8a",
            "32479a760a8d4052b7ea3318bdf6f4c7",
            "086ae55496cc4cdca662ce6c6f953e55",
            "8cfee00a3a7c4bd986baa40fff870939",
            "92a5ab7b04f04e779f3630b58fb498d1",
            "c477a93c3bcb4127b7d4038878795927",
            "33979e9417694d2ea6cb4061836bf7db",
            "89c7fd5be85449fbb41222b8e588c779",
            "76a57657b6d64ed68f3a05f8935c4ea8",
            "db242f483be64fe3ab122a9b22ef1988",
            "052a633ef2d7471cb9814f6f7a576167",
            "5c166370de144fbaa29803098062e22c",
            "a18319cf958a41ea8c0ef8e3c608be56",
            "31139de530f849f48f280bb58afab62e",
            "8776cbfdf2214d0aa7f76c9a5cd1771a",
            "4a5f31405d0b462ea99b44c2b83e9b62",
            "a303a1c9c6964c56908a94c6c51c82a9",
            "1054a2c35dd74844a8d33c929b81adf6",
            "d80d44913ed5466289e2862d140ebcb1",
            "85584ef6125f400784fae318f9fecbed",
            "416b4f68d10845f9803e59297547a18e",
            "ade38dbb9c5e40ab8f4007759fdb41ee",
            "ad27128c03e246b1a139f17953c0f279",
            "0f7587534bd64d3cafd273f56c110f9f",
            "5efab0222e4d4d9d99ed4a1a21d1fc9d",
            "e1471ca9ff654fbcb839bcedbef87dd6",
            "80a7a7faa40c4fdca21810a9619ff93d",
            "d0d6a2f047e748baa127912cd17e4edb",
            "0a3a15c5c0364e0faf50b94109ce34db",
            "92fddcf4479f4c069dae6371ec52bcf0",
            "c3372aa18e194e9eaae94698c75f22f6",
            "0682e94134d940668fff63bfebd2cbb5",
            "97af096e52534c2ab0f0e2ac69d6b84a",
            "6c82d334377840cf8643db9f1ab0e67d",
            "471e0dac2264490f9e3f1062c0131afd",
            "d358bf43fbf84d5ea11f7f0dd4b95cbf",
            "834f29a5ac674c4392ed920dbf873451",
            "dcc4f916ac064840a4a6fe967b8c03e8",
            "87974adff9b64ab998293f4a8c4d877e",
            "1fc33f328cdf4931a065311b89203521",
            "dde9ef239362482a8d8f606befe6e044",
            "35bf4dc1727f4995b8ea30b665f4dc3d",
            "e4fe6e6dd1e449128d03a09dcc8a713b",
            "b6e16f4817d34e9b94c583b9735cc2a3",
            "c3abda545a17414793a21627a6599fde",
            "0521a7fc0375462a8a48bac89e38c403",
            "8b25020a7e874a06acbbef15bf7ef1e7",
            "fe3cda4edb22450e878d5a483f257e66",
            "6dd2678d61ce4480a2a718f1d57ff659",
            "aea006c8881144b1b17a1536d8533953",
            "79fff71db6f847aca3acc6312418c37d",
            "19a6d3e5c07546e6b5c37aa82daba26a",
            "4cbc6b57935a42b0a75d5ea684396ba2",
            "c4a68f3a9e0c4bbd8dda2933989450e4",
            "7051619eb01242948775fa6b12a06a6b",
            "e7d25c01d8dc4cbc86350afebc057532",
            "d9375c55d6c04735878970662a895a8e",
            "710e46fbe12b43baa613aa93dd0f5dcc",
            "a0d03051f6dd4bb9b701955483759aaa",
            "2fbce035d96d4b92a1b23cf2a4be0de8",
            "49a9175ca1324310939cab6a76510ace",
            "3c4a3ad90ea9447292a2b7cda591d671",
            "aae76837a418414eafaa79d07bf52e11",
            "d6eade4af7ee463bbde452a5d907d3ba",
            "7fc519627e7b409c898b9eef24debe73",
            "dd816ac730c64adebcbc3e23b6ae406f",
            "368d91f0b62c4b20bed3f5b5850647ed",
            "b126f2f5314441bb9016032a954d8297",
            "510dc2159e4546338c2e9f43d0bdb13c",
            "0667d4e7d5c044a9adcb44ab2fa93ece",
            "bf8acb622e484338b671952a26e9bfe1",
            "71075d0ea47947cfa76af999adbb496f",
            "9ed0a348a5e4409f9a5cd48270efee45",
            "3d83812c74094e0091e175ad08738718",
            "499802a182ef4238b7007b9324476140",
            "a088c998bdef404fb3feeb8285520f4c",
            "a6af12c325944bf490941bf3c668fff3",
            "99457c65e4c34e7e8872c2bf467a4657",
            "1d1dc31967cd4726b54b80ebfa2845d9",
            "97b64b9d6d3f48de954908def53b79a9",
            "3c4a79fe395249759813f44a8fa355fa",
            "6db4e34823f44a339b18358f179114c7",
            "10b6f4b912c14b52add6af1254ad9ac8",
            "d947774b3a0b46b3ab8a8d99e0e7d136",
            "8843dde571e24e539fcc1a53dd5f065d",
            "768bffa71b204c0da3f38578beef82a3",
            "4b8146e675ea4a2397e3a61f0bbae070",
            "954f2d6030e64839bc30645432895efb",
            "021e7f6d41824de2a467aff98296ba4d",
            "180b86ab8cbd434c814ce1367a60b379",
            "3639ea6dbeac4fffaf998d25d1f279c5",
            "abb9be9915cc4d268ff942c8394c9042",
            "048aab1d23f04ecfadc9b932af4b434f",
            "866d69dfa25848e6a07b005659703f54",
            "b56f93eca05044bda65405f4b015ff91",
            "a20d60f94b274c89a70df592b115ce87",
            "a16220c9b3b944b7b71f70cd1fac6aea",
            "a8747fb3ee944837a4acefcdf0e2351b",
            "47cb398465054a2a8b5b12fb045f26d0",
            "dba1fc29d5dd46a9a40cee151b5a264b",
            "38d339e31cb74dc189d79761bfe4f5f3",
            "ae8047b3df764761b34c158e3d2e5fd5",
            "880c01d3b9ba4d39869c96d9557a243e",
            "efe733d104f7471dbcb45127e3d0061b",
            "ffb9e2ed4d1a4598a7d744c36dbc9a33",
            "54cb148e182a4b9e9f896bc25d22d63b",
            "8eb356f81d7948a78c276c44c6f8250b",
            "327e427e506b4f5b8485e3271e7298c9",
            "8861b715204f489f819a2f149da4279d",
            "f059eb3346e344d680a5a7f1af8873ae",
            "4c97d9f660134a37bef17e59716c3f88",
            "41006eb5d5b64bef8e9b84bfd568ffb3",
            "f11d5af7a2ec4207a1ce992db4dc7ffd",
            "37dd05061cd04a04b46aed68ecfa61d0",
            "645c877f46984da9832764e179e177a6",
            "76ca3708e95f457088362c678ff3c244",
            "0b7b2268381947c38d6401c071d67085",
            "9012414b57074293bd43a815e8dc3f03",
            "c8de5c8e44e243b6a69e7e4bf5e9bccd",
            "48e5f4fba89849ae979bca9605236296",
            "27f2df7fbcc44ab4b0329f8a3875369e",
            "884f28ce0e1b42d2ad213b54ae82f585",
            "87d9885f469e4ea8887777be6d5ab43b",
            "a275cba965814d3cb73bedbf04b2350c",
            "00b094a476764f368c1d8fd1e468dd01",
            "eddee0c029e341da9bfce4851852e1cb",
            "2104151d971f4053aae23d2b6e5972c1",
            "e7d11d661d6a48a89a96df927bf649cc",
            "62bfca27023949c09fe8e0d1352e17c8",
            "57ebea0830e04d90aeadb6a3c3f605c7",
            "f8ed4e322bbe4c8894be9e777aa54849",
            "16e733fc53bf4688a61d91784f490a4c",
            "f633c78595f24c74a262b20f1c87c577",
            "3b08c148153a4e39a785637921ea717a",
            "f10c3278258c4118ad3cc7dd7ea414c2",
            "43ff14ec037e4bdeb601ed7c765aa552",
            "7ed890cdd2784efe962c781aa7271860",
            "b2f61c62b9c54fbba2c4592da4ff1f38",
            "527af08ac0ef40b598d8338cec1dcfac",
            "6211d6b642984cb0b85b07d728228c5c",
            "bb658407a7a84b498accd11f14dbd71e",
            "71ce732649b94c0e876fd5eef62a55e5",
            "bda05f6f6bdb4540b407e512e17fa2e6",
            "fbaf0dc7391f4f36a7fc981fd6a27ad1",
            "eaf2b11103774324bdc8fee2b1bc0dd7",
            "8bbbf16bdc8a4fbd951ec33104a9321b",
            "22d1ecd8070d4bb2b74b139a1532acf1",
            "eb9d71d7facd4ccbb3f08348037379ce",
            "519228177e82467c89039845d5a7bdc0",
            "35fa4b923ff940e395803bc91d5cf3c0",
            "67bc3b7461734e748132febe61206a7f",
            "864ac70d3a314e6fa53cd7bc6d0b8599",
            "1293d2f22b294ca8872ba47314cc64dc",
            "2250a639fe4944d3a6469992e640d740",
            "46a403d4c53648f99810d44217f5ed88",
            "15db6fa446db4b5a8e0d0ff488763b01",
            "f8872a083171421d91c63feff2d3e816",
            "45e2ed4d075349faafec42319eff9070",
            "54a47f14a9dc4f5b8f889dab9912e879",
            "65f050786fa14192989ebdd06d5263e7",
            "908f5cc810c94d40a7d05d962fc13885",
            "37017b7faf814145a19a8cd2ffce3435",
            "2456a28826b246fab33109a43ce4678c",
            "2d2e9f2e8c9b49f28869372d75eb3308",
            "c391997f408e4afca5245e77239cb1fa",
            "d652289ff5e5480ca8ef00771885c194",
            "5b41554f96264cd996dd5c979b210211",
            "40520cb9e2304f66bc193a73dc0f5368",
            "b2fed458088046ebae469529745c3fcb",
            "24084f5252cf4ac580a2ba6fccc690a1",
            "cef91d6808d04b73ae4182afd8841b0f",
            "c8a0005e29654a1a9d0602a0d8ab1088",
            "96bc525ba5d24d808d53c2fe614d9912",
            "d46d2850fb454391ae7afde1a9c55f79",
            "d6c1626813aa44e89f3fc9f363e93e13",
            "1c76e1b1e84d48a19b4126d5aa8107ab",
            "ba47b42d3559446c92412ffac7950fab",
            "3ec870550e9f433a92fd4c17da86d634",
            "a9ff9005e5b74dd9a18932652eaa304d",
            "38a032f8581b46c281e90d805255ae15",
            "1fce82e2f66e42b8bbd63aece0fe4dda",
            "670cea08414d4d8bbc4abea7ce020507",
            "c16726eb9e8d4c9e89b1bb503177f8b1",
            "d04809156be1493a86fa12523622f052",
            "eaff4a16025e4f7eb6a5cf19a112e430",
            "f9c8ae44773a41a7b52b7f1481dde9b7",
            "16a2ed851fc94205a678def212abab3b",
            "9baaf12e813848889fefd7cdfb23ef07",
            "2c9970d5035f409cbcc6334c5505a161",
            "1a6158d4adab4b02a710f9525442fb24",
            "fd73f637e6294453b7251367b1194b76",
            "81475b4331c249ffb6120fedd35fe069",
            "416e948b979e42ec93b15a2f5aa43e3e",
            "6266607d1a8b4e8d9ffaafe32d193ac5",
            "cfee881c5b7d479cb5b3c1ff6d13513f",
            "bd0cce1f1caa4720bebade394df980c8",
            "07addc41216c49a5a674d46b246ad732",
            "44fd0e125f9643cd8bf33c64f64e28cb",
            "0c61459953bc404ba7c2ba15e47d8e6e",
            "159218c6ac014453adc0cbc2408175bd",
            "6fc6609d5ae943cbab984e518ad854e6",
            "56d260c84aa04c7986ac0935bba5ca5e",
            "1657e551198447c696a5b112bb4623c3",
            "2f81bf810a9946258e6a9341e15b3a82",
            "d5a4fa0450114756902f77f146e30faf",
            "42de894694984d188203206656fefe44",
            "7e132e4d407f4faea1b277ad26b4020d",
            "26506f324df24c378388c119af0b925f",
            "270b482a34434ab18092538c4ad7c9b4",
            "6330944bf11a4321aa222891bffcaa63",
            "3c77e00aae3b44c1ab08b4d6bb7e3ab5",
            "dd2df7e1c0264030a76f2856b8879d46",
            "4857d9eeb6eb494e887ce731700dcca2",
            "bb0109521edf4c70ab664c6012c4bfe0",
            "f922a910973d4a46877534032fb60396",
            "03fca05fe63047ff9188ec3e23e7eab3",
            "44f8afe3b64f47f48bd07cfaa14af1ef",
            "d201c523099d4086b36479a07a2d2b7c",
            "daa283c957934990911ca986f0594226",
            "3bffa722fb484e5b898565d7c62f9926",
            "310c7813fcd243f0a030e0f7b6979387",
            "54ea040d2daa46838cc321e99d9d2b50",
            "feaae9892e3e44e78ce110e627e044b6",
            "8e653824109246b688463effe3512c40",
            "00f8400126444739a56a1041f7b5cbbb",
            "f5289558448f4533b313efb26845bd1f",
            "c7402a617d0c4b3b97fa5763f5a4cef2",
            "de98895f24604bff9c812fba4ee3927f",
            "9e9ca93de3e0435baa716fbb55ca622d",
            "172064f977974ed1b224d2eb4e96eb3e",
            "807045c0c69e40008079ac2c1d06d325",
            "5d3b87c525ac42218c15fffe0f1888fb",
            "a61a688c60f14828ad59bb090eb2f4d5",
            "267e541fb8dc4025b4e99e594dd1a354",
            "44553fafdd3146fe95896a1d0fd63619",
            "b7071829de8c4b04a8558dfe0fb328b4",
            "c093d48dccbe47f7825aab366dfd304f",
            "f88ec5904a214b1bbd53a6628dc7555c",
            "1194f120ec1f48d8862c823b11ed7e58",
            "1a7853d938bb4474bc94482e68d86734",
            "a47d8cee03974b669643299fade1c2c1",
            "21c363414a7248689a0387041d3c37a6",
            "2e5ada7fc27f43359f35e31cde201f0b",
            "fa99335065d340b0a69a26bd39efeef7",
            "73f16976c1bb480e95c3d203ca889d3c",
            "a2c53fdff0ac43979d0d7280c0860605",
            "b2c19458c532430bb1729a451258ae44",
            "f3c1e2a5af1f49a3be7c648e6fe1e946",
            "9b8e280118194b3c8e6a0754b23be036",
            "cc68537b9e7c449080b0360d8143a1ea",
            "9d799bc3f5764b50b1e9b1cce92ccbf5",
            "c718d11bef7349f89092362c89b98419",
            "5f6f76916c9244a38faa264ab6c57b9d",
            "60421e689e6847daa27d6e48f52c1ccf",
            "9da9328ec8ab4dd2aca38e5a7efeb4f3",
            "97a760fcccee40ffbb4aa0be0bfca049",
            "1ed580a0aa6342ef97aff2b863f3dc90",
            "2b73d2c91a064768b613755fef4fb257",
            "ae766284912f4a74ae22f4a9677e75f3",
            "30514dd6b8714d9c93f7401ba23e2980",
            "ad5f10c649a3496fa29574c3043250e7",
            "edc3e545f3ee471ba1d0a56f32b6a0fa",
            "08996847f4804ccc8b6e586d6fe84ca0",
            "b583db87bd2749b8ba0009ddecdc736a",
            "18ac077aa2e24010a30e600f5700c093",
            "e7887ff47426451d865336587b8d13f3",
            "daf85e76cf454c9f82a88fd006766ebd",
            "47b6a7f4ff6d4269a740f63b5dbacbe6",
            "0a6d4405ddfe484a9ebb15792bb1bfa8",
            "94975e5e1cbf4040aa97736bb13bd153",
            "c4388423967a44ff8fc38ae196c82155",
            "891df18700d14d8682fbc7a43dd34a8d",
            "86819562e566497ba8c509f1907b1b15",
            "b2b9f4f5ec7e449c904e460e8a09b75c",
            "4af5b87ed8c048738ff4136c4e958186",
            "950fe6161c8246bf9a4d0e4f5aac3196",
            "a1894fb2784b466a9a91628ff06b82ab",
            "899c373f109b4911a0499dc8fe04cecb",
            "9c30a64234c54b60b3ff9a57a7ef3024",
            "14d8c4f99dde415ebaf1880e5ec1f9d2",
            "9d04a596bfe642e3b7bf39694cbfcfb3",
            "ca51de9db62b4f7b87994ff0025f9bc7",
            "8387f16e517740289e77c493b9c02d23",
            "1fcb962cfe964426a943ef741de847d1",
            "bcf10f795e5c4e6c828374de67cc9d47",
            "9303a168e12c4a75af14bcdfa2f76cdc",
            "aa2b84d21d98474088f7ad7dbacef19f",
            "f5eeb6518d344d3c91fa5aebeb3bb342",
            "101d39863d204ddfb9917f361e8b568c",
            "7741efb7decb4e8e91a7b1ab2dd2c10f",
            "0d1f981e928c444aa2228b211d693df6",
            "2af7d1cccb5b40ccaa5c368121fce29e",
            "7422206fd6f54a2fbae40a7357bbb6bd",
            "d4c46847c4f843f583d75ed3cc752634",
            "2a8f24a6cae24b7f97cebd5840f29102",
            "decde48f3b69411aa1a6b18da06696e9",
            "2409cd5199da4e7f8b2675cfb305c093",
            "7ce4d920e5d141deb6c005caecbfd3b9",
            "7f2e4b7acdb448d490c0d16adde60b38",
            "6a54a3db8ab048a9bf670bac89bcc218",
            "fd6d220233c24cf0ac14346e15cd67d4",
            "896bc61420054b69bada3a6b29266864",
            "8f3cb09d46a344dca6b07422a2d01b0b",
            "c0cff6d3ce874fb7b2d9abb590018c52",
            "cfb9b93357bd4a4691ba870cda5699c4",
            "d760e7fa00df4858987d0e8a5880edbc",
            "cd1f24c286f04a508c9f7f308a49f70c",
            "d1b272c60b694496a3ce59ff376fcb39",
            "b8fe4b0b691742ac88df8d7778af1da4",
            "fea9b490470a4afd81f0a9da60de599c",
            "ac70bebd532b455fabf3f78c17403da3",
            "b9ddfba2f6b94c079b9d4288398df2d2",
            "25d24f4bd78e46fe80e0fc5700924c2b",
            "42c5e823c76340bab8ee04fde5018380",
            "b11ff2a30f424df69338d9591114482c",
            "c22d3472125c42a1861969111d91f338",
            "51b418edfb184f1cbfc50f8983936e5c",
            "8fd42dbed1b04e4ab794356726643613",
            "acbbce4a3aca4cfead6913522a9ed78b",
            "6f3a8d9f161643899d2ad81529e92dad",
            "7e85813e2faf49bdaab3c7edb61590a4",
            "d3e733b9a12e4aeb9f48683691a6e87b",
            "9bcf5d2c94ee49368736641ec19eded3",
            "1927d3deb7aa4da298ed68ebcef03a52",
            "6525547fccab4558be9d683f269d9a33",
            "e1d227692e7441e0984a7cd794dd705d",
            "04a690c704ae4c32b172a54236c7d9d3",
            "740448c069e44cf28a2169cef2be1bbd",
            "7420cf54f2ff4b2fac4bff1767fd2a03",
            "0abaca9194a548f3a6a4883e384d436d",
            "19e811204c2e420eaae8e5466f710974",
            "57a8a401070c432f82d186026bd613f5",
            "866517225a024e188b4682c7fb704793",
            "7c28444d05e5421e8fa3eefcc0b75796",
            "9f1ba86c422f4cf694c734a5de0bad4b",
            "ce46c2cbe9104706bce7c3081f2266f4",
            "23283373c50741d192a0148a65e0859b",
            "312eac9bfc5547748da0ceca68424a18",
            "5affc53d191f46f0ab966783ead8a746",
            "c81f5f52c90841bfbf223c4711409134",
            "779c3fca69fa4c69b5f2db1189ef6188",
            "9e45b4a87628451fbe8b92b97f464f2a",
            "c61620d640214769bb006ed6befa2c04",
            "20990ecaa83c459a989e55ae30acc415",
            "6a5ccf52515247998cc7eb74fa1c09ee",
            "01c47806900743c2bf316ceca79fb19b",
            "1b56128f233c4552b34c2bc0944aad7b",
            "294eefa2585c42c4b2bd7a5463360aa7",
            "244ee4a215e54444a45fc681750c11f2",
            "afe9a8f404f04c749a615df93324678d",
            "1d5a18309f6c4d29825b4bf1b4f4f7fa",
            "7c5c250529da4ce596e42fc63e55fe6d",
            "b03161e7a6784cec8cb4bed4d07acaeb",
            "31790fafc5654721997a0cc52e55f567",
            "536ef45e1052429fbd65d7cb02d67fe0",
            "d8dcf71ed3d34ab4919b7a0fdc2c9884",
            "5a45d7d51d4c4e32850bd2b249905ce8",
            "0536ca1b28f24757888e2191bc90e20b",
            "20bc60e3cc984f749d93c9f75a0eb0ca",
            "2f9f154d5a404ed787d1cde5bac3c8dc",
            "792e5ab9ad6c45a3a808f2748aadb66b",
            "09483e301ef24d6eac0162f4668713eb",
            "e3647627fb0049eea42a67d3f259db54",
            "eb580c8088154790a52f588357d771b4",
            "081509af7fc648c1a8710d29a32e8aff",
            "035dfb160b504fd4bc6893901e33275c",
            "76654fc0ddf44ce5870d64fb1e8844a3",
            "f1f3f0d57c174722ae4414d3de720627",
            "0367b38784b94236b3990ebef99a9e6f",
            "06586edadcbf4ddab0e39bda0428a905",
            "03bc1a5aacd342288964787034020245",
            "3eafa8a633584e12a4ae22745c5a1772",
            "965de721252e4373b95782d10a7a2ef5",
            "1deaa3d736694068a7f00d7eab15f9e2",
            "610c988be58c4589bd2a6194bc6d34a8",
            "ce622c83c80c43ccad4a927b1f9c301d",
            "2737a1901e2c4999b44fc79b8b76ec64",
            "8c95d72dbed94b2c89e9cf2512accf78",
            "bcc1ecfea2c7475190dbf31d547f9560",
            "795ff4d0e65b4baaaa41fa19e9e3579a",
            "c54d1b32bfc74a12acf7f0aa6fb087f4",
            "a2ccb8174613444c88baf27a198ef59f",
            "4d7479d2c8aa443daf36e50600c93b5f",
            "033380c58e0e460b99a86330248b95d7",
            "e299339a8d6d445f907185f0981485ce",
            "43f24184c01e4929ad48cdd411729a36",
            "b066d7d3dd7d4325a2ef99a09809c841",
            "b73cb64c55cf4afbb287303a8abb2e7b",
            "909c8d60c7084a8a9628fbf7554ceb6f",
            "2d6f3d2e037449c3bee8370598ec7ac0",
            "bb47d84a678f4f08b4fb7e6bcc635615",
            "0c1b6c054d80487eb87b6b9c1611eab8",
            "4a3ca7ddda1d46339312d4cdab762e43",
            "74ca231eca844f6c9d8a7554d621f716",
            "f21b48013f3d48f19414d751acf33a7a",
            "c4b2ae67a0c14c73a714e8f3178eafe5",
            "7eba5a7682f040f68f8b814f38318074",
            "cb01caca94784fbfaf2ffc2e859e0f64",
            "80196a3203954757aefc739b87ed5e9a",
            "190ea63672ca4473aef47d822d28d381",
            "7a663f46345644bba658ee8e72c3f5be",
            "80129c5514154820b80a9888f03c9bac",
            "2c82c940359349fdba3e2800ec3d2e06",
            "3509892b1a854b9491929079dd41be5e",
            "27c1a1d0e4024d6e82c4de4576faa1f2",
            "6fca08028290420cbc5c4a60e8803de6",
            "1fa2329c6f234146a6d5138f96cd683d",
            "2b25dd462a574c81a3a167936ed98587",
            "3901a7fdc4de47efa85dd1ca4dcd83b0",
            "dc46071185524a6b8fce1ba78b5299c1",
            "f3d19a2ee69d421898247a6b3e3707a0",
            "d36e8bb8877b4511945fc88c46f971fa",
            "4e7415bde9404fb099053bea67c57217",
            "fc1f0471771741bbb1d95b331b84336e",
            "23cae34940bf4a73813efcd6e4537319",
            "978459f168794af697abfc11dddeaf20",
            "ef02e747df7549c39e5013d2cda617d3",
            "88a2e097aed14a4fa7a7ef7a0f159fe6",
            "0f9fd09197fc4a8490a358cde99af013",
            "138fae734feb46fbbbe6133ed35e6271",
            "cac48395e1944326b8088c0669acb19b",
            "3301b7530cfa453ea3609bf5bf604fb4",
            "2af677966a604e1b9955fef7f3a929a4",
            "9da3b8019ee4487f806a5b479c042eac",
            "1c923211306046c9b291cc6fc5ed5a9f",
            "79e03b2752fc4bc684ba0b55697b1226",
            "78391dc026134e239a24928a85c900b5",
            "58b79131da644d45bb22698321cb3b2d",
            "fc0a8dcd32894e70b51d97e69e2d08bb",
            "b22c97bd7c12460b8d4839cbc212d9b4",
            "3210a1a0db30483da611d844f6c8dda9",
            "32ab5fdccd16410f8851eea242adc1f3",
            "219aa4a8ae884e0194c97e65dec8e7d8",
            "f861714eaf294970a6d13ef46c691bce",
            "701f3653343e47039460fc412460d046",
            "5fffd61b6a724a479ce900cb254c8b5b",
            "e5448393d47b435ab65667bcc4aa1775",
            "9961a09ee361491dab2fc8ab8c3a877e",
            "c3dcc2b7152845f7aaa7410502966cef",
            "ff1f54b35d6248708de885c571908a6b",
            "79341fe4ffdc4c62b2c51a5eab6d8e52",
            "48d634fb4327410f8fe4fc7770af2e00",
            "97cf8cd7904b4f19a6e5f287b2100a60",
            "face4ae2c67f431db8660a665798bbd0",
            "1d2aa3b3de5a48039089018d9231fc67",
            "f3f4c1ede5ef42b394c9bcf706ad532e",
            "a8df6f1d21d64ba3b45ee9971314c03e",
            "9f94b0c07a5b43b3952161aadb0fd86e",
            "ef14a03b58664535999f0b2259392eee",
            "f3870e3611ec419290a343630cc82a10",
            "97b3ee6bcb9c4373b2998142452b4aa5",
            "029e20054f5a487587a7644802cef581",
            "522f9b35523a46958f017742959f2798",
            "d5030b4db0454cefb5653afeb9a40519",
            "c35de6ec4e68463d87f67ae163e81add",
            "bfd10abdd71a4350929def4b714e341c",
            "7c55a8baa27d44309940a5f3eef50d44",
            "023c8dd06c384575939c15dea141add0",
            "02ab69f6cc74444085d008881786d6e5",
            "a35310ebd2454b0298ed5e533859370d",
            "022ed71a6706404686188b558418da18",
            "bd0fad4fb997437d847afb753735eb2d",
            "31f384efae4f43e9b00350dc754e54c0",
            "e64062e71c304d1fa233c8fa6149b062",
            "9072cbb10fc84bd4a23dda3e8da41025",
            "4410109205b248ce918c0c644b317b5a",
            "b65a920a081e4b348aaa2f5e5632bc94",
            "dd5fa362731c462fb6b1da4d4be0a5f0",
            "53c9c57a8e8a43a18ac87e5bfb2d4b01",
            "752eee7e4bed473fb121a7dc66445eff",
            "d7563e505f9b4f03bc414579412eb209",
            "6445ab20e38d4fa6a3525181fce9da6a",
            "d6b473c9470b44fda13e418d64c7b34e",
            "0e19f10d995445daa868300aa8896047",
            "c8c3b6b169cb49599aeff7843d1d631c",
            "61b72a0dcdb04c4785d408c7645b04ea",
            "f78dc94a33464f39a6c71385c27894df",
            "1db60f144612425aa82282acde442346",
            "c6b58697dfe147b898fb3516ca78c4a6",
            "823bdf2bdfab4f0a8de5565f2bafa2f1",
            "38c094bcd21149168bac8a46d847153d",
            "8ef630eb7d9b42e78b9a3fab83fe8bbd",
            "ee3aebe185b84259bc06b37223914f91",
            "ffeba2c083714f61a13015688532f855",
            "0ceb190599ae4deea8e38f0b1d8debed",
            "4b438a4c94704cbf961e7b4591be1709",
            "d929ea5b2a584ac8b9b0dda58f9169ac",
            "c2a3d797184747a9a300e0d323fc4c67",
            "7010824853d743aebfd011e32d268ec6",
            "a6ed78306ddf4fd7be6e7234d5199486",
            "fa6a694f020445458537a7371aa2dbaa",
            "8b2c3334e414450f911ff90c2c8b86a6",
            "9b6e8929f9314595bb63c582a685cf74",
            "63a5bb3d8e594d399d12bd8b0953e3d7",
            "fc78bd47cc2448c0ab50bb8d2365f333",
            "7fe1ab9a6da149edb89a405ca14a6984",
            "a64ade8ca9434a40be1b16450a95e9f4",
            "06fea6f544164d32ab0db0769e4cf688",
            "fe50e1f4136a46819aa8fc33f81a77e8",
            "351bc380f357471c954cbc23940bf6cb",
            "5bd6ffdc82624fab90a55fb1c9063e5a",
            "910fbeb6eb2b4059830ef088e204eec3",
            "bf8153d3920949f88e4fc2ea84f41bdb",
            "4c1baee5afb044e3905c8b5551b63c63",
            "542746e698d74cdc9208fdb0aefc211b",
            "a24c341580ef484ca21ea46a7770e320",
            "04f5e09fd3464716b53f2f73dd2a1b1b",
            "22848a4cdfcd4df08ad841b53bdf72ff",
            "3138f117c45d4b48984ac66bcdc89dba",
            "48f7f550e9fe44f3b6d96b0d6b884102",
            "fec7bdb7391d40b7a79a0b9dcca18392",
            "a52769456aad42149520a3bcc7084a05",
            "d32078095b2b4260aa17a9f2dc0baf39",
            "93560325512a48f388c325f021ac491a",
            "8be7f2ce3ad743bd84d21fca00f86983",
            "c5e0bdba4b90456bacea45eefc9ff84f",
            "fc8315605882411c93334a78ce666476",
            "7c54b4b33302448992b2aa07ab87f07f",
            "1102ae5b840e4799950fefcec4448b50",
            "d4062c3ace054b90956b928458a56a22",
            "eb7399b8dfb54147981084c7630e8000",
            "93809b79e3b14dc28eea0770aef225e7",
            "d4a6846767e148a5adaa34039a359002",
            "2fca786dc9b04010914ea4479fa56ea3",
            "796325fb6b2d45349f782b0ed376769a",
            "83ca18c32b6841a3adda3f6114076ae0",
            "e12d50d827ca49268898aac634a0b857",
            "f22d2cf6e07e475a9b29b7537bed13e5",
            "1ecb9ee53747465d86dc847233081f82",
            "fec7fb1c3c93413aaf47980b796a4097",
            "ac5d0ffab6bc49ea96c9cb00fe520526",
            "1b4923eb3fc0475d968fb7d9f887f2a3",
            "b2cddf73dfd64b6c989f87fea2e1345c",
            "049ad91afdd24a2f9c3dae43285b703c",
            "107355c934044ae48f3a9fe19273a4c8",
            "d4deb4adc09a4a218d5fe79fb87bc638",
            "572ae695ba464cf6a81b6e472ea72c2e",
            "517870ca89924900a2de02b4088dc24b",
            "05c982674ca440279170a9f025e49845",
            "99d8e8b3844c4b4a8cfd3cf4245a195e",
            "f7cb5227151b4989b305c52ab83ba180",
            "c8642e0ed54e49a387c5b7ce19d9f153",
            "3cc30c0fa6514636a2f8101c44c28f23",
            "c314191b801945a6b66437cf3c0b5e29",
            "ade6fa15268f4307aea7e7589eca2310",
            "5675396d94cb47f6aba6422ec23e19e4",
            "dccb4819689e41aea0b5e5c1ee8f6a51",
            "a511c2d507db461dab8a694f84a023e6",
            "2b5dc6347f584e9d996e0273640c07ad",
            "f5118dc6b1f341adabe321ef33b06660",
            "ab8ca1c2b22a472f86b44bc790b3bb60",
            "ddaeb564eeca4016ba94ea2a9eb19caf",
            "ed1e300732494b83b2c403d7b811c646",
            "fe4b2a5dc29a422cb19e217f77850315",
            "a90fd8224c9e47799a5689512bd2a809",
            "371e9090390c47dca2d6cc4269eea1cd",
            "453a60772a25453bbfcff1e7c868152f",
            "32612027ab4c43e58c8645fa6dce0ec3",
            "296b4c35aedf44d48b81f85f6a48419c",
            "3a106f9d9adf4d59aa2ed40174bdb2cb",
            "3c4fc290c73349d88aa2a1d0ad1ce101",
            "83fbbded0209422b9dc56bedfbe58e44",
            "9bc7f498786743fab3dd086ca338af96",
            "4f2b7b0f8447489caca8000f6614c037",
            "ca11a1e340a54df6b0f1cd9e9fb535cc",
            "7b2c18e8845f406fb19553da91a96197",
            "60ae8697dfbe4e0abc18d66de848c3cc",
            "4b4f812a9aa345f39f1c4614be54e0dd",
            "4ad87e51ace54d96bc010bac07b5a929",
            "9e41308d38204a49ba7c795c046b3c2f",
            "53c35d4cadce407bb36d48ca2d1fdb7a",
            "529b93aeefd7493282db3883ea56dfd4",
            "9107b3a2acfd4b4890e4665d325c0272",
            "11a1ce2a139c4652ac6983931cfc7a4b",
            "28ded807839149da913389df2a68d0ea",
            "22ad89f37f57439683409f27947dbe39",
            "b432fdf7dc2148d49944e5c8ef05d17b",
            "3e3e98c688354b64af99e553d18561fb",
            "de63d34d39c04b3bb2922213f7cb3281",
            "782f5f5a013149a4a4b440aeeea92c64",
            "bfedc51539b741969980c4f541200ab7",
            "33278ea785fb43ddbce6cb0310470b79",
            "33b9c8a813e945a29a65422c5ac469d3",
            "4553bb4331e743849549ca80d363ddb7",
            "4fd0ac79799248df8f8403caed6203d9",
            "014e8491a2384032bf4d3bb50e7888d8",
            "13053c881b154855ac884d4f752b7472",
            "32d65c479efa46bf80521243632673f8",
            "e37f5da78d8044fdac907c378297cbca",
            "6fb3db72153a4769b2722d36677c6e0c",
            "f895b6c255c44649a51a088e421f0f23",
            "2538c0eb23cb459092d223231e340e25",
            "571b469a5f194b1698a322ba7c37c60c",
            "69b85d12445046e0adbee902bbf5086a",
            "59b2213e5edf439cb3ba0475afced1c7",
            "0471d896b8c94ea5893ddc0d39fef4f4",
            "8390b00dbb4b4470b6fd80f14aceb5d4",
            "7ff33666afab4337b3a0cd7ac32dbe92",
            "415e48c053b74dc4b10cdda365cda273",
            "72b8969b314944a789fd3c0c0c13ba74",
            "387bc3940ba049aca93c0bafc78a0993",
            "8ffd4da741a246f98b2222d432ae5769",
            "54408e7749ee45908f835181c9e43b09",
            "f53c6b30b0554741a9c0b499c9d66d02",
            "5ab688753d51474caf2bc2e3c210ef37",
            "5b24385e18f24ffea6dfe4dc1b537b11",
            "9958c5f4e7864bf79076ea46882fad4a",
            "9f5d8cb3a8c64c178f244da0d07994bf",
            "3bee5bb02b744586a4123a6c67eaccbf",
            "7a77556a56fc43ea978c0655d7bc5937",
            "13da9a1302fe40aaa71c2ba2fb33848b",
            "319390a957cf469f9ddd19407fe54a37",
            "e864e151bd2f445f82e9739ece98d939",
            "b8fe10da14c44dbf8b549d7dc5a626b2",
            "d46c9918b0564b399cbe1562b1975419",
            "3fb58f4573624fc1a5b7df0fe85da555",
            "42eb41a82de944eb989cedfe35bb1cbd",
            "193ab9629de547ae82672ece2a54fe7c",
            "04d28af924ac4884a7463e4962b0294e",
            "fd7a70a01e58444cb34105328d78ecbe",
            "5ef51dd91fe641fd8c009049a7c627d1",
            "996972e6f085493ebc343e3906658e11",
            "76eb339ef31c4701984f04506b9f8395",
            "4e088f63fd2a4746a59832f3d249a0b2",
            "0c1184f2edd84fdaa640e776c7f0ed38",
            "6e12530bc6c8460db359f06db5596d81",
            "4a2a14b8e03f4fbb9ecbd89c5dec8878",
            "75c7ac76706348c4ada081f52bc6e8cb",
            "094085e5c5a64ac9b892d14c8bd7226f",
            "df7d53a48c1649cbbffd3f7f317452d6",
            "b949173834b34511b666074b4c9bc5fa",
            "6575713e96a34cffb9ebd7ccff40bfea",
            "3da65caa052544a1a46959ed5533bd4f",
            "4da5c70587a642f1ae8a572b9c08dd28",
            "64cb7ef670ca43d78ff436d12422a23b",
            "37478f7e184240689574c7a2d9bfae06",
            "5fa1b6e55df047d2a978289817679fe1",
            "03807d6579f94b0898e2b75392f76527",
            "d2766fe39f9b463596cef7363649fe45",
            "fab3b68a27514a12b421c5ec06aa3ae1",
            "e1802f4e3f39484ab5cca9f1e5f2277f",
            "7cd5dc277fa0416c993b457128c1d1ab",
            "4d50f79637154a4c852ce6a3536b828b",
            "c622c69855e34441a1aac5f4d45cb5d4",
            "4178241e1eb0471781ea4334c4be496c",
            "37bac7a219e84fe5ae05c61c86c9e8a4",
            "7b26f7daa65041efad9de1190773a50e",
            "0697d775b032463295d8ecfca89ee56b",
            "786e9e01e4f44a2986145d1d395f0cd2",
            "137be368d5e64fb5878ff3df1ecfaab3",
            "e88cda067f394841932909668c2bf19a",
            "bbaba6508e814700a3235db75d0fb659",
            "06b96f7cae3f4de9af6848c052423361",
            "c2aec7ddbe9d460ea9b8f7a07e390495",
            "43e263995f1b4e7c8139411875f78919",
            "9a49d3659c604cfd945966c9710a849c",
            "005976ff419b40488628e0204e1685b2",
            "3c4ae4e46d5c44c8bd080c0d43c6c9ee",
            "448bef07045a4f5dbe8a172aa2b259c5",
            "38250bf99cdf46fd92112fd531a87a20",
            "0e71c1af832243bea0241a0b34277ddc",
            "132d661736b5443a9e1ca9f0d381ec19",
            "f4aa3ab4d1fe46b98a86efaeac0f70db",
            "6307eaec5fe94ed6983e2d261c43b314",
            "0cfacb281d854086925e0beb8a225416",
            "ec6e4bfc37d34318bfa2e983cb5cb371",
            "87b1a24f7e74489eb4a795b8150e332c",
            "b3f5c0cb2abc49ec94da2982dd36f822",
            "de572e73bd844cd1aee429b5af926855",
            "84e6df3dc5884091b9a071f574ba7cbc",
            "ab6c2539d3554948adc8638b938c39e8",
            "bb6af8b96b044dbf9bb991d673d86ebb",
            "8b2dda56cfd24ff3ad331bde8f471014",
            "c0702e02299440d7b058beed389abe38",
            "ff1ae294216443a38eebc6b2740e2f60",
            "a92b22ed75de4b9989665dab52e9bb5a",
            "866ecf493cd042f6816f1e651af80139",
            "dca1e9b173dd47248adce4805371047b",
            "91b16786d0ce4cafa64404394a8f7c4f",
            "05eac7dfd8a649edab2b488c14625d90",
            "f7119cd7076c482fab68ff666082510f",
            "58fe2c78f7b440e8abe3c868af1b4100",
            "97b7c67f553c43a0af24ea5e10f6c121",
            "7a1ff276918f403c845110b71f766498",
            "17facbf5f2b048aab98a401487ffe65d",
            "dc0103a9573b4093af533f38157bc8a2",
            "e3623c121f6b42068286643180cebe31",
            "a71e7371bf5d47a89677c605d0540dd0",
            "d59305efe0104ea4bf329a57bf3585cc",
            "68261ec771194e55a427fcb3ed47e446",
            "ff013151a0fe4265ba5f44cf122a1468",
            "d7f37f26ea4d4d6e8eb1bb817ecc7eca",
            "773cf9c30cc64ac1adda59123d8075b5",
            "dff0566a346043b5a3d724b4c6dfd736",
            "4cd606dd8ff74d069657f30a4f86c05b",
            "632e9213ba604151a8106f72a6b7560f",
            "5c0c742cdb384395966b88ecfeb403dc",
            "4a6105374de04444abe6850723bcc2f8",
            "776b01e2e5834037a7e26581fe1f894c",
            "e31528246dee4c979faffdf8e7ad31f6",
            "0cb68986dfa342378ef4bd629c3d1105",
            "8214b6e8b9ed420ca14e04b13678eafe",
            "d3d676bb24894fcab258b5cd73af8c0d",
            "63eb6ac8ae7e4c50b6be449108e4a2dc",
            "6339319e1d194e9786ad6bccaf362f22",
            "cc9addae347947888080068db816da4b",
            "4994f84bc0e64eb39904d22405a0e7e6",
            "e380195dcb1245b3842c0c068685c665",
            "88ef3730ed2645abb651447375f44d93",
            "996f0773574c4b8a8b14b3535e4a429c",
            "7d1f3b9d63e6400ab0a7589cb1e6078b",
            "c1bd0bbcac1440599b1f936a2203374b",
            "8cb9bf08a03b4ddb8f93f5fc59160dac",
            "347358942bf343cab90d1be24e793898",
            "48a095c5b26c4e7790b9602c9fd835ee",
            "df6827fb830b4a07b35e0d44e78f0395",
            "702ed1eebfc240e7b172ab136c35d429",
            "db86a637b735415fb18ce12b4b3a5d6f",
            "ff9072d8a9e54579ada0368cf89d9d91",
            "fb05c35e096b46a1b95c26a8b09dbbf4",
            "2a5f2004a42c4e8294c4483f9ec46967",
            "774005f58c7245eea0c17be3f12f800a",
            "1125f263924e4b4c8fcd1a1b3a74b12b",
            "70a485ae808c492986e3b70e811ca48b",
            "5757a041523b4171a15abd232b84ec03",
            "ca798c76ab554a00a0c05e6685be43cb",
            "72ff04c858d04c9f9b9f1898a4f6b8e2",
            "758e1225b9d54cd28ac062c123116232",
            "884df306ea5f45dba8f71134b3438ec1",
            "b0a9effab36b44d4b55894e9d1237cc4",
            "4a3ba616fd994c09955dcab971e1055e",
            "aa017bd1ff87413fb057ed452a9f9fd1",
            "18094ab9c62841b78829318b0e162239",
            "4a341d1bd3214b4cb542ea2e4d8db415",
            "666e4f3a5c6a4209827bf348248ac3cd",
            "d04dab1550094ea989e926592bba62df",
            "33e1f798b11340a083eb9d6c74c3cb88",
            "2a51bf38a2b6427ca8d2ca4b027664fa",
            "a7f667b888b941b299a7ae0a257875b7",
            "9c16e3601c874b53b79638b558ef69f6",
            "996b43da89c44fb19eb874dfe0565e78",
            "791dc9478340487cbb3038c56aba6c06",
            "1f6f2abe80024a55a2390c8f676c4633",
            "237a03474a63400dac7922039d5eb505",
            "3958c84e07b2428db1d2847ce7e01607",
            "459dfffc39504ac2928a61b4775e7100",
            "386a69b30982445493ddced3ac210fb9",
            "dd96dbc6d5c049f69c766e6b2643e720",
            "62a514308aa24f1b8f4eeb31ecc0e78f",
            "b6744b1975024ea6ab46c58bd70215c7",
            "9501bf9807de484eaca5d0d6094e8b9c",
            "17b8ee35014b4723805c86c419c2d7f2",
            "21687432514d415d974ea9519254b940",
            "0ccd0db05df34bb78dd1af9eba84fae1",
            "0b452b1048ce43cb97ae24f6398c30c2",
            "b074075295b94fc0a96c6844eda97ce0",
            "0735c144a1294353ab4ea7a476358684",
            "870360620bf346848db88a71d26d9fd5",
            "3b8578a78fd842d3ab318944baa7d4bd",
            "870311ba2d364edca9efffc1c5952c1f",
            "ef8abc51ed514991a9e8519d26e394a7",
            "8e9a36d44cf64dbc8ec256bcb8532145",
            "459f5d6f9bbf401f8ded3d80fc5d8742",
            "7324809defd84717ae086c8686d6b872",
            "4fbc9d04cb0a406087b5a4ce22bb0456",
            "7a880099bdeb4d3e8d042d6e8f2ae621",
            "7ed137029b9541a0b302c4474ca428d5",
            "6ea4ca9efcdb4cb58636957c9f0ff466",
            "279bad6ec3f74dc0b0a961884ece547e",
            "1efbb4267dd641e0ac3945f55a200fcd",
            "d24dced0aa3b4728b649a8b1c920e1fa",
            "262208529ed64cdaa0c64e7d447b67ac",
            "5aa53765d9984ad2a8c8556f4c9a4962",
            "5621f909807f4cc086392d7f96b7668d",
            "49d6ac2bae1e47cba8d3085afb6478e5",
            "f4aec3d0f346461e9675ff121768b9e0",
            "df027fbc431842ddbcdd831213e81a40",
            "43fc62015a964f9899da35448d954002",
            "81156d95a1844548b336876d81ed0e51",
            "fe0c78ae252d4a78a6adec33d919fa1d",
            "6f86a655085f4b46ba83732e09380efc",
            "cae664db1c0145368c023f6b4cf78633",
            "7c9c663888c946f69d15db86d299114e",
            "f15e1b9c6ffa48c88ac255fe0067690c",
            "0d384b05cbe748249967031fa5c81ee3",
            "b82751ade5124ed2ac2d4552935e1183",
            "48f32d21a12849aba5451374dc005553",
            "3a0f0b8e52fd4696a336daa31d6f6cb3",
            "9503fb0f78c748bfb2f5980217dd6e1d",
            "459f969b398f40e6a9f9a46a15f28e4d",
            "9f88b609b8c84349839a813dcd16abe3",
            "68deab6f8c264cbebd629e843caf677a",
            "461e85f2ebfd45ada9ff17b4f3addec3",
            "843bee75f4024bc987700b6c902496b7",
            "a7e5406dac8346e78790c8d774ccb600",
            "20ed1eb2fe7d46f2ba2632eacaab16be",
            "193dc8ab655d49c0a81d9e5a3927637e",
            "8b4f392780d743f4b4dcf5c243afbdca",
            "2499e4a92d7041e7bec3908308dbdd56",
            "b6525457cf35453ab2b0c38c42b17b44",
            "36d2e108aa23478cb88ceaf2a1620464",
            "c61db4b218f348d0acdf1a464624c1d5",
            "a3b4c9da4ffd426aa4eefc7fcd0b6e48",
            "a4599cafa0ad487b9adb748f2163813c",
            "946ac90b8cd547bfaafa9508d6a106a1",
            "061b87f76d3943dfa1ae9ac89c587487",
            "b26aaaf3841e4b7aac9e396b448816bc",
            "7cdf3e646c054d6688a36215d96da2f2",
            "96252fc700eb4cf7be8c98ebc145273b",
            "df16b2055860489bbafb7476090bbcd7",
            "241bd82964f54fe2b7e61dee992c7618",
            "bae53af7cd1c4470929ae70fabb272e4",
            "4de5a1437d1e493c9becec3a2128b6e6",
            "7cbe88aaa2c84283b89762a208d3f3a4",
            "827a2d3710c3468ca26030de64f85028",
            "495c11c546b84284a6b4eb51f75dc16c",
            "defa0b699bd24be2ba5b9098825e1358",
            "3c8f437481c7417ba1ee240e1d6b8696",
            "bd787ea01a2e4d9885d25d0a00fa553f",
            "c1616d436e6d41dba8b6f82d98b4ad44",
            "6d6c8796b9434be899daf0c23e25b8a0",
            "0afd443c53374cb4a193cc44857eabd4",
            "7f19f385e090410683aa04e5f89c9efa",
            "9b2458b034734a8cbbf3cbffbe3daab1",
            "a7d89efb73724e9883dcd82fc4809f21",
            "11805a7346194f6c874813849bf60203",
            "b188a93654ba4bf5acb999c84412bca0",
            "a2a7d09edd9744b2850f220f43d24783",
            "d4a020729d5f49fdba5c4ded49df9436",
            "1222c3e4cb164e0bb597db530700c417",
            "e10e9a0a8d3d4ab489bf5943d4a1b0b5",
            "2b2f0835132a4ba99b2948a6e8f992f3",
            "140abcd062fd4ec0a8c1db2516da9496",
            "443e2ad55fe94f48a81ecbc190b913b4",
            "daf9494f5253472a981432922a2d6625",
            "15ca50ac0fe64087b3b5e6668334526f",
            "634ca898858f4cb3bb0395afae3257e4",
            "2e83a14377f84334ae9f7518327dd65d",
            "d0ed857680fa4b6fa67da2c25c12de68",
            "d7a0a8c16761427f82101e32089db1f8",
            "e427e3bb4f884e08a2cf9c1d4633a43f",
            "f3e8f728eb6e49b1877445fe4a133299",
            "2e4a789537b24dfdafcef5b736f9e087",
            "977f51feafd44a78bec6751c4fb38558",
            "8755622d7843410cbe1d6f7d702c1664",
            "92a1afda179449239ca150a5304074e5",
            "32a994d92af84291b436022e69306d85",
            "9ba0aa89ce0e4e89a8c702641ad4adda",
            "b579b203093644beb966c2e25f39dc0e",
            "7be9ddc5fbd54dc8b405fd00e9574b2b",
            "0fc80609c43b4e2d8b2ac8183583b452",
            "285d5f28dc0f424784a8b48a1485b308",
            "baa2fa1b147e42d3a275fbb3451bdea6",
            "621891e2c0f8446dad9e8f065c5d72bc",
            "0dcc9879145c4aaca63e2d991af3631b",
            "be3313273af64f8cb7a414997a749e0f",
            "02e91d55f22445c68297b549407ed7f5",
            "3bb434ab774d4e5d9cdb51ca439f0563",
            "579e152dca054f4497a1d405d8001e7a",
            "7398a77bcfa6436986966c08b89003f8",
            "2950141d888f4cdb8724e1ad11af6f0f",
            "fa0ec41cf49a4c6ab7d0662b6d02503f",
            "9dc8976955d44ba786faa547c5293044",
            "a19cc67a038a458ea1f91d36fb6ea5e4",
            "a48bf93dd89540dd8fc23526086d5777",
            "417e7422c8f0494fbacc507893e34df5",
            "45852a780b4b4069a823338d60f29205",
            "af6898a78c114f09849b0b072d6ab54c",
            "0a01fd4532f640b5b4ee86a10262854b",
            "b6e335a5298144db8eb1c5aa655b427d",
            "b3e544b5d2fa4eaeb1f977cc728fbb6d",
            "445e8186be254271bf256773ac58efe9",
            "29cc1ea675104926b281ecf048484bc0",
            "0311470979a8470db52d07837590302d",
            "9c3196bf4fbb46cf83a35b33b01526ee",
            "769c9e4711a54e4c8a1889fd951800bc",
            "ba743392c0be4bdbbc92532eccaa261f",
            "384fb16b84234dfbb50db7eeadbf4794",
            "e9c6b915d9174aeda6e1b0d09cf1ed6e",
            "96d2c5d59e5346338f1deaaae4ed9868",
            "1bd0d1c59313410190e807cf4a12ddb4",
            "7607d60f1bf24b138fd4b3fb918ba658",
            "f8c6db8334e346929d058952119cb371",
            "72e63ee637d9494a8864df691907d2c8",
            "d1889f9c273349f5958912dfaa912276",
            "386e2ac01c3646f0a4eeb016b2045af3",
            "b8f246d002a0416eba73e3a9f99652c2",
            "5d49787420574c4881244b88a5dcecad",
            "ff834dd4d7cb40b89a75d1bc6489b1ed",
            "75744efbf0d3423ba47890d80ea78520",
            "b4092a9751fe4e67a0a5914ec979120b",
            "a98c96f4e79a46caba7b72c314d20c77",
            "a6f72bae12654c309d97613a2f4c2d26",
            "719bc3ddc8be449da4c28e63f481f601",
            "4b772bdda7a04712a544b3bbcc856893",
            "9b24d8e0db5d4180bc3c0640db939c88",
            "f36e48a02349440e9b065472d6a62728",
            "49888c7f5ac144e499c2dc6c13bddb26",
            "d863fc219f8c4cc8815a279a36be56cd",
            "72e970c61ed844d6bce581177c73afdd",
            "476e5d16cc6f455da8f4200b42d637a1",
            "9694dfd360b74a8eac9505e737fd5118",
            "811783af047f4fc0add3e11c4383fb4b",
            "7ab3363f5de54eeb8512c0277dfbd2be",
            "0b2da26e35e94882b4ebb39a4df32f12",
            "3728d9b69f3049978d86580854d99fcd",
            "80256993686b48248f0fd13e5a661312",
            "9e78c45ef7644f3f9987a9c4ae6f6abb",
            "65a7fe15493b4dbabf05e3287696645e",
            "6d843d8c151e45cfa404765c94a4dc36",
            "04f40e052c394ac6b0fe7e3e3fa74d8e",
            "f244f61d1ecf4425b695707305e2a6f2",
            "4330794112a24dfe82ae5c3a927cf401",
            "29480ca8f6e54f8c97f74e5307f9e368",
            "2fea9f50f1de4d87be7f96efb50425a8",
            "496e5d543bf54860aa9c411290ffe33a",
            "67a8b538378f45c586fddaea82bc53b8",
            "6f61733af1094a0aac4183521a6f8c2f",
            "bd0b835b61924542ba1e2d5567bcc70a",
            "82d31541082b4002b26829e3976ed94c",
            "0abdd5797b0e4b548e734c9f87298040",
            "d4548096294242699ff22b897c23c9f9",
            "7687f42f3c904ba3870a1bbfa7a8c6c6",
            "612709a8fc5647d1a62b14cf9c516490",
            "e9ba68e653af400dbe4f02d8472c124f",
            "e06bc3fae7bb447d99e433e530a8c522",
            "0c34c76d83034788875bf96371b462e4",
            "676c2dba0131478ba4d0b5df7e13b83a",
            "9f97528b2b2f44798304c0e23d57acd9",
            "83b37ce6a5fa4d7c9a65062b342b50dc",
            "d138f58349554a90ac74907c3f431e53",
            "89993a777f5f4515965440fadcddb1ec",
            "d0815de7efa3441aa04f6b18e83f46b3",
            "9d8095da3e8e4d7eb5c8e64e37ad9e32",
            "b94df83d8f2f4c7c8b4cf2c2252b2214",
            "541f980cc544477d828d95c7c935f47d",
            "fbef726483a740de98632bc3d5b1d1f7",
            "ca6448fbcde6404b80f8494694892234",
            "ae1deddf1253450e857b6fca3e8333e0",
            "ad76d997254d42e898c51e3c3bd3a2d8",
            "7e254ac16156475da3b05bb62882d37d",
            "56e93fb966264aa8a3bd36767e5b1714",
            "55d2838ff7354258b9e712b7e4463552",
            "1432114b9c604708a78d0ac355a5951d",
            "afccfac95a3048a08dc4505c0af0c41c",
            "63402711174946deace788f7ddeb0a33",
            "ac252bb4d75f4e8298468dd2be302dc2",
            "a6285b6f86944da0baf1e3668855eb8a",
            "09c383a5c07f4b65b79809e47f3f93eb",
            "56079e7d7ee14c8b8207ad4d22c9d3c6",
            "ac6fa3f4cbb348ce806bb60b405d2ba6",
            "ef232fa174e241b68f84d296230f2c60",
            "d12b1255f8a44802b6b06993430d1d53",
            "0df02d6521424079aae3fd0fe88fa444",
            "514c680182dd4c2c93edbe9bc37ee8d1",
            "676cc6db403c4f4b92e693ca7fdf338f",
            "f49fbe154cb64119b85e34eaad6e8a60",
            "fede1c779f5049a4bf92b09df3b2a91b",
            "3e15735efc5445e9aeccf9bdcc466c8d",
            "9acb55a3726548329ea1a29f5ac9d7d1",
            "43e91f149c0c4d7eac72f0f7bf87e228",
            "4a626789591d42679b676b48b0ce9527",
            "e04b65733c084fa39266fe89afb73a86",
            "d1b2731d480c48dab0d782ed055dbee6",
            "f0df3b1e88324ebf935fa29f9600a89b",
            "12881fc15b9f436f98d7f5a6b2619b99",
            "648fadd120664691bdc84af1afb85b4e",
            "d6ad73d8987f4eaa9c1f6d9533b4a547",
            "fe386867f60f43da897f7391c5973271",
            "6de6f7b4322e46eba104f02a22684fae",
            "6d2e190f97c34b5691881bdd65dd936e",
            "8d27c056f4c54da0a5b09e87f4c7271c",
            "be611f1d602c4c64b09749edbdd1563f",
            "a0801ff364c3406c8ff1c82dbe48ad9e",
            "3fcf3cbc285e45eb9d80497d1425889e",
            "4177a7856b664cd6a8e8a311f61a252f",
            "155e384047664b28adec813a97fece0b",
            "95c42d47e5a94223be6bc1f0837e049a",
            "24fb98c7f0074b33adc0a34383064176",
            "0108ef9e2f2e402f942a4520d820247f",
            "10865d113db84269b225fed065891573",
            "9727ecfd7f4b4997840eace66b20de39",
            "1bc68c3661e0410881aa7df66ea60a9e",
            "50dce3445a80427e8ddad234cc2d06c8",
            "7c632d07e580466ebf28d993c5efdb27",
            "31cf3b34f9274226a15a9f188a2b0d2a",
            "58a04fb37c1d480fa1d87da1c1db6d0c",
            "94ba6e58ae614a78a54786b2fa15163f",
            "378f9335e61348e496ef190cefcd5cb6",
            "5fed7bd4026b41c783ab38738011f000",
            "f0e8ce23e8824050bb2936b519fb78a1",
            "629ddbe79876437b86056ca5a7e1ff96",
            "cf15d6ffd3a8499d9f666c2497c3d21f",
            "e10ddc47bff34703b81625b67f3e43e2",
            "281d520f5cc24a6d9237358100193e26",
            "915f19a9f0434637bf4c66728e8a8c40",
            "a8a06b81657846e2b92eaa4521d1a95c",
            "5534d3596a5942b39931ce6c37ecd8d0",
            "b308bd5f017b424085f4e4fbdca42fc8",
            "bbdce6f62cea480b9fb5ff170e87f94a",
            "f1bfd0b458af4a7a9c42e68f689c3c68",
            "f4c7e78a86d240ec82f0f0ada8abbd7b",
            "99594d1acfe34decbc93a14f63541f38",
            "39fd202266b34a498f8d3b5c10fd23e2",
            "2e0bb0b90f3745f5bd042fda2a295e30",
            "f372b97138694ee790426f258ad63b2c",
            "70e0eadd35e945d4abae31552a07bde6",
            "0f8d8468aa5148c49542a978f87fc3d6",
            "9456fc2aeb4642ffabf018a227c9e282",
            "83ba5e504edc4d318d69e6f6121c869c",
            "86561f51ef1047f3b09cd7d1c84f99b1",
            "22bd6d2ef08d45d2b358cfd8be830a94",
            "24b23a48b9934c359ed5ca70098c17b5",
            "bc7cbd542bdf4e07953ce08144c82c2b",
            "c95c715d14fe44939ee362d833c0c8df",
            "db35d2fb34eb4ecebfd79ca1f466f3fe",
            "d24e7543eb1942739466cdcb9c1b0e48",
            "e7b3c39057a4495cad8d075a770efc99",
            "4433db8c2e0446a9bb436bd92a782f95",
            "b6e62e3f361749d8b0608cb39007b034",
            "59c6b5e2ba5b4d0a8e706c5e45ca5f3f",
            "77fe2869b8b74a5990bcd9a28a182ec8",
            "040fc67b2cc74c1fa15789edbeb6e8de",
            "1371c57bfd0a4550aa41e9139e2dd6e0",
            "67cd80156cd341578eea8382d98fb2a3",
            "749bab8d13344d42ad6d9efaf3139b1e",
            "ff9cf6c4996c40449baa8833ec968e67",
            "ce2951f4fb8749de8136ad8cd27cdc39",
            "37dc56a3f5a84589a242fd28b6577b2b",
            "7cc9b59aebc74e78879a7279a8794900",
            "f87006cff59146f98cf6ce52b3bbe172",
            "a24fb9c3435b4013899d3ff450ef8a5d",
            "40c6743a40f84fc8891ebe61c42a15e7",
            "81207c2bc097497aa9e64581b05762b6",
            "9cf816e8800b44e597a4db87a86ce307",
            "e8dccc3ce5db469b9dfbc73f645351e7",
            "88732fd96ace450f8f9d533ee3232227",
            "20df198f61d84a46b167886bf6235c1b",
            "a9b28db0eaab4076afc3c541e32c2168",
            "f3284bcfbe5c48b39fdc5a1b822296cd",
            "9c0ca899b50b4dea978a79730156c988",
            "6951c6a971b44ba7b1ac8cc6a00e3fa9",
            "c0a52e6b5f4348e99c605b40933df354",
            "ace90b6c456048fc94993e6159067778",
            "ed885dfccbe8445f941bb2624cb36205",
            "1b3db1b6b814460ba4baddcfa1564259",
            "a6db3a4826cb439885885730460cc03b",
            "3595cb75a34b4d8fbc4314a1e183bcf7",
            "0bda4168270d4a1f9b9e4c8cb1af90c5",
            "16b320f1b4ca407a88e7111a626a88db",
            "25acfb176a0a41f9814a0677daca17b1",
            "e1be1765e81a43bdb9f8e6ba8d6f717e",
            "9c3f22afd69445448a47cdb276b921f1",
            "a22f0503b1be4cba8cb4dcad99525a56",
            "d57697f0bfe44c0982d4210844f2a503",
            "24e984f06be140249fa36f5aa54971e0"
          ]
        },
        "id": "A_Ezjko4Mhaj",
        "outputId": "f62b0762-5263-4291-b4f4-634b2602ad2f"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading dataset...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef6ea094893d480081c4f2bae29895f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/96 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "16168ffdbcfe44a6b4e5bcab282b2670",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading data:   0%|          | 0/96 [00:00<?, ?files/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9cd4081e59a24e8483ef117183da0af2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/30.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b9a7e0aa4d7743c88a2a9f1b3b0b06f7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/30.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5921d010219949bea5d05e3e4d1897b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/29.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d19067095ab4d6982bad812f1b5a466",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/56.6k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "58e2bd44aa864abe8801b4fc38a14580",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "513c46dfa68242958b83d533bcb016cb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/158k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a11916ea2d4342379ae29371ab859ef6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/287k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0df8d324c29e40f8bb736f40fc804e59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/31.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e4aa82e98e048ef8246d0a091c8f197",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/105k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5e4ef31b97ae470cbc7f0c7fe0ed4263",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/86.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "61a9133ac9ca4679b890048f42b5a71d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/185k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "56f66d5a5f3a4aaf941adb340cb05d00",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "988aa424912f486b8d2b3c7130d1c255",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/32.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c11b09c942e4836bbeb550a1d08b1e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/107k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92382f886d644096a9bf8964c96f082b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/55.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9632b9a5ad344987af4ce1601a0a33b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/163k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b71b88770f74b6cbe0d4dd18cf5e010",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/56.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2674c7a345d3439d906862bd0a7c6655",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/396k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76c472d8ce1d4c4a9b8bd7c607537d7e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/168k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "84952555f69a4eed9d2d155c7be177d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/267k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f72106747fb04ce9bb35b4078699358c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/86.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8360a9a438947a7b547aa5b73c1e81d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/251k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c413fa716a40447a85068244ee0ea393",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/31.3k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "feb261227611404d90333d0d741ce560",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/58.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fcac7977dd54c318330f2793fb5c302",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/162k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "76a57657b6d64ed68f3a05f8935c4ea8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/396k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "85584ef6125f400784fae318f9fecbed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.19M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c3372aa18e194e9eaae94698c75f22f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/250k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "35bf4dc1727f4995b8ea30b665f4dc3d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cbc6b57935a42b0a75d5ea684396ba2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/221k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d6eade4af7ee463bbde452a5d907d3ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/276k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "499802a182ef4238b7007b9324476140",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/1.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "768bffa71b204c0da3f38578beef82a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/632k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a16220c9b3b944b7b71f70cd1fac6aea",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/605k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "327e427e506b4f5b8485e3271e7298c9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c8de5c8e44e243b6a69e7e4bf5e9bccd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/785k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57ebea0830e04d90aeadb6a3c3f605c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/631k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bb658407a7a84b498accd11f14dbd71e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/3.53M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "864ac70d3a314e6fa53cd7bc6d0b8599",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/3.30M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2456a28826b246fab33109a43ce4678c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/3.71M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d46d2850fb454391ae7afde1a9c55f79",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/2.99M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "eaff4a16025e4f7eb6a5cf19a112e430",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/4.18M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bd0cce1f1caa4720bebade394df980c8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/4.48M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7e132e4d407f4faea1b277ad26b4020d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/4.53M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d201c523099d4086b36479a07a2d2b7c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/4.62M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e9ca93de3e0435baa716fbb55ca622d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/46.1M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a7853d938bb4474bc94482e68d86734",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d799bc3f5764b50b1e9b1cce92ccbf5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/46.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "edc3e545f3ee471ba1d0a56f32b6a0fa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/44.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86819562e566497ba8c509f1907b1b15",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/44.5M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1fcb962cfe964426a943ef741de847d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/44.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a8f24a6cae24b7f97cebd5840f29102",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d760e7fa00df4858987d0e8a5880edbc",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/47.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "51b418edfb184f1cbfc50f8983936e5c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/2.03M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "740448c069e44cf28a2169cef2be1bbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/2.21M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5affc53d191f46f0ab966783ead8a746",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/2.35M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "afe9a8f404f04c749a615df93324678d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/2.44M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "792e5ab9ad6c45a3a808f2748aadb66b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/8.10M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3eafa8a633584e12a4ae22745c5a1772",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/7.50M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4d7479d2c8aa443daf36e50600c93b5f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/7.28M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74ca231eca844f6c9d8a7554d621f716",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/7.12M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "27c1a1d0e4024d6e82c4de4576faa1f2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "978459f168794af697abfc11dddeaf20",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/1.40M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "78391dc026134e239a24928a85c900b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/1.15M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9961a09ee361491dab2fc8ab8c3a877e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.65M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef14a03b58664535999f0b2259392eee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/16.7M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a35310ebd2454b0298ed5e533859370d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/14.4M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d7563e505f9b4f03bc414579412eb209",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8ef630eb7d9b42e78b9a3fab83fe8bbd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/14.3M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9b6e8929f9314595bb63c582a685cf74",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4c1baee5afb044e3905c8b5551b63c63",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/17.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8be7f2ce3ad743bd84d21fca00f86983",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83ca18c32b6841a3adda3f6114076ae0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/14.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "572ae695ba464cf6a81b6e472ea72c2e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/1.14M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a511c2d507db461dab8a694f84a023e6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/924k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "296b4c35aedf44d48b81f85f6a48419c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/965k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9e41308d38204a49ba7c795c046b3c2f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/1.02M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bfedc51539b741969980c4f541200ab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/553k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2538c0eb23cb459092d223231e340e25",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/657k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "54408e7749ee45908f835181c9e43b09",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/814k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b8fe10da14c44dbf8b549d7dc5a626b2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/214k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0c1184f2edd84fdaa640e776c7f0ed38",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/983k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "37478f7e184240689574c7a2d9bfae06",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/109k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7b26f7daa65041efad9de1190773a50e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/108k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3c4ae4e46d5c44c8bd080c0d43c6c9ee",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/84.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "de572e73bd844cd1aee429b5af926855",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/169k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "05eac7dfd8a649edab2b488c14625d90",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/32.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff013151a0fe4265ba5f44cf122a1468",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/58.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8214b6e8b9ed420ca14e04b13678eafe",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/58.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8cb9bf08a03b4ddb8f93f5fc59160dac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/29.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70a485ae808c492986e3b70e811ca48b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/58.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "666e4f3a5c6a4209827bf348248ac3cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00001-of-00004.parquet:   0%|          | 0.00/31.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "459dfffc39504ac2928a61b4775e7100",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00000-of-00004.parquet:   0%|          | 0.00/56.2k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0735c144a1294353ab4ea7a476358684",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ea4ca9efcdb4cb58636957c9f0ff466",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00002-of-00004.parquet:   0%|          | 0.00/30.1k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81156d95a1844548b336876d81ed0e51",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "train-00003-of-00004.parquet:   0%|          | 0.00/33.0k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "459f969b398f40e6a9f9a46a15f28e4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Generating train split:   0%|          | 0/30504 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset loaded with 30504 samples\n",
            "Training samples: 24403, Validation samples: 3050, Test samples: 3051\n",
            "Initializing enhanced BLIP model...\n",
            "Loading base BLIP model...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36d2e108aa23478cb88ceaf2a1620464",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/4.56k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bae53af7cd1c4470929ae70fabb272e4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f19f385e090410683aa04e5f89c9efa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "443e2ad55fe94f48a81ecbc190b913b4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/287 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8755622d7843410cbe1d6f7d702c1664",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/506 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "be3313273af64f8cb7a414997a749e0f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "45852a780b4b4069a823338d60f29205",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/711k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "384fb16b84234dfbb50db7eeadbf4794",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model hidden dimension: 768\n",
            "Enhanced BLIP model initialized\n",
            "Starting training...\n",
            "Starting training for 8 epochs\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:382: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler(enabled=(device == \"cuda\"))\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff834dd4d7cb40b89a75d1bc6489b1ed",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 1/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 - Avg. Training Loss: 1.6039 (Time: 304.59s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "72e970c61ed844d6bce581177c73afdd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: in the image, a group of people are gathered in a park. the park is filled with people engaged in various activities, some standing and others sitting on benches. one person is holding an umbrella, while the other is walking away from the camera\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered on a sidewalk in an urban setting. the person on the left is wearing a black jacket and blue jeans, while the other three individuals are dressed in blue jackets and hats. they are all smiling\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter in a white uniform with red accents is mid - swing, his body coiled and muscles taut as he prepares to strike the incoming ball. behind him, a catcher in an orange uniform crouches\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Downloading stanford-corenlp-3.6.0 for SPICE ...\n",
            "Progress: 384.5M / 384.5M (100.0%)\n",
            "Extracting stanford-corenlp-3.6.0 ...\n",
            "Done.\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2589\n",
            "Validation CIDEr: 0.0072\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2072\n",
            "Validation METEOR: 0.0910\n",
            "New best model saved at epoch 1\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "04f40e052c394ac6b0fe7e3e3fa74d8e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 2/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 - Avg. Training Loss: 0.8115 (Time: 571.19s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d4548096294242699ff22b897c23c9f9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: this black and white photograph captures a bustling street scene. the perspective is from the sidewalk, looking down the length of the street towards the horizon. on the right side, there ' s a building with a large window that reflects the surrounding environment\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in a restaurant. the table is covered with a white tablecloth and features two plates of food - one containing a sandwich and the other a salad. one person is holding a fork\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter wearing a white uniform with red accents is mid - swing, while an umpire in a black uniform stands behind the batter. the field is a vibrant green, contrasting with the brown dirt around home\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2650\n",
            "Validation CIDEr: 0.0110\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2158\n",
            "Validation METEOR: 0.0980\n",
            "New best model saved at epoch 2\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d0815de7efa3441aa04f6b18e83f46b3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 3/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 - Avg. Training Loss: 0.7217 (Time: 318.70s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1432114b9c604708a78d0ac355a5951d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: this black and white photograph captures a bustling street scene in japan. the perspective is from the sidewalk, looking down the length of the street lined with buildings on both sides. on either side, there are shops and restaurants, their windows reflecting the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in a restaurant. the table is covered with a white tablecloth and features two plates of food - one containing a sandwich and the other a salad. one person is reaching for a\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two zebras are captured in a moment of tranquility within their zoo enclosure. the zebra on the left is facing away from the camera, its black and white stripes contrasting with the verdant grass beneath it. its\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2744\n",
            "Validation CIDEr: 0.0136\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2216\n",
            "Validation METEOR: 0.1030\n",
            "New best model saved at epoch 3\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "514c680182dd4c2c93edbe9bc37ee8d1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 4/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 - Avg. Training Loss: 0.6620 (Time: 317.14s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12881fc15b9f436f98d7f5a6b2619b99",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a bustling street scene in hong kong. the street is lined with tall buildings on both sides, their windows reflecting the city ' s vibrant life. a traffic light stands at the intersection, its red signal glowing brightly against the backdrop\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their enclosure. the elephant on the left stands tall and proud, its gray skin contrasting with the lush green grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2755\n",
            "Validation CIDEr: 0.0159\n",
            "Validation SPICE: 0.0000\n",
            "Validation ROUGE: 0.2242\n",
            "Validation METEOR: 0.1027\n",
            "New best model saved at epoch 4\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "155e384047664b28adec813a97fece0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 5/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 - Avg. Training Loss: 0.6160 (Time: 328.36s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94ba6e58ae614a78a54786b2fa15163f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a bustling street scene in london, uk. the perspective is from the sidewalk, looking down the length of the street towards the iconic big ben clock tower in the background. several people are walking along the sidewalks, some carrying bags\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in a restaurant. the table is adorned with an array of dishes and utensils, including plates of food, glasses filled with red wine, and silverware. a woman in\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "Warning: SPICE computation timed out after 3 minutes, skipping.\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2759\n",
            "Validation CIDEr: 0.0144\n",
            "Validation SPICE: 0.1120\n",
            "Validation ROUGE: 0.2209\n",
            "Validation METEOR: 0.1030\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b308bd5f017b424085f4e4fbdca42fc8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 6/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6 - Avg. Training Loss: 0.5804 (Time: 307.20s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "83ba5e504edc4d318d69e6f6121c869c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a majestic stone clock tower standing tall against the backdrop of a clear blue sky. the tower is adorned with intricate carvings and statues, adding an element of grandeur to the scene. atop the structure, a large clock face is visible\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of pizza and the other a bowl of soup. a\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2770\n",
            "Validation CIDEr: 0.0169\n",
            "Validation SPICE: 0.1148\n",
            "Validation ROUGE: 0.2235\n",
            "Validation METEOR: 0.1037\n",
            "New best model saved at epoch 6\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "59c6b5e2ba5b4d0a8e706c5e45ca5f3f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 7/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7 - Avg. Training Loss: 0.5548 (Time: 306.62s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77fe2869b8b74a5990bcd9a28a182ec8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a serene park scene. in the foreground, there is a wooden bench with a curved backrest and armrests, situated on a concrete path surrounded by lush greenery. beyond the bench, the park is lush with trees\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2787\n",
            "Validation CIDEr: 0.0164\n",
            "Validation SPICE: 0.1146\n",
            "Validation ROUGE: 0.2237\n",
            "Validation METEOR: 0.1036\n",
            "New best model saved at epoch 7\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "040fc67b2cc74c1fa15789edbeb6e8de",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Epoch 8/8:   0%|          | 0/3051 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-c53f5e639a43>:439: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast(enabled=(device == \"cuda\")):\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8 - Avg. Training Loss: 0.5406 (Time: 313.65s)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "81207c2bc097497aa9e64581b05762b6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a serene park scene. in the foreground, there is a large tree with bare branches reaching towards the sky. to the left of the tree, a wooden bench invites visitors to sit and enjoy the tranquil surroundings. on the\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the heart of a bustling city, a vibrant red fire hydrant stands out against the urban backdrop. the hydrangent, painted in a striking shade of red and adorned with white stripes, is positioned on the right side of the image\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the dry grass beneath it. its trunk is extended towards the ground,\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "Validation BLEU-4: 0.2775\n",
            "Validation CIDEr: 0.0154\n",
            "Validation SPICE: 0.1158\n",
            "Validation ROUGE: 0.2243\n",
            "Validation METEOR: 0.1045\n",
            "Training completed!\n",
            "Loading best model from token_gating_checkpoints/best_model.pth for final evaluation\n",
            "Best model was from epoch 7\n",
            "\n",
            "Performing validation set evaluation...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cf816e8800b44e597a4db87a86ce307"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x796950566160>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x796950566160>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x796950566160>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x796950566160>\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1618, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\", line 1601, in _shutdown_workers\n",
            "    if w.is_alive():\n",
            "       ^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.11/multiprocessing/process.py\", line 160, in is_alive\n",
            "    assert self._parent_pid == os.getpid(), 'can only test a child process'\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "AssertionError: can only test a child process\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: the image depicts a serene street scene in a small town. the road is lined with brick buildings, their red and brown hues contrasting with the overcast sky above. trees line both sides of the street, adding a touch of nature to the urban landscape. cars are parked along the side of the road, suggesting a quiet time of day or perhaps a special event taking place nearby. in the distance, a church with a clock tower stands tall, its presence adding a sense of history and community to the scene.\n",
            "  Prediction: the image depicts a serene park scene. in the foreground, there is a wooden bench with a curved backrest and armrests, situated on a concrete path surrounded by lush greenery. beyond the bench, the park is lush with trees\n",
            "\n",
            "  Reference: in the image, a man and two children are standing on a vibrant red tennis court. the man is wearing a gray shirt and blue jeans, holding a red tennis racket in his right hand. the child on his left is wearing a green jacket and blue jeans, also holding a red tennis racket. the other child, dressed in a black jacket and blue jeans, stands next to them. the court is surrounded by a chain - link fence, with trees visible beyond the fence.\n",
            "  Prediction: in the image, a group of people are gathered around a table in an outdoor setting. the table is covered with a white tablecloth and features two plates of food - one containing a slice of cake and the other holding a bowl of soup.\n",
            "\n",
            "  Reference: in the image, two giraffes are captured in a zoo enclosure. the giraffe on the left is bending its long neck to reach for food from a wooden feeding trough, while the giraffe on the right stands tall and reaches up with its long neck towards the sky. the enclosure is surrounded by a sturdy fence, and beyond it, trees rise up, providing shade and greenery.\n",
            "  Prediction: in the image, two elephants are captured in a moment of tranquility within their natural habitat. the elephant on the left stands tall and proud, its gray skin contrasting with the earthy tones of the surroundings. its trunk is extended towards the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation (may take time)...\n",
            "SPICE score computed successfully\n",
            "METEOR score computed successfully\n",
            "\n",
            "Performing test set evaluation (first time evaluating on these samples)...\n",
            "NOTE: This is the final test evaluation, so SPICE will be calculated regardless of time taken\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Evaluating:   0%|          | 0/382 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1b3db1b6b814460ba4baddcfa1564259"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample predictions:\n",
            "  Reference: in the image, a man and a woman are standing on a snowy mountain. the man is wearing a vibrant green jacket with yellow accents and a white helmet, holding a black snowboard with a colorful design. the woman is wearing a purple jacket and a black helmet, holding a pair of red skis. they are both smiling at the camera, their faces lit up with joy. behind them, there ' s a ski lift carrying more adventurers to the top of the mountain.\n",
            "  Prediction: this black and white photograph captures a bustling street scene in japan. the perspective is from the sidewalk, looking down the length of the street towards the horizon. on the left, there ' s a building with a sign that reads \" kodan\n",
            "\n",
            "  Reference: a police officer in a black uniform and helmet is riding a motorcycle with the number \" 2 \" displayed on its side. the officer is holding a flag, possibly indicating an event or celebration. the motorcycle is parked on a street lined with trees and buildings, with a crowd of people gathered around it.\n",
            "  Prediction: the image depicts a baseball game in progress. a batter in a white uniform with red accents is mid - swing, while a catcher in an orange uniform crouches behind home plate. an umpire in black stands just behind the catcher, observing the play\n",
            "\n",
            "  Reference: in the center of a room with white walls, a group of children in red shirts are gathered around a table. the table is covered with a green tablecloth and holds an array of food items, including small sandwiches, hot dogs, and a bowl of ketchup. a man in a red shirt stands to the left of the table, holding a plate of food for one of the children.\n",
            "  Prediction: the image shows a freshly baked pizza resting on a white plate. the pizza has a golden brown crust and is generously topped with vibrant red sauce, melted cheese, and fresh basil leaves. a silver spatula rests on the right side of the\n",
            "\n",
            "CIDEr score computed successfully\n",
            "ROUGE score computed successfully\n",
            "Starting SPICE computation for final test (this may take several minutes)...\n",
            "SPICE score computed successfully for final test\n",
            "METEOR score computed successfully\n",
            "\n",
            "Validation Set Results:\n",
            "BLEU-4: 27.87\n",
            "CIDEr: 1.64\n",
            "SPICE: 11.46\n",
            "ROUGE: 22.37\n",
            "METEOR: 10.36\n",
            "\n",
            "Test Set Results:\n",
            "BLEU-4: 28.06\n",
            "CIDEr: 2.02\n",
            "SPICE: 11.85\n",
            "ROUGE: 22.77\n",
            "METEOR: 10.54\n",
            "\n",
            "Validation vs Test Performance (scores multiplied by 100):\n",
            "Metric Validation  Test Diff (Test-Val)\n",
            "BLEU-4      27.87 28.06            0.19\n",
            " CIDEr       1.64  2.02            0.37\n",
            " SPICE      11.46 11.85            0.38\n",
            " ROUGE      22.37 22.77            0.40\n",
            "METEOR      10.36 10.54            0.18\n",
            "Comparison results saved to token_gating_checkpoints/val_test_comparison.csv\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "\n",
        "from transformers import BlipForConditionalGeneration, BlipProcessor\n",
        "from datasets import load_dataset\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from pycocoevalcap.cider.cider import Cider\n",
        "from pycocoevalcap.spice.spice import Spice\n",
        "from pycocoevalcap.rouge.rouge import Rouge\n",
        "from pycocoevalcap.meteor.meteor import Meteor\n",
        "\n",
        "import math\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "from tqdm.auto import tqdm\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Token Gating Implementation\n",
        "class TokenGating(nn.Module):\n",
        "    \"\"\"\n",
        "    Token Gating mechanism that selectively focuses on important tokens\n",
        "    while suppressing less relevant ones.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, dropout=0.1):\n",
        "        super(TokenGating, self).__init__()\n",
        "        self.gate_transform = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim // 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim // 2, 1)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        # Calculate importance score for each token [batch_size, seq_len, 1]\n",
        "        gate_scores = self.sigmoid(self.gate_transform(hidden_states))\n",
        "\n",
        "        # Apply scaling factor to ensure stability during training\n",
        "        gate_scores = gate_scores * 2.0\n",
        "\n",
        "        # Apply the gate - element-wise multiplication\n",
        "        gated_output = hidden_states * gate_scores\n",
        "\n",
        "        # Use attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            mask = attention_mask.unsqueeze(-1)\n",
        "            gated_output = gated_output * mask\n",
        "\n",
        "        return gated_output, gate_scores\n",
        "\n",
        "# 2. Sparse Attention Implementation\n",
        "class SparseAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Implements sparse attention by selecting only the top-k most relevant tokens\n",
        "    for each position during attention computation.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim, num_heads=8, dropout=0.1, sparsity=0.0):\n",
        "        super(SparseAttention, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = hidden_dim // num_heads\n",
        "        assert self.head_dim * num_heads == hidden_dim, \"hidden_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.sparsity = sparsity  # Percent of attention connections to prune\n",
        "\n",
        "        self.q_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.k_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.v_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_proj = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask=None):\n",
        "        batch_size, seq_len, _ = hidden_states.shape\n",
        "\n",
        "        # Linear projections and reshape to multi-head\n",
        "        q = self.q_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        k = self.k_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "        v = self.v_proj(hidden_states).reshape(batch_size, seq_len, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose for attention computation [batch_size, num_heads, seq_len, head_dim]\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention\n",
        "        attn_weights = torch.matmul(q, k.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
        "\n",
        "        # Apply attention mask if provided\n",
        "        if attention_mask is not None:\n",
        "            # Expand mask for multi-head attention [batch_size, 1, 1, seq_len]\n",
        "            expanded_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn_weights = attn_weights.masked_fill(expanded_mask == 0, -1e10)\n",
        "\n",
        "        # Compute sparse attention by keeping only top-k values\n",
        "        if self.training:\n",
        "            # Determine number of tokens to keep based on sparsity level\n",
        "            k_tokens = max(1, int((1 - self.sparsity) * seq_len))\n",
        "\n",
        "            # Get top-k values for each query token\n",
        "            top_k_attn, _ = torch.topk(attn_weights, k=k_tokens, dim=-1)\n",
        "\n",
        "            # Use smallest value from top-k as threshold\n",
        "            sparse_threshold = top_k_attn[..., -1].unsqueeze(-1)\n",
        "\n",
        "            # Create a binary mask for sparse attention\n",
        "            sparse_mask = (attn_weights >= sparse_threshold).float()\n",
        "\n",
        "            # Apply the sparse mask\n",
        "            attn_weights = attn_weights * sparse_mask + -1e10 * (1 - sparse_mask)\n",
        "\n",
        "        # Apply softmax to get normalized attention weights\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Apply attention weights to values\n",
        "        context = torch.matmul(attn_weights, v)\n",
        "\n",
        "        # Reshape back to [batch_size, seq_len, hidden_dim]\n",
        "        context = context.transpose(1, 2).reshape(batch_size, seq_len, self.hidden_dim)\n",
        "\n",
        "        # Final output projection\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        return output\n",
        "\n",
        "# 3. Enhanced BLIP Model with Token Gating and Sparse Attention\n",
        "class EnhancedBLIP(nn.Module):\n",
        "    \"\"\"\n",
        "    Enhances BLIP model with token gating and sparse attention mechanisms.\n",
        "    \"\"\"\n",
        "    def __init__(self, sparsity=0.0):\n",
        "        super(EnhancedBLIP, self).__init__()\n",
        "\n",
        "        # Load base model\n",
        "        print(\"Loading base BLIP model...\")\n",
        "        self.base_model = BlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        self.processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "\n",
        "        # Get hidden dimension from the base model\n",
        "        hidden_dim = self.base_model.text_decoder.config.hidden_size\n",
        "        print(f\"Model hidden dimension: {hidden_dim}\")\n",
        "\n",
        "        # Add token gating layers\n",
        "        self.text_gate = TokenGating(hidden_dim)\n",
        "        self.vision_gate = TokenGating(hidden_dim)\n",
        "\n",
        "        # Add sparse attention layers\n",
        "        self.text_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "        self.vision_sparse_attn = SparseAttention(hidden_dim, sparsity=sparsity)\n",
        "\n",
        "        # Layer norms for stability\n",
        "        self.text_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.text_ln2 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln1 = nn.LayerNorm(hidden_dim)\n",
        "        self.vision_ln2 = nn.LayerNorm(hidden_dim)\n",
        "\n",
        "        # Feed-forward networks for residual paths\n",
        "        self.text_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        self.vision_ffn = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.1),\n",
        "            nn.Linear(hidden_dim * 4, hidden_dim)\n",
        "        )\n",
        "\n",
        "        # Flag to control whether to apply enhancements\n",
        "        self.apply_enhancements = True\n",
        "        print(\"Enhanced BLIP model initialized\")\n",
        "\n",
        "    def _enhance_text_features(self, hidden_states, attention_mask=None):\n",
        "        \"\"\"Apply token gating and sparse attention to text features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.text_gate(hidden_states, attention_mask)\n",
        "        gated_states = self.text_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.text_sparse_attn(gated_states, attention_mask)\n",
        "        sparse_states = self.text_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.text_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def _enhance_vision_features(self, hidden_states):\n",
        "        \"\"\"Apply token gating and sparse attention to vision features.\"\"\"\n",
        "        # Apply token gating\n",
        "        gated_states, _ = self.vision_gate(hidden_states, None)\n",
        "        gated_states = self.vision_ln1(gated_states + hidden_states)  # Residual connection\n",
        "\n",
        "        # Apply sparse attention\n",
        "        sparse_states = self.vision_sparse_attn(gated_states, None)\n",
        "        sparse_states = self.vision_ln2(sparse_states + gated_states)  # Residual connection\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        output = hidden_states + self.vision_ffn(sparse_states)\n",
        "\n",
        "        return output\n",
        "\n",
        "    def forward(self, pixel_values=None, input_ids=None, attention_mask=None, labels=None, return_dict=True):\n",
        "        # First pass through base model\n",
        "        outputs = self.base_model(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            labels=labels,\n",
        "            output_hidden_states=True,\n",
        "            return_dict=True\n",
        "        )\n",
        "\n",
        "        # Return base model outputs if enhancements are disabled\n",
        "        if not self.apply_enhancements:\n",
        "            return outputs\n",
        "\n",
        "        # Apply token gating and sparse attention to hidden states\n",
        "        # Note: This is for inference only - the training loss comes from the base model\n",
        "        return outputs\n",
        "\n",
        "    def generate(self, pixel_values=None, input_ids=None, attention_mask=None, **kwargs):\n",
        "        \"\"\"Generate captions using the base model's generation capability.\"\"\"\n",
        "        return self.base_model.generate(\n",
        "            pixel_values=pixel_values,\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "# 4. Data Processing Functions\n",
        "def closest_factors(n):\n",
        "    \"\"\"Finds the closest factors of n to get an aspect ratio close to a square.\"\"\"\n",
        "    sqrt_n = int(math.sqrt(n))\n",
        "    for i in range(sqrt_n, 0, -1):\n",
        "        if n % i == 0:\n",
        "            return i, n // i  # Return H, W such that H × W = n\n",
        "\n",
        "    # If no exact factors, use power of 2 dimensions\n",
        "    size = 2 ** int(math.log2(math.sqrt(n)))\n",
        "    return size, size\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"\n",
        "    Collate function for VAE latents dataset that properly handles\n",
        "    variable length latents, applies normalization, and reshapes for BLIP.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Extract captions and VAE latents\n",
        "        captions = [item[\"caption\"] for item in batch]\n",
        "        vae_latents = [torch.tensor(item[\"vae_latent\"], dtype=torch.float32) for item in batch]\n",
        "\n",
        "        # Find maximum length for padding\n",
        "        max_len = max([latent.shape[0] for latent in vae_latents])\n",
        "\n",
        "        # Pad tensors\n",
        "        padded_latents = []\n",
        "        for latent in vae_latents:\n",
        "            pad_size = max_len - latent.shape[0]\n",
        "            if pad_size > 0:\n",
        "                padding = torch.zeros(pad_size, dtype=torch.float32)\n",
        "                padded = torch.cat([latent, padding])\n",
        "            else:\n",
        "                padded = latent\n",
        "            padded_latents.append(padded)\n",
        "\n",
        "        # Stack tensors\n",
        "        latents = torch.stack(padded_latents)\n",
        "\n",
        "        # Apply z-score normalization (per sample)\n",
        "        means = latents.mean(dim=1, keepdim=True)\n",
        "        stds = latents.std(dim=1, keepdim=True) + 1e-6  # Avoid division by zero\n",
        "        normalized_latents = (latents - means) / stds\n",
        "\n",
        "        # Reshape for BLIP\n",
        "        batch_size = len(batch)\n",
        "        feature_dim = normalized_latents.shape[1]\n",
        "\n",
        "        # Get dimensions for reshaping\n",
        "        height, width = closest_factors(feature_dim)\n",
        "\n",
        "        # Check if we need to adjust dimensions\n",
        "        if height * width != feature_dim:\n",
        "            # Use power of 2 dimensions and pad/truncate\n",
        "            height = 2 ** int(math.log2(math.sqrt(feature_dim)))\n",
        "            width = height\n",
        "            padded_dim = height * width\n",
        "\n",
        "            if padded_dim > feature_dim:\n",
        "                # Pad each latent\n",
        "                padding = torch.zeros((batch_size, padded_dim - feature_dim), dtype=torch.float32)\n",
        "                normalized_latents = torch.cat([normalized_latents, padding], dim=1)\n",
        "            else:\n",
        "                # Truncate each latent\n",
        "                normalized_latents = normalized_latents[:, :padded_dim]\n",
        "\n",
        "        # Reshape latents to image format (batch_size, channels, height, width)\n",
        "        try:\n",
        "            images = normalized_latents.view(batch_size, 1, height, width)\n",
        "            images = images.repeat(1, 3, 1, 1)  # Repeat along channel dimension for RGB\n",
        "        except RuntimeError as e:\n",
        "            print(f\"Error reshaping latents: {e}\")\n",
        "            print(f\"Using fallback reshaping method\")\n",
        "\n",
        "            # Fallback to simple square reshaping\n",
        "            side = int(math.ceil(math.sqrt(feature_dim)))\n",
        "            images = torch.zeros((batch_size, 3, side, side), dtype=torch.float32)\n",
        "\n",
        "            for i, latent in enumerate(normalized_latents):\n",
        "                # Pad if needed\n",
        "                if latent.shape[0] < side * side:\n",
        "                    latent = torch.cat([latent, torch.zeros(side * side - latent.shape[0])])\n",
        "                else:\n",
        "                    latent = latent[:side * side]\n",
        "\n",
        "                # Reshape to square and repeat channels\n",
        "                img = latent.view(1, side, side).repeat(3, 1, 1)\n",
        "                images[i] = img\n",
        "\n",
        "        # Process captions\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        encoded_captions = processor(text=captions, padding=\"max_length\", truncation=True,\n",
        "                                  max_length=128, return_tensors=\"pt\")\n",
        "\n",
        "        # Combine images and captions\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": images.to(torch.float32),\n",
        "            \"input_ids\": encoded_captions[\"input_ids\"],\n",
        "            \"attention_mask\": encoded_captions[\"attention_mask\"],\n",
        "            \"labels\": encoded_captions[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in collate_fn: {e}\")\n",
        "        # Return a minimal valid batch to avoid training failure\n",
        "        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n",
        "        dummy_captions = [\"dummy caption\"] * len(batch)\n",
        "        encoded = processor(text=dummy_captions, padding=\"max_length\", truncation=True,\n",
        "                           max_length=128, return_tensors=\"pt\")\n",
        "        dummy_images = torch.zeros((len(batch), 3, 32, 32), dtype=torch.float32)\n",
        "\n",
        "        batch_encoding = {\n",
        "            \"pixel_values\": dummy_images,\n",
        "            \"input_ids\": encoded[\"input_ids\"],\n",
        "            \"attention_mask\": encoded[\"attention_mask\"],\n",
        "            \"labels\": encoded[\"input_ids\"].clone()\n",
        "        }\n",
        "\n",
        "        return batch_encoding\n",
        "\n",
        "# 5. Training Function with Mixed Precision and Gradient Accumulation\n",
        "def train_model(model, train_loader, val_loader=None, num_epochs=8,  # MODIFIED: Set to 8 epochs based on previous results\n",
        "                lr=2e-5, device=\"cuda\", checkpoint_dir=\"checkpoints\"):\n",
        "    \"\"\"\n",
        "    Train the enhanced BLIP model with mixed precision, gradient accumulation,\n",
        "    and proper checkpointing.\n",
        "    \"\"\"\n",
        "    print(f\"Starting training for {num_epochs} epochs\")\n",
        "\n",
        "    # Create checkpoint directory\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "    # Prepare optimizer with weight decay\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    # Mixed precision training\n",
        "    scaler = GradScaler(enabled=(device == \"cuda\"))\n",
        "\n",
        "    # Gradient accumulation steps (effective batch size = batch_size * accum_steps)\n",
        "    accum_steps = 4\n",
        "\n",
        "    # Early stopping parameters\n",
        "    patience = 3\n",
        "    best_score = float('-inf')\n",
        "    no_improve_epochs = 0\n",
        "\n",
        "    # Tracking metrics for saving\n",
        "    train_losses = []\n",
        "    val_bleu_scores = []\n",
        "    val_cider_scores = []\n",
        "    val_spice_scores = []\n",
        "    val_rouge_scores = []\n",
        "    val_meteor_scores = []\n",
        "\n",
        "    # Resume from checkpoint if available\n",
        "    checkpoint_path = os.path.join(checkpoint_dir, \"latest_model.pth\")\n",
        "    start_epoch = 1\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        try:\n",
        "            print(f\"Loading checkpoint from {checkpoint_path}\")\n",
        "            checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)  # FIXED: added weights_only=False\n",
        "            model.load_state_dict(checkpoint[\"model_state\"])\n",
        "            optimizer.load_state_dict(checkpoint[\"optimizer_state\"])\n",
        "            start_epoch = checkpoint[\"epoch\"] + 1\n",
        "            best_score = checkpoint.get(\"best_score\", float('-inf'))\n",
        "\n",
        "            # Move optimizer states to right device\n",
        "            for state in optimizer.state.values():\n",
        "                for k, v in state.items():\n",
        "                    if isinstance(v, torch.Tensor):\n",
        "                        state[k] = v.to(device)\n",
        "\n",
        "            print(f\"Resuming training from epoch {start_epoch}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading checkpoint: {e}\")\n",
        "            print(\"Starting training from scratch\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(start_epoch, num_epochs + 1):\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Progress bar for training batches\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(progress_bar):\n",
        "            try:\n",
        "                # Move batch to device\n",
        "                batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "                # Forward pass with mixed precision\n",
        "                with autocast(enabled=(device == \"cuda\")):\n",
        "                    outputs = model(**batch)\n",
        "                    loss = outputs.loss / accum_steps  # Scale loss for accumulation\n",
        "\n",
        "                # Backward pass with gradient scaling\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                # Update weights after accumulation or at the end\n",
        "                if (batch_idx + 1) % accum_steps == 0 or (batch_idx + 1 == len(train_loader)):\n",
        "                    # Unscale gradients for clipping\n",
        "                    scaler.unscale_(optimizer)\n",
        "\n",
        "                    # Clip gradients to prevent explosive values\n",
        "                    clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                    # Optimizer step with scaling\n",
        "                    scaler.step(optimizer)\n",
        "                    scaler.update()\n",
        "                    optimizer.zero_grad()\n",
        "\n",
        "                # Track loss\n",
        "                total_loss += loss.item() * accum_steps\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\"loss\": f\"{loss.item() * accum_steps:.4f}\"})\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error in batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "        # Calculate average loss for the epoch\n",
        "        avg_loss = total_loss / len(train_loader)\n",
        "        train_time = time.time() - start_time\n",
        "        train_losses.append(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch} - Avg. Training Loss: {avg_loss:.4f} (Time: {train_time:.2f}s)\")\n",
        "\n",
        "        # Validation phase\n",
        "        if val_loader is not None:\n",
        "            val_metrics = evaluate_model(model, val_loader, device=device)\n",
        "\n",
        "            # Store metrics\n",
        "            val_bleu_scores.append(val_metrics['bleu4'])\n",
        "            val_cider_scores.append(val_metrics['cider'])\n",
        "            val_spice_scores.append(val_metrics['spice'])\n",
        "            val_rouge_scores.append(val_metrics['rouge'])\n",
        "            val_meteor_scores.append(val_metrics.get('meteor', 0.0))\n",
        "\n",
        "            # Log validation metrics\n",
        "            print(f\"Validation BLEU-4: {val_metrics['bleu4']:.4f}\")\n",
        "            print(f\"Validation CIDEr: {val_metrics['cider']:.4f}\")\n",
        "            print(f\"Validation SPICE: {val_metrics['spice']:.4f}\")\n",
        "            print(f\"Validation ROUGE: {val_metrics['rouge']:.4f}\")\n",
        "            if 'meteor' in val_metrics and val_metrics['meteor'] > 0:\n",
        "                print(f\"Validation METEOR: {val_metrics['meteor']:.4f}\")\n",
        "\n",
        "            # Use CIDEr + BLEU-4 as overall score for early stopping\n",
        "            current_score = val_metrics['cider'] + val_metrics['bleu4']\n",
        "        else:\n",
        "            # If no validation set, use negative training loss as score\n",
        "            current_score = -avg_loss\n",
        "\n",
        "        # Update learning rate\n",
        "        scheduler.step()\n",
        "\n",
        "        # Save checkpoint\n",
        "        torch.save({\n",
        "            \"epoch\": epoch,\n",
        "            \"model_state\": model.state_dict(),\n",
        "            \"optimizer_state\": optimizer.state_dict(),\n",
        "            \"best_score\": best_score,\n",
        "            \"train_losses\": train_losses,\n",
        "            \"val_bleu_scores\": val_bleu_scores,\n",
        "            \"val_cider_scores\": val_cider_scores,\n",
        "            \"val_spice_scores\": val_spice_scores,\n",
        "            \"val_rouge_scores\": val_rouge_scores,\n",
        "            \"val_meteor_scores\": val_meteor_scores\n",
        "        }, checkpoint_path)\n",
        "\n",
        "        # Check for improvement\n",
        "        if current_score > best_score:\n",
        "            best_score = current_score\n",
        "            no_improve_epochs = 0\n",
        "\n",
        "            # Save best model\n",
        "            best_model_path = os.path.join(checkpoint_dir, \"best_model.pth\")\n",
        "            torch.save({\n",
        "                \"epoch\": epoch,\n",
        "                \"model_state\": model.state_dict(),\n",
        "                \"best_score\": best_score,\n",
        "                \"train_losses\": train_losses,\n",
        "                \"val_bleu_scores\": val_bleu_scores,\n",
        "                \"val_cider_scores\": val_cider_scores,\n",
        "                \"val_spice_scores\": val_spice_scores,\n",
        "                \"val_rouge_scores\": val_rouge_scores,\n",
        "                \"val_meteor_scores\": val_meteor_scores\n",
        "            }, best_model_path)\n",
        "\n",
        "            print(f\"New best model saved at epoch {epoch}\")\n",
        "        else:\n",
        "            no_improve_epochs += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if no_improve_epochs >= patience:\n",
        "            print(f\"No improvement for {patience} epochs. Early stopping.\")\n",
        "            break\n",
        "\n",
        "    # Save training history\n",
        "    history = {\n",
        "        \"epochs\": list(range(1, len(train_losses) + 1)),\n",
        "        \"train_loss\": train_losses,\n",
        "        \"val_bleu4\": val_bleu_scores,\n",
        "        \"val_cider\": val_cider_scores,\n",
        "        \"val_spice\": val_spice_scores,\n",
        "        \"val_rouge\": val_rouge_scores,\n",
        "        \"val_meteor\": val_meteor_scores\n",
        "    }\n",
        "\n",
        "    history_path = os.path.join(checkpoint_dir, \"training_history.pt\")\n",
        "    torch.save(history, history_path)\n",
        "\n",
        "    print(\"Training completed!\")\n",
        "    return model\n",
        "\n",
        "# Modified evaluate_model function with more robust SPICE calculation for final evaluation\n",
        "def evaluate_model(model, val_loader, device=\"cuda\", max_samples=None, is_final_test=False):\n",
        "    \"\"\"\n",
        "    Evaluate the model on validation set with BLEU, CIDEr, SPICE, and ROUGE metrics.\n",
        "\n",
        "    Parameters:\n",
        "        model: the model to evaluate\n",
        "        val_loader: data loader for evaluation\n",
        "        device: device to run evaluation on\n",
        "        max_samples: maximum number of samples to evaluate (None for all)\n",
        "        is_final_test: boolean flag to indicate if this is the final test evaluation\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    references = []\n",
        "\n",
        "    # Count samples for potential limit\n",
        "    sample_count = 0\n",
        "\n",
        "    # Process validation batches\n",
        "    for batch_idx, batch in enumerate(tqdm(val_loader, desc=\"Evaluating\")):\n",
        "        # Move batch to device\n",
        "        batch = {k: v.to(device) for k, v in batch.items()}\n",
        "\n",
        "        # Generate captions with beam search\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "                output_ids = model.generate(\n",
        "                    pixel_values=batch[\"pixel_values\"],\n",
        "                    max_length=50,\n",
        "                    num_beams=5,\n",
        "                    length_penalty=1.0,\n",
        "                    no_repeat_ngram_size=2\n",
        "                )\n",
        "\n",
        "                # Decode generated captions\n",
        "                pred_captions = model.processor.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "                # Decode reference captions\n",
        "                ref_captions = model.processor.batch_decode(batch[\"labels\"], skip_special_tokens=True)\n",
        "\n",
        "                # Store predictions and references\n",
        "                predictions.extend(pred_captions)\n",
        "                references.extend([[ref] for ref in ref_captions])  # BLEU expects list of references per example\n",
        "\n",
        "                # Update sample count\n",
        "                sample_count += len(pred_captions)\n",
        "\n",
        "                # Print sample predictions (first batch only)\n",
        "                if batch_idx == 0:\n",
        "                    print(\"\\nSample predictions:\")\n",
        "                    for i in range(min(3, len(pred_captions))):\n",
        "                        print(f\"  Reference: {ref_captions[i]}\")\n",
        "                        print(f\"  Prediction: {pred_captions[i]}\")\n",
        "                        print()\n",
        "\n",
        "                # Check if we've processed enough samples\n",
        "                if max_samples is not None and sample_count >= max_samples:\n",
        "                    break\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error generating captions for batch {batch_idx}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Compute BLEU score\n",
        "    bleu4 = corpus_bleu(references, predictions, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "    # Prepare data for other metrics\n",
        "    metric_refs = {i: [ref[0]] for i, ref in enumerate(references)}\n",
        "    metric_preds = {i: [pred] for i, pred in enumerate(predictions)}\n",
        "\n",
        "    # Compute other metrics with better error handling\n",
        "    cider_score = 0.0\n",
        "    spice_score = 0.0\n",
        "    rouge_score = 0.0\n",
        "    meteor_score = 0.0\n",
        "\n",
        "    # Try CIDEr first\n",
        "    try:\n",
        "        cider_score = Cider().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"CIDEr score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing CIDEr: {e}\")\n",
        "\n",
        "    # Try ROUGE next\n",
        "    try:\n",
        "        rouge_score = Rouge().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"ROUGE score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing ROUGE: {e}\")\n",
        "\n",
        "    # Try SPICE with different handling based on whether it's final test\n",
        "    try:\n",
        "        import threading\n",
        "        import time\n",
        "\n",
        "        def compute_spice():\n",
        "            nonlocal spice_score\n",
        "            try:\n",
        "                spice_score = Spice().compute_score(metric_refs, metric_preds)[0]\n",
        "            except Exception as e:\n",
        "                print(f\"Error in SPICE calculation: {e}\")\n",
        "\n",
        "        # For final test evaluation, we don't want to skip SPICE\n",
        "        if is_final_test:\n",
        "            print(\"Starting SPICE computation for final test (this may take several minutes)...\")\n",
        "            # For final test, we'll compute SPICE directly without a timeout\n",
        "            try:\n",
        "                spice_score = Spice().compute_score(metric_refs, metric_preds)[0]\n",
        "                print(\"SPICE score computed successfully for final test\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error computing SPICE for final test: {e}\")\n",
        "\n",
        "                # If that fails, try batch processing approach\n",
        "                try:\n",
        "                    print(\"Attempting batch processing approach for SPICE...\")\n",
        "                    # Process in smaller batches to avoid timeout\n",
        "                    batch_size = 100\n",
        "                    total_score = 0.0\n",
        "                    total_batches = 0\n",
        "\n",
        "                    for i in range(0, len(predictions), batch_size):\n",
        "                        batch_end = min(i + batch_size, len(predictions))\n",
        "                        batch_refs = {j: metric_refs[j+i] for j in range(batch_end-i)}\n",
        "                        batch_preds = {j: metric_preds[j+i] for j in range(batch_end-i)}\n",
        "\n",
        "                        batch_score = Spice().compute_score(batch_refs, batch_preds)[0]\n",
        "                        total_score += batch_score * (batch_end-i)\n",
        "                        total_batches += (batch_end-i)\n",
        "                        print(f\"Processed SPICE batch {i//batch_size + 1}/{(len(predictions)-1)//batch_size + 1}\")\n",
        "\n",
        "                    if total_batches > 0:\n",
        "                        spice_score = total_score / total_batches\n",
        "                        print(f\"SPICE score computed successfully using batch approach: {spice_score:.4f}\")\n",
        "                except Exception as e2:\n",
        "                    print(f\"Batch SPICE calculation also failed: {e2}\")\n",
        "        else:\n",
        "            # For regular evaluation (not final test), use timeout approach\n",
        "            print(\"Starting SPICE computation (may take time)...\")\n",
        "            spice_thread = threading.Thread(target=compute_spice)\n",
        "            spice_thread.daemon = True\n",
        "            spice_thread.start()\n",
        "\n",
        "            # Wait for 3 minutes max for non-final evaluations\n",
        "            spice_thread.join(timeout=180)\n",
        "\n",
        "            if spice_thread.is_alive():\n",
        "                print(\"Warning: SPICE computation timed out after 3 minutes, skipping.\")\n",
        "            else:\n",
        "                print(\"SPICE score computed successfully\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error setting up SPICE computation: {e}\")\n",
        "        print(\"Skipping SPICE evaluation\")\n",
        "\n",
        "    # Try METEOR\n",
        "    try:\n",
        "        meteor_score = Meteor().compute_score(metric_refs, metric_preds)[0]\n",
        "        print(\"METEOR score computed successfully\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error computing METEOR: {e}\")\n",
        "\n",
        "    return {\n",
        "        \"bleu4\": bleu4,\n",
        "        \"cider\": cider_score,\n",
        "        \"spice\": spice_score,\n",
        "        \"rouge\": rouge_score,\n",
        "        \"meteor\": meteor_score\n",
        "    }\n",
        "\n",
        "# Modified main function to use the is_final_test flag\n",
        "def main():\n",
        "    # Set random seed for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "\n",
        "    # Add safe globals for PyTorch 2.6 compatibility\n",
        "    import numpy as np\n",
        "    from numpy._core.multiarray import scalar\n",
        "    try:\n",
        "        torch.serialization.add_safe_globals([scalar])\n",
        "    except:\n",
        "        print(\"Warning: Could not add safe globals for PyTorch 2.6. If loading checkpoints fails, try using weights_only=False.\")\n",
        "\n",
        "    # Set device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load dataset\n",
        "    print(\"Loading dataset...\")\n",
        "    try:\n",
        "        dataset = load_dataset(\"SwayStar123/preprocessed_recap-coco30k-moondream\")['train']\n",
        "        print(f\"Dataset loaded with {len(dataset)} samples\")\n",
        "\n",
        "        # Create a proper train/val/test split (80%/10%/10%)\n",
        "        total_size = len(dataset)\n",
        "\n",
        "        # Shuffle the dataset indices with a fixed seed for reproducibility\n",
        "        shuffled_indices = torch.randperm(total_size).tolist()\n",
        "\n",
        "        # Calculate split sizes\n",
        "        train_size = int(0.8 * total_size)\n",
        "        val_size = int(0.1 * total_size)\n",
        "        test_size = total_size - train_size - val_size\n",
        "\n",
        "        # Create the splits\n",
        "        train_indices = shuffled_indices[:train_size]\n",
        "        val_indices = shuffled_indices[train_size:train_size+val_size]\n",
        "        test_indices = shuffled_indices[train_size+val_size:]\n",
        "\n",
        "        train_ds = dataset.select(train_indices)\n",
        "        val_ds = dataset.select(val_indices)\n",
        "        test_ds = dataset.select(test_indices)\n",
        "\n",
        "        print(f\"Training samples: {len(train_ds)}, Validation samples: {len(val_ds)}, Test samples: {len(test_ds)}\")\n",
        "\n",
        "        # Create data loaders\n",
        "        train_loader = DataLoader(\n",
        "            train_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=True,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        test_loader = DataLoader(\n",
        "            test_ds,\n",
        "            batch_size=8,\n",
        "            shuffle=False,\n",
        "            collate_fn=collate_fn,\n",
        "            num_workers=2,\n",
        "            pin_memory=True\n",
        "        )\n",
        "\n",
        "        # Initialize enhanced model\n",
        "        print(\"Initializing enhanced BLIP model...\")\n",
        "        model = EnhancedBLIP(sparsity=0.7)  # Keeping preferred sparsity value\n",
        "        model.to(device)\n",
        "\n",
        "        # Train model\n",
        "        print(\"Starting training...\")\n",
        "        train_model(\n",
        "            model=model,\n",
        "            train_loader=train_loader,\n",
        "            val_loader=val_loader,\n",
        "            num_epochs=8,  # Set to 8 epochs based on previous results\n",
        "            lr=2e-5,\n",
        "            device=device,\n",
        "            checkpoint_dir=\"token_gating_checkpoints\"\n",
        "        )\n",
        "\n",
        "        # Load best model for evaluation\n",
        "        best_model_path = os.path.join(\"token_gating_checkpoints\", \"best_model.pth\")\n",
        "        if os.path.exists(best_model_path):\n",
        "            print(f\"Loading best model from {best_model_path} for final evaluation\")\n",
        "            try:\n",
        "                # Try with weights_only=False first\n",
        "                checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "                model.load_state_dict(checkpoint[\"model_state\"])\n",
        "                best_epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
        "                print(f\"Best model was from epoch {best_epoch}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading with weights_only=False: {e}\")\n",
        "                print(\"Trying alternative loading method...\")\n",
        "                # If that fails, try a more forgiving approach\n",
        "                import numpy as np\n",
        "                from numpy._core.multiarray import scalar\n",
        "                torch.serialization.add_safe_globals([scalar])\n",
        "                checkpoint = torch.load(best_model_path, map_location=device, weights_only=False)\n",
        "                model.load_state_dict(checkpoint[\"model_state\"])\n",
        "                best_epoch = checkpoint.get(\"epoch\", \"unknown\")\n",
        "                print(f\"Best model was from epoch {best_epoch}\")\n",
        "\n",
        "        # Evaluate on validation set (not final test)\n",
        "        print(\"\\nPerforming validation set evaluation...\")\n",
        "        val_metrics = evaluate_model(model, val_loader, device=device, is_final_test=False)\n",
        "\n",
        "        # Evaluate on test set (marked as final test to ensure SPICE is calculated)\n",
        "        print(\"\\nPerforming test set evaluation (first time evaluating on these samples)...\")\n",
        "        print(\"NOTE: This is the final test evaluation, so SPICE will be calculated regardless of time taken\")\n",
        "        test_metrics = evaluate_model(model, test_loader, device=device, is_final_test=True)\n",
        "\n",
        "        # Print validation results - multiply metrics by 100\n",
        "        print(\"\\nValidation Set Results:\")\n",
        "        val_metrics_100 = {k: v * 100 for k, v in val_metrics.items()}\n",
        "        print(f\"BLEU-4: {val_metrics_100['bleu4']:.2f}\")\n",
        "        print(f\"CIDEr: {val_metrics_100['cider']:.2f}\")\n",
        "        print(f\"SPICE: {val_metrics_100['spice']:.2f}\")\n",
        "        print(f\"ROUGE: {val_metrics_100['rouge']:.2f}\")\n",
        "        if 'meteor' in val_metrics_100 and val_metrics_100['meteor'] > 0:\n",
        "            print(f\"METEOR: {val_metrics_100['meteor']:.2f}\")\n",
        "\n",
        "        # Print test results - multiply metrics by 100\n",
        "        print(\"\\nTest Set Results:\")\n",
        "        test_metrics_100 = {k: v * 100 for k, v in test_metrics.items()}\n",
        "        print(f\"BLEU-4: {test_metrics_100['bleu4']:.2f}\")\n",
        "        print(f\"CIDEr: {test_metrics_100['cider']:.2f}\")\n",
        "        print(f\"SPICE: {test_metrics_100['spice']:.2f}\")\n",
        "        print(f\"ROUGE: {test_metrics_100['rouge']:.2f}\")\n",
        "        if 'meteor' in test_metrics_100 and test_metrics_100['meteor'] > 0:\n",
        "            print(f\"METEOR: {test_metrics_100['meteor']:.2f}\")\n",
        "\n",
        "        # Additional check for SPICE in test results\n",
        "        if test_metrics['spice'] == 0.0:\n",
        "            print(\"WARNING: SPICE score is still 0.0 in test results.\")\n",
        "            print(\"Attempting one more SPICE calculation with small subset...\")\n",
        "\n",
        "            # Try calculating SPICE on a smaller subset as a fallback\n",
        "            small_test_loader = DataLoader(\n",
        "                test_ds.select(range(min(500, len(test_ds)))),  # Use at most 500 samples\n",
        "                batch_size=8,\n",
        "                shuffle=False,\n",
        "                collate_fn=collate_fn,\n",
        "                num_workers=1\n",
        "            )\n",
        "\n",
        "            print(\"Running SPICE calculation on reduced test set...\")\n",
        "            small_test_metrics = evaluate_model(model, small_test_loader, device=device, is_final_test=True)\n",
        "\n",
        "            if small_test_metrics['spice'] > 0.0:\n",
        "                print(f\"SPICE calculation successful on reduced set: {small_test_metrics['spice'] * 100:.2f}\")\n",
        "                test_metrics['spice'] = small_test_metrics['spice']\n",
        "                print(f\"Updated SPICE score: {test_metrics['spice'] * 100:.2f}\")\n",
        "\n",
        "        # Save test results (saving both original values and multiplied by 100)\n",
        "        test_results_path = os.path.join(\"token_gating_checkpoints\", \"test_results.pt\")\n",
        "        torch.save({\n",
        "            # Original values\n",
        "            \"bleu4\": test_metrics['bleu4'],\n",
        "            \"cider\": test_metrics['cider'],\n",
        "            \"spice\": test_metrics['spice'],\n",
        "            \"rouge\": test_metrics['rouge'],\n",
        "            \"meteor\": test_metrics.get('meteor', 0.0),\n",
        "            # Values multiplied by 100 for easier reporting\n",
        "            \"bleu4_100\": test_metrics['bleu4'] * 100,\n",
        "            \"cider_100\": test_metrics['cider'] * 100,\n",
        "            \"spice_100\": test_metrics['spice'] * 100,\n",
        "            \"rouge_100\": test_metrics['rouge'] * 100,\n",
        "            \"meteor_100\": test_metrics.get('meteor', 0.0) * 100\n",
        "        }, test_results_path)\n",
        "\n",
        "        # Compare validation and test results (with metrics multiplied by 100)\n",
        "        print(\"\\nValidation vs Test Performance (scores multiplied by 100):\")\n",
        "        metrics_comparison = {\n",
        "            'Metric': ['BLEU-4', 'CIDEr', 'SPICE', 'ROUGE', 'METEOR'],\n",
        "            'Validation': [\n",
        "                f\"{val_metrics['bleu4'] * 100:.2f}\",\n",
        "                f\"{val_metrics['cider'] * 100:.2f}\",\n",
        "                f\"{val_metrics['spice'] * 100:.2f}\",\n",
        "                f\"{val_metrics['rouge'] * 100:.2f}\",\n",
        "                f\"{val_metrics.get('meteor', 0.0) * 100:.2f}\"\n",
        "            ],\n",
        "            'Test': [\n",
        "                f\"{test_metrics['bleu4'] * 100:.2f}\",\n",
        "                f\"{test_metrics['cider'] * 100:.2f}\",\n",
        "                f\"{test_metrics['spice'] * 100:.2f}\",\n",
        "                f\"{test_metrics['rouge'] * 100:.2f}\",\n",
        "                f\"{test_metrics.get('meteor', 0.0) * 100:.2f}\"\n",
        "            ],\n",
        "            'Diff (Test-Val)': [\n",
        "                f\"{(test_metrics['bleu4'] - val_metrics['bleu4']) * 100:.2f}\",\n",
        "                f\"{(test_metrics['cider'] - val_metrics['cider']) * 100:.2f}\",\n",
        "                f\"{(test_metrics['spice'] - val_metrics['spice']) * 100:.2f}\",\n",
        "                f\"{(test_metrics['rouge'] - val_metrics['rouge']) * 100:.2f}\",\n",
        "                f\"{(test_metrics.get('meteor', 0.0) - val_metrics.get('meteor', 0.0)) * 100:.2f}\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        comparison_df = pd.DataFrame(metrics_comparison)\n",
        "        print(comparison_df.to_string(index=False))\n",
        "\n",
        "        # Save comparison results\n",
        "        comparison_path = os.path.join(\"token_gating_checkpoints\", \"val_test_comparison.csv\")\n",
        "        comparison_df.to_csv(comparison_path, index=False)\n",
        "        print(f\"Comparison results saved to {comparison_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in main function: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}
